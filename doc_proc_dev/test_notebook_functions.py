#!/usr/bin/env python3
"""
ë…¸íŠ¸ë¶ í•¨ìˆ˜ë“¤ì„ í…ŒìŠ¤íŠ¸í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
"""

import os
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from langchain_core.documents import Document
from langchain_text_splitters import MarkdownHeaderTextSplitter

def test_notebook_functions():
    """ë…¸íŠ¸ë¶ì˜ ì£¼ìš” í•¨ìˆ˜ë“¤ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
    
    print("ğŸš€ ë…¸íŠ¸ë¶ í•¨ìˆ˜ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    # 1. ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ë¡œë“œ í…ŒìŠ¤íŠ¸
    print("\n1. ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ë¡œë“œ í…ŒìŠ¤íŠ¸")
    markdown_file_path = "../data/dev_center_guide_allmd.md"
    
    if not os.path.exists(markdown_file_path):
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {markdown_file_path}")
        return False
    
    with open(markdown_file_path, 'r', encoding='utf-8') as f:
        markdown_content = f.read()
    
    print(f"âœ… ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {len(markdown_content)} ë¬¸ì")
    
    # 2. MarkdownHeaderTextSplitter ì„¤ì • í…ŒìŠ¤íŠ¸
    print("\n2. MarkdownHeaderTextSplitter ì„¤ì • í…ŒìŠ¤íŠ¸")
    headers_to_split_on = [
        ("#", "Header 1"),
        ("##", "Header 2"),
        ("###", "Header 3"),
        ("####", "Header 4"),
        ("#####", "Header 5"),
        ("######", "Header 6"),
    ]
    
    markdown_splitter = MarkdownHeaderTextSplitter(
        headers_to_split_on=headers_to_split_on,
        return_each_line=False,
    )
    
    print("âœ… MarkdownHeaderTextSplitter ì„¤ì • ì™„ë£Œ")
    
    # 3. ì˜ë¯¸ì  ë¶„í•  í…ŒìŠ¤íŠ¸
    print("\n3. ì˜ë¯¸ì  ë¶„í•  í…ŒìŠ¤íŠ¸")
    semantic_chunks = markdown_splitter.split_text(markdown_content)
    print(f"âœ… ë¶„í•  ì™„ë£Œ: {len(semantic_chunks)}ê°œì˜ ì²­í¬ ìƒì„±")
    
    # 4. Document ê°ì²´ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
    print("\n4. Document ê°ì²´ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸")
    documents = semantic_chunks
    
    # ê° Documentì˜ ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€ ì •ë³´ ì„¤ì •
    for i, doc in enumerate(documents):
        doc.metadata.update({
            'source': markdown_file_path,
            'chunk_id': i,
            'chunk_type': 'semantic_markdown',
            'content_length': len(doc.page_content),
        })
    
    print(f"âœ… Document ê°ì²´ ì²˜ë¦¬ ì™„ë£Œ: {len(documents)}ê°œ")
    
    # 5. í†µê³„ ì •ë³´ í…ŒìŠ¤íŠ¸
    print("\n5. í†µê³„ ì •ë³´ í…ŒìŠ¤íŠ¸")
    if documents:
        total_chars = sum(len(doc.page_content) for doc in documents)
        avg_chars = total_chars / len(documents)
        
        print(f"ì´ ë¬¸ì„œ ìˆ˜: {len(documents)}ê°œ")
        print(f"ì´ ë¬¸ì ìˆ˜: {total_chars:,}ì")
        print(f"í‰ê·  ì²­í¬ í¬ê¸°: {avg_chars:.1f}ì")
        
        # í—¤ë”ê°€ ìˆëŠ” ë¬¸ì„œ ìˆ˜
        docs_with_headers = [doc for doc in documents if any(key.startswith('Header') for key in doc.metadata.keys())]
        print(f"í—¤ë”ê°€ í¬í•¨ëœ ë¬¸ì„œ: {len(docs_with_headers)}ê°œ")
        
        # ì²« ë²ˆì§¸ ë¬¸ì„œ ìƒ˜í”Œ ì¶œë ¥
        first_doc = documents[0]
        print(f"ì²« ë²ˆì§¸ ë¬¸ì„œ ë©”íƒ€ë°ì´í„°: {first_doc.metadata}")
        print(f"ì²« ë²ˆì§¸ ë¬¸ì„œ ë‚´ìš© (ì²˜ìŒ 100ì): {first_doc.page_content[:100]}...")
    
    # 6. í—¤ë” ì •ë³´ ë¶„ì„ í…ŒìŠ¤íŠ¸
    print("\n6. í—¤ë” ì •ë³´ ë¶„ì„ í…ŒìŠ¤íŠ¸")
    header_types = {}
    for doc in documents:
        for key in doc.metadata.keys():
            if key.startswith('Header'):
                header_types[key] = header_types.get(key, 0) + 1
    
    for header_type, count in sorted(header_types.items()):
        print(f"  {header_type}: {count}ê°œ ë¬¸ì„œ")
    
    print("\nâœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    return True

if __name__ == "__main__":
    try:
        success = test_notebook_functions()
        if not success:
            sys.exit(1)
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()
        sys.exit(1) 