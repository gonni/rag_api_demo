#!/usr/bin/env python3
"""
SemanticChunker í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
SemanticChunkerë¥¼ ì‚¬ìš©í•˜ì—¬ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ ì˜ë¯¸ì ìœ¼ë¡œ ë¶„í• í•˜ëŠ” ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""

import os
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from langchain_core.documents import Document
from langchain_experimental.text_splitter import SemanticChunker
from langchain_ollama import OllamaEmbeddings

def test_semantic_chunker():
    """SemanticChunkerë¥¼ ì‚¬ìš©í•œ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì˜ ì˜ë¯¸ì  ë¶„í• ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
    
    # ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ê²½ë¡œ
    markdown_file_path = "../data/dev_center_guide_allmd.md"
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(markdown_file_path):
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {markdown_file_path}")
        return False
    
    try:
        # íŒŒì¼ ë‚´ìš© ì½ê¸°
        with open(markdown_file_path, 'r', encoding='utf-8') as f:
            markdown_content = f.read()
        
        print(f"âœ… ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {len(markdown_content)} ë¬¸ì")
        
        # ì„ë² ë”© ëª¨ë¸ ì„¤ì •
        print("ğŸ”„ ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘...")
        embeddings = OllamaEmbeddings(model="exaone3.5:latest")
        
        # SemanticChunker ì„¤ì •
        print("ğŸ”„ SemanticChunker ì„¤ì • ì¤‘...")
        semantic_chunker = SemanticChunker(
            embeddings=embeddings,
            breakpoint_threshold_type="percentile",
            breakpoint_threshold_amount=95,
            min_chunk_size=100,
        )
        
        print("âœ… SemanticChunker ì„¤ì • ì™„ë£Œ")
        
        # ì˜ë¯¸ì  ë¶„í•  ìˆ˜í–‰
        print("ğŸ”„ ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œë¥¼ ì˜ë¯¸ì ìœ¼ë¡œ ë¶„í•  ì¤‘...")
        print("âš ï¸  ì´ ê³¼ì •ì€ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤...")
        semantic_chunks = semantic_chunker.split_text(markdown_content)
        
        print(f"âœ… ë¶„í•  ì™„ë£Œ: {len(semantic_chunks)}ê°œì˜ ì²­í¬ ìƒì„±")
        
        # Document ê°ì²´ ìƒì„±
        documents = []
        for i, chunk in enumerate(semantic_chunks):
            # ë©”íƒ€ë°ì´í„° êµ¬ì„±
            metadata = {
                'source': markdown_file_path,
                'chunk_id': i,
                'chunk_type': 'semantic_chunker',
                'content_length': len(chunk),
                'chunker_model': 'SemanticChunker',
                'embedding_model': 'exaone3.5:latest',
            }
            
            # Document ê°ì²´ ìƒì„±
            doc = Document(
                page_content=chunk,
                metadata=metadata
            )
            
            documents.append(doc)
        
        print(f"âœ… Document ê°ì²´ ìƒì„± ì™„ë£Œ: {len(documents)}ê°œ")
        
        # í†µê³„ ì •ë³´ ì¶œë ¥
        if documents:
            total_chars = sum(len(doc.page_content) for doc in documents)
            avg_chars = total_chars / len(documents)
            
            print(f"\nğŸ“Š === í†µê³„ ì •ë³´ ===")
            print(f"ì´ ë¬¸ì„œ ìˆ˜: {len(documents)}ê°œ")
            print(f"ì´ ë¬¸ì ìˆ˜: {total_chars:,}ì")
            print(f"í‰ê·  ì²­í¬ í¬ê¸°: {avg_chars:.1f}ì")
            
            # ì²­í¬ í¬ê¸° í†µê³„
            chunk_sizes = [len(doc.page_content) for doc in documents]
            print(f"ìµœì†Œ ì²­í¬ í¬ê¸°: {min(chunk_sizes)}ì")
            print(f"ìµœëŒ€ ì²­í¬ í¬ê¸°: {max(chunk_sizes)}ì")
            
            # ì²« ë²ˆì§¸ ë¬¸ì„œ ìƒ˜í”Œ ì¶œë ¥
            print(f"\nğŸ“„ === ì²« ë²ˆì§¸ Document ìƒ˜í”Œ ===")
            first_doc = documents[0]
            print(f"ë©”íƒ€ë°ì´í„°: {first_doc.metadata}")
            print(f"ë‚´ìš© (ì²˜ìŒ 150ì): {first_doc.page_content[:150]}...")
            
            return True
            
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    print("ğŸš€ SemanticChunker í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    success = test_semantic_chunker()
    
    if success:
        print("\nâœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print("\nâŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        sys.exit(1) 