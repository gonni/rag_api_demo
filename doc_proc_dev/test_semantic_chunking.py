#!/usr/bin/env python3
"""
Semantic Chunking í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
MarkdownHeaderTextSplitterë¥¼ ì‚¬ìš©í•˜ì—¬ ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ ì˜ë¯¸ì ìœ¼ë¡œ ë¶„í• í•˜ëŠ” ê¸°ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""

import os
import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent.parent
sys.path.append(str(project_root))

from langchain_core.documents import Document
from langchain_text_splitters import MarkdownHeaderTextSplitter

def test_semantic_chunking():
    """ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì˜ ì˜ë¯¸ì  ë¶„í• ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
    
    # ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ê²½ë¡œ
    markdown_file_path = "../data/dev_center_guide_allmd.md"
    
    # íŒŒì¼ ì¡´ì¬ í™•ì¸
    if not os.path.exists(markdown_file_path):
        print(f"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {markdown_file_path}")
        return False
    
    try:
        # íŒŒì¼ ë‚´ìš© ì½ê¸°
        with open(markdown_file_path, 'r', encoding='utf-8') as f:
            markdown_content = f.read()
        
        print(f"âœ… ë§ˆí¬ë‹¤ìš´ íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {len(markdown_content)} ë¬¸ì")
        
        # MarkdownHeaderTextSplitter ì„¤ì •
        headers_to_split_on = [
            ("#", "Header 1"),
            ("##", "Header 2"),
            ("###", "Header 3"),
            ("####", "Header 4"),
            ("#####", "Header 5"),
            ("######", "Header 6"),
        ]
        
        markdown_splitter = MarkdownHeaderTextSplitter(
            headers_to_split_on=headers_to_split_on,
            return_each_line=False,
        )
        
        print("âœ… MarkdownHeaderTextSplitter ì„¤ì • ì™„ë£Œ")
        
        # ì˜ë¯¸ì  ë¶„í•  ìˆ˜í–‰
        print("ğŸ”„ ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œë¥¼ ì˜ë¯¸ì ìœ¼ë¡œ ë¶„í•  ì¤‘...")
        semantic_chunks = markdown_splitter.split_text(markdown_content)
        
        print(f"âœ… ë¶„í•  ì™„ë£Œ: {len(semantic_chunks)}ê°œì˜ ì²­í¬ ìƒì„±")
        
        # Document ê°ì²´ë“¤ì€ ì´ë¯¸ ìƒì„±ë˜ì–´ ìˆìŒ
        documents = semantic_chunks
        
        # ê° Documentì˜ ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€ ì •ë³´ ì„¤ì •
        for i, doc in enumerate(documents):
            # ê¸°ì¡´ ë©”íƒ€ë°ì´í„°ì— ì¶”ê°€ ì •ë³´ ì„¤ì •
            doc.metadata.update({
                'source': markdown_file_path,
                'chunk_id': i,
                'chunk_type': 'semantic_markdown',
                'content_length': len(doc.page_content),
            })
        
        print(f"âœ… Document ê°ì²´ ìƒì„± ì™„ë£Œ: {len(documents)}ê°œ")
        
        # í†µê³„ ì •ë³´ ì¶œë ¥
        if documents:
            total_chars = sum(len(doc.page_content) for doc in documents)
            avg_chars = total_chars / len(documents)
            
            print(f"\nğŸ“Š === í†µê³„ ì •ë³´ ===")
            print(f"ì´ ë¬¸ì„œ ìˆ˜: {len(documents)}ê°œ")
            print(f"ì´ ë¬¸ì ìˆ˜: {total_chars:,}ì")
            print(f"í‰ê·  ì²­í¬ í¬ê¸°: {avg_chars:.1f}ì")
            
            # í—¤ë”ê°€ ìˆëŠ” ë¬¸ì„œ ìˆ˜ (ë©”íƒ€ë°ì´í„°ì—ì„œ í—¤ë” ì •ë³´ í™•ì¸)
            docs_with_headers = [doc for doc in documents if any(key.startswith('Header') for key in doc.metadata.keys())]
            print(f"í—¤ë”ê°€ í¬í•¨ëœ ë¬¸ì„œ: {len(docs_with_headers)}ê°œ")
            
            # ì²« ë²ˆì§¸ ë¬¸ì„œ ìƒ˜í”Œ ì¶œë ¥
            print(f"\nğŸ“„ === ì²« ë²ˆì§¸ Document ìƒ˜í”Œ ===")
            first_doc = documents[0]
            print(f"ë©”íƒ€ë°ì´í„°: {first_doc.metadata}")
            print(f"ë‚´ìš© (ì²˜ìŒ 150ì): {first_doc.page_content[:150]}...")
            
            return True
            
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")
        return False

if __name__ == "__main__":
    print("ğŸš€ Semantic Chunking í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    success = test_semantic_chunking()
    
    if success:
        print("\nâœ… ëª¨ë“  í…ŒìŠ¤íŠ¸ê°€ ì„±ê³µì ìœ¼ë¡œ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!")
    else:
        print("\nâŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.")
        sys.exit(1) 