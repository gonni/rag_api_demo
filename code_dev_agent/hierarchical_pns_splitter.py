"""
PNS ë¬¸ì„œ ì „ìš© ê³„ì¸µì  ë¶„í•  ì „ëµ

ì´ ëª¨ë“ˆì€ PNS ê´€ë ¨ ë¬¸ì„œë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ë¶„í• í•˜ì—¬ 
ì»¨í…ìŠ¤íŠ¸ ì†ì‹¤ì„ ìµœì†Œí™”í•˜ëŠ” ì „ëµì„ ì œê³µí•©ë‹ˆë‹¤.
"""

import re
from typing import List, Dict, Any, Tuple
from langchain.docstore.document import Document
from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter


class PNSHierarchicalSplitter:
    """PNS ë¬¸ì„œ ì „ìš© ê³„ì¸µì  ë¶„í• ê¸°"""
    
    def __init__(self, document_path: str):
        self.document_path = document_path
        self.raw_text = self._load_document()
        
    def _load_document(self) -> str:
        """ë¬¸ì„œ ë¡œë“œ"""
        with open(self.document_path, 'r', encoding='utf-8') as f:
            return f.read()
    
    def split_documents(self) -> List[Document]:
        """PNS ìµœì í™” ë¬¸ì„œ ë¶„í• """
        print("ğŸš€ PNS ê³„ì¸µì  ë¬¸ì„œ ë¶„í•  ì‹œì‘...")
        
        # 1ë‹¨ê³„: PNS ì„¹ì…˜ ì‹ë³„ ë° ì¶”ì¶œ
        pns_sections = self._extract_pns_sections()
        print(f"ğŸ“‹ PNS ì„¹ì…˜ ìˆ˜: {len(pns_sections)}")
        
        # 2ë‹¨ê³„: ê° ì„¹ì…˜ë³„ ìµœì í™”ëœ ë¶„í• 
        all_documents = []
        for section_name, section_content in pns_sections.items():
            section_docs = self._split_pns_section(section_name, section_content)
            all_documents.extend(section_docs)
            print(f"  {section_name}: {len(section_docs)}ê°œ ì²­í¬")
        
        print(f"âœ… ì´ {len(all_documents)}ê°œ PNS ìµœì í™” ë¬¸ì„œ ìƒì„±")
        return all_documents
    
    def _extract_pns_sections(self) -> Dict[str, str]:
        """PNS ê´€ë ¨ ì„¹ì…˜ ì¶”ì¶œ"""
        sections = {}
        
        # PNS ê´€ë ¨ í—¤ë” íŒ¨í„´
        pns_patterns = [
            r'(### PNS Payment Notification ë©”ì‹œì§€ ë°œì†¡ ê·œê²©.*?)(?=###|\Z)',
            r'(### PNS Subscription Notification ë©”ì‹œì§€ ë°œì†¡ ê·œê²©.*?)(?=###|\Z)',
            r'(## PNS.*?)(?=##|\Z)',
            r'(#.*?PNS.*?)(?=#|\Z)'
        ]
        
        for pattern in pns_patterns:
            matches = re.findall(pattern, self.raw_text, re.DOTALL | re.IGNORECASE)
            for match in matches:
                # ì„¹ì…˜ ì´ë¦„ ì¶”ì¶œ
                lines = match.strip().split('\n')
                section_name = lines[0].strip()
                if section_name.startswith('#'):
                    section_name = section_name.lstrip('#').strip()
                
                sections[section_name] = match.strip()
        
        return sections
    
    def _split_pns_section(self, section_name: str, section_content: str) -> List[Document]:
        """PNS ì„¹ì…˜ë³„ ìµœì í™”ëœ ë¶„í• """
        documents = []
        
        # 1. ë©”ì‹œì§€ ê·œê²© í…Œì´ë¸” ì‹ë³„
        table_sections = self._extract_message_specifications(section_content)
        
        # 2. ê° í…Œì´ë¸” ì„¹ì…˜ì„ í•˜ë‚˜ì˜ ì™„ì „í•œ ë¬¸ì„œë¡œ ìƒì„±
        for table_name, table_content in table_sections.items():
            # ì»¨í…ìŠ¤íŠ¸ ê°•í™”
            enhanced_content = self._enhance_pns_context(section_name, table_name, table_content)
            
            # ì™„ì „í•œ ë©”ì‹œì§€ ê·œê²©ì„ í•˜ë‚˜ì˜ ë¬¸ì„œë¡œ ìœ ì§€
            metadata = {
                'section_name': section_name,
                'table_name': table_name,
                'content_type': 'pns_message_specification',
                'contains_pns': True,
                'contains_purchasestate': 'purchasestate' in table_content.lower(),
                'is_complete_spec': True,
                'chunk_size': len(enhanced_content)
            }
            
            documents.append(Document(
                page_content=enhanced_content,
                metadata=metadata
            ))
        
        # 3. ë‚˜ë¨¸ì§€ ë‚´ìš©ë„ ì ì ˆíˆ ë¶„í• 
        remaining_content = self._extract_remaining_content(section_content, table_sections)
        if remaining_content:
            remaining_docs = self._split_remaining_content(section_name, remaining_content)
            documents.extend(remaining_docs)
        
        return documents
    
    def _extract_message_specifications(self, content: str) -> Dict[str, str]:
        """ë©”ì‹œì§€ ê·œê²© í…Œì´ë¸” ì¶”ì¶œ"""
        specifications = {}
        
        # í…Œì´ë¸” íŒ¨í„´ ì°¾ê¸°
        table_patterns = [
            r'(\|.*?Element Name.*?Description.*?\|.*?\|.*?\|.*?\|.*?)(?=\n\n|\Z)',
            r'(\|.*?Parameter Name.*?Data Type.*?Description.*?\|.*?\|.*?\|.*?\|.*?)(?=\n\n|\Z)'
        ]
        
        for pattern in table_patterns:
            matches = re.findall(pattern, content, re.DOTALL)
            for i, match in enumerate(matches):
                # í…Œì´ë¸” ì œëª© ì°¾ê¸°
                lines_before = content[:content.find(match)].split('\n')
                title = ""
                for line in reversed(lines_before[-10:]):  # ìµœê·¼ 10ì¤„ì—ì„œ ì œëª© ì°¾ê¸°
                    if line.strip() and not line.startswith('|') and not line.startswith('-'):
                        title = line.strip()
                        break
                
                if not title:
                    title = f"Message Specification {i+1}"
                
                specifications[title] = match.strip()
        
        return specifications
    
    def _enhance_pns_context(self, section_name: str, table_name: str, table_content: str) -> str:
        """PNS ì»¨í…ìŠ¤íŠ¸ ê°•í™”"""
        enhanced = f"[PNS ì„¹ì…˜]: {section_name}\n"
        enhanced += f"[ë©”ì‹œì§€ ê·œê²©]: {table_name}\n"
        enhanced += f"[ì„¤ëª…]: ì´ ë‚´ìš©ì€ PNS(Payment Notification Service) ê²°ì œì•Œë¦¼ì„œë¹„ìŠ¤ì˜ ë©”ì‹œì§€ ê·œê²©ì…ë‹ˆë‹¤.\n\n"
        enhanced += table_content
        
        # purchaseState ê´€ë ¨ ì •ë³´ê°€ ìˆìœ¼ë©´ ê°•ì¡°
        if 'purchasestate' in table_content.lower():
            enhanced += "\n\n[ì¤‘ìš”]: ì´ ë©”ì‹œì§€ì—ëŠ” purchaseState í•„ë“œê°€ í¬í•¨ë˜ì–´ ìˆì–´ ê²°ì œ ìƒíƒœë¥¼ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
        
        return enhanced
    
    def _extract_remaining_content(self, content: str, table_sections: Dict[str, str]) -> str:
        """í…Œì´ë¸” ì™¸ ë‚˜ë¨¸ì§€ ë‚´ìš© ì¶”ì¶œ"""
        remaining = content
        
        # í…Œì´ë¸” ë‚´ìš© ì œê±°
        for table_content in table_sections.values():
            remaining = remaining.replace(table_content, '')
        
        # ë¹ˆ ì¤„ ì •ë¦¬
        remaining = re.sub(r'\n\s*\n\s*\n', '\n\n', remaining)
        return remaining.strip()
    
    def _split_remaining_content(self, section_name: str, content: str) -> List[Document]:
        """ë‚˜ë¨¸ì§€ ë‚´ìš© ë¶„í• """
        if not content or len(content) < 100:
            return []
        
        # ì ì ˆí•œ í¬ê¸°ë¡œ ë¶„í• 
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=800,
            chunk_overlap=150,
            separators=["\n\n", "\n", ". ", "? ", "! ", ", "]
        )
        
        chunks = splitter.split_text(content)
        documents = []
        
        for i, chunk in enumerate(chunks):
            metadata = {
                'section_name': section_name,
                'content_type': 'pns_related_content',
                'chunk_index': i,
                'contains_pns': True,
                'contains_purchasestate': 'purchasestate' in chunk.lower(),
                'chunk_size': len(chunk)
            }
            
            documents.append(Document(
                page_content=chunk,
                metadata=metadata
            ))
        
        return documents


class PNSContextualRetriever:
    """PNS ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ê²€ìƒ‰ê¸°"""
    
    def __init__(self, documents: List[Document], embedding_model_name: str = "bge-m3:latest"):
        self.documents = documents
        self.embedding_model_name = embedding_model_name
        self.vector_store = None
        self.bm25_retriever = None
        
    def build_retrievers(self):
        """ê²€ìƒ‰ê¸° êµ¬ì¶•"""
        print(f"ğŸ”§ PNS ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ê¸° êµ¬ì¶• ì¤‘... (ë¬¸ì„œ ìˆ˜: {len(self.documents)})")
        
        from langchain_ollama import OllamaEmbeddings
        from langchain_community.vectorstores import FAISS
        from langchain_community.retrievers import BM25Retriever
        
        # Vector store êµ¬ì¶•
        embeddings = OllamaEmbeddings(model=self.embedding_model_name)
        self.vector_store = FAISS.from_documents(self.documents, embeddings)
        
        # BM25 ê²€ìƒ‰ê¸° êµ¬ì¶•
        self.bm25_retriever = BM25Retriever.from_documents(self.documents)
        self.bm25_retriever.k = 30
        
        print("âœ… PNS ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ê¸° êµ¬ì¶• ì™„ë£Œ")
    
    def retrieve_pns_context(self, query: str, k: int = 10) -> List[Document]:
        """PNS ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ê²€ìƒ‰"""
        if not self.vector_store or not self.bm25_retriever:
            raise ValueError("ê²€ìƒ‰ê¸°ê°€ êµ¬ì¶•ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # 1. ì™„ì „í•œ ë©”ì‹œì§€ ê·œê²© ìš°ì„  ê²€ìƒ‰
        complete_specs = self._find_complete_specifications(query)
        
        # 2. ë²¡í„° ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ì°¾ê¸°
        vector_results = self.vector_store.similarity_search_with_score(query, k=20)
        
        # 3. BM25 ê²€ìƒ‰ìœ¼ë¡œ í‚¤ì›Œë“œ ë§¤ì¹­
        bm25_results = self.bm25_retriever.get_relevant_documents(query)[:20]
        
        # 4. ê²°ê³¼ í†µí•© ë° ìš°ì„ ìˆœìœ„ ì ìš©
        all_candidates = self._merge_and_prioritize(
            complete_specs, vector_results, bm25_results, query
        )
        
        return [doc for score, doc in all_candidates[:k]]
    
    def _find_complete_specifications(self, query: str) -> List[Tuple[float, Document]]:
        """ì™„ì „í•œ ë©”ì‹œì§€ ê·œê²© ì°¾ê¸°"""
        complete_specs = []
        
        for doc in self.documents:
            if doc.metadata.get('is_complete_spec', False):
                score = self._calculate_spec_relevance(query, doc)
                if score > 0:
                    complete_specs.append((score, doc))
        
        return sorted(complete_specs, key=lambda x: x[0], reverse=True)
    
    def _calculate_spec_relevance(self, query: str, doc: Document) -> float:
        """ë©”ì‹œì§€ ê·œê²© ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        query_lower = query.lower()
        content_lower = doc.page_content.lower()
        
        # PNS ê´€ë ¨ í‚¤ì›Œë“œ
        pns_keywords = ['pns', 'payment notification', 'ë©”ì‹œì§€', 'ê·œê²©']
        for keyword in pns_keywords:
            if keyword in query_lower:
                score += 10
        
        # purchaseState ê´€ë ¨
        if 'purchasestate' in query_lower and 'purchasestate' in content_lower:
            score += 15
        
        # ë©”ì‹œì§€ íƒ€ì… ê´€ë ¨
        if 'message' in query_lower or 'ë©”ì‹œì§€' in query:
            score += 8
        
        return score
    
    def _merge_and_prioritize(self, complete_specs, vector_results, bm25_results, query: str) -> List[Tuple[float, Document]]:
        """ê²°ê³¼ í†µí•© ë° ìš°ì„ ìˆœìœ„ ì ìš©"""
        all_candidates = {}
        
        # ì™„ì „í•œ ë©”ì‹œì§€ ê·œê²© (ìµœìš°ì„ )
        for score, doc in complete_specs:
            all_candidates[doc.page_content] = (score * 2, doc)  # ê°€ì¤‘ì¹˜ 2ë°°
        
        # ë²¡í„° ê²€ìƒ‰ ê²°ê³¼
        for doc, score in vector_results:
            if doc.page_content not in all_candidates:
                all_candidates[doc.page_content] = (score, doc)
        
        # BM25 ê²€ìƒ‰ ê²°ê³¼
        for doc in bm25_results:
            if doc.page_content not in all_candidates:
                all_candidates[doc.page_content] = (0.5, doc)  # ê¸°ë³¸ ì ìˆ˜
        
        # ì ìˆ˜ìˆœ ì •ë ¬
        return sorted(all_candidates.values(), key=lambda x: x[0], reverse=True)
