"""
ê³„ì¸µë³„ í—¤ë” ê¸°ë°˜ ë¬¸ì„œ ë¶„í• ê¸°
ê¸°ìˆ  ë¬¸ì„œì˜ í—¤ë” ê³„ì¸µì— ë”°ë¼ ë¬¸ì„œë¥¼ ë¶„í• í•˜ì—¬ RAGì— ìµœì í™”ëœ ì²­í¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
"""

import re
from typing import List, Dict
from dataclasses import dataclass
import json

@dataclass
class DocumentChunk:
    """ë¬¸ì„œ ì²­í¬ë¥¼ ë‚˜íƒ€ë‚´ëŠ” í´ë˜ìŠ¤"""
    content: str
    level: int  # í—¤ë” ë ˆë²¨ (1=H1, 2=H2, 3=H3, 4=H4)
    title: str
    full_path: List[str]  # ê³„ì¸µì  ê²½ë¡œ (ì˜ˆ: ["07. PNS", "PNS ìƒì„¸", "Payment Notification"])
    metadata: Dict
    start_line: int
    end_line: int

class HierarchicalDocumentSplitter:
    """ê³„ì¸µë³„ í—¤ë” ê¸°ë°˜ ë¬¸ì„œ ë¶„í• ê¸°"""
    
    def __init__(self, include_parent_context: bool = True, max_chunk_size: int = 2000):
        """
        Args:
            include_parent_context: ìƒìœ„ ê³„ì¸µì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ í¬í•¨í• ì§€ ì—¬ë¶€
            max_chunk_size: ìµœëŒ€ ì²­í¬ í¬ê¸° (ë¬¸ì ìˆ˜)
        """
        self.include_parent_context = include_parent_context
        self.max_chunk_size = max_chunk_size
        self.header_pattern = re.compile(r'^(#{1,4})\s+(.+?)(?:\s+<.*)?$', re.MULTILINE)
        
    def split_document(self, text: str) -> List[DocumentChunk]:
        """
        ë¬¸ì„œë¥¼ ê³„ì¸µë³„ë¡œ ë¶„í• í•©ë‹ˆë‹¤.
        
        Args:
            text: ë¶„í• í•  ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸
            
        Returns:
            List[DocumentChunk]: ë¶„í• ëœ ë¬¸ì„œ ì²­í¬ë“¤
        """
        lines = text.split('\n')
        chunks = []
        
        # í—¤ë” ì •ë³´ ì¶”ì¶œ
        headers = self._extract_headers(text)
        
        # ê° í—¤ë”ë³„ë¡œ ë¬¸ì„œ ì²­í¬ ìƒì„±
        for i, header in enumerate(headers):
            start_line = header['line_num']
            end_line = headers[i + 1]['line_num'] - 1 if i + 1 < len(headers) else len(lines)
            
            # í•´ë‹¹ ì„¹ì…˜ì˜ ë‚´ìš© ì¶”ì¶œ
            section_content = '\n'.join(lines[start_line:end_line])
            
            # ìƒìœ„ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€ (ì˜µì…˜)
            if self.include_parent_context:
                parent_context = self._get_parent_context(header, headers)
                if parent_context:
                    section_content = parent_context + '\n\n' + section_content
            
            # ì²­í¬ ìƒì„±
            chunk = DocumentChunk(
                content=section_content,
                level=header['level'],
                title=header['title'],
                full_path=header['path'],
                metadata={
                    'level': header['level'],
                    'section_id': f"section_{i}",
                    'parent_titles': header['path'][:-1],
                    'char_count': len(section_content),
                    'line_range': f"{start_line}-{end_line}"
                },
                start_line=start_line,
                end_line=end_line
            )
            
            chunks.append(chunk)
            
            # í•˜ìœ„ ë ˆë²¨ë³„ë¡œë„ ì²­í¬ ìƒì„± (ê³„ì¸µì  ì ‘ê·¼)
            sub_chunks = self._create_hierarchical_chunks(header, section_content, start_line)
            chunks.extend(sub_chunks)
        
        return chunks
    
    def _extract_headers(self, text: str) -> List[Dict]:
        """í…ìŠ¤íŠ¸ì—ì„œ í—¤ë” ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        lines = text.split('\n')
        headers = []
        path_stack: List[str] = []
        
        for line_num, line in enumerate(lines):
            match = self.header_pattern.match(line)
            if match:
                level = len(match.group(1))  # # ê°œìˆ˜
                title = match.group(2).strip()
                
                # ê²½ë¡œ ìŠ¤íƒ ê´€ë¦¬
                while len(path_stack) >= level:
                    path_stack.pop()
                
                path_stack.append(title)
                
                headers.append({
                    'level': level,
                    'title': title,
                    'path': path_stack.copy(),
                    'line_num': line_num
                })
        
        return headers
    
    def _get_parent_context(self, current_header: Dict, all_headers: List[Dict]) -> str:
        """í˜„ì¬ í—¤ë”ì˜ ìƒìœ„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤."""
        parent_context = []
        
        # ìƒìœ„ ë ˆë²¨ í—¤ë”ë“¤ì˜ ì œëª©ì„ ì»¨í…ìŠ¤íŠ¸ë¡œ ì¶”ê°€
        for parent_title in current_header['path'][:-1]:
            parent_context.append(f"ìƒìœ„ ì„¹ì…˜: {parent_title}")
        
        return '\n'.join(parent_context) if parent_context else ""
    
    def _create_hierarchical_chunks(self, header: Dict, content: str, start_line: int) -> List[DocumentChunk]:
        """ê³„ì¸µì ìœ¼ë¡œ ì²­í¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        chunks = []
        
        # ê¸´ ë‚´ìš©ì„ ë” ì‘ì€ ì²­í¬ë¡œ ë¶„í• 
        if len(content) > self.max_chunk_size:
            sub_chunks = self._split_long_content(content, header, start_line)
            chunks.extend(sub_chunks)
        
        return chunks
    
    def _split_long_content(self, content: str, header: Dict, start_line: int) -> List[DocumentChunk]:
        """ê¸´ ë‚´ìš©ì„ ì‘ì€ ì²­í¬ë¡œ ë¶„í• í•©ë‹ˆë‹¤."""
        chunks = []
        paragraphs = content.split('\n\n')
        current_chunk = ""
        chunk_count = 0
        
        for para in paragraphs:
            if len(current_chunk + para) > self.max_chunk_size and current_chunk:
                # í˜„ì¬ ì²­í¬ë¥¼ ì €ì¥
                chunk = DocumentChunk(
                    content=current_chunk.strip(),
                    level=header['level'] + 1,  # í•˜ìœ„ ë ˆë²¨ë¡œ ì„¤ì •
                    title=f"{header['title']} (Part {chunk_count + 1})",
                    full_path=header['path'] + [f"Part {chunk_count + 1}"],
                    metadata={
                        'level': header['level'] + 1,
                        'section_id': f"section_{header['title']}_{chunk_count}",
                        'parent_titles': header['path'],
                        'char_count': len(current_chunk),
                        'is_sub_chunk': True
                    },
                    start_line=start_line,
                    end_line=start_line
                )
                chunks.append(chunk)
                current_chunk = para
                chunk_count += 1
            else:
                current_chunk += "\n\n" + para if current_chunk else para
        
        # ë§ˆì§€ë§‰ ì²­í¬ ì¶”ê°€
        if current_chunk.strip():
            chunk = DocumentChunk(
                content=current_chunk.strip(),
                level=header['level'] + 1,
                title=f"{header['title']} (Part {chunk_count + 1})",
                full_path=header['path'] + [f"Part {chunk_count + 1}"],
                metadata={
                    'level': header['level'] + 1,
                    'section_id': f"section_{header['title']}_{chunk_count}",
                    'parent_titles': header['path'],
                    'char_count': len(current_chunk),
                    'is_sub_chunk': True
                },
                start_line=start_line,
                end_line=start_line
            )
            chunks.append(chunk)
        
        return chunks
    
    def get_chunks_by_level(self, chunks: List[DocumentChunk], level: int) -> List[DocumentChunk]:
        """íŠ¹ì • ë ˆë²¨ì˜ ì²­í¬ë“¤ë§Œ ë°˜í™˜í•©ë‹ˆë‹¤."""
        return [chunk for chunk in chunks if chunk.level == level]
    
    def find_relevant_chunks(self, chunks: List[DocumentChunk], query: str) -> List[DocumentChunk]:
        """ì¿¼ë¦¬ì™€ ê´€ë ¨ëœ ì²­í¬ë“¤ì„ ì°¾ìŠµë‹ˆë‹¤."""
        relevant_chunks = []
        query_lower = query.lower()
        
        for chunk in chunks:
            # ì œëª©ì´ë‚˜ ë‚´ìš©ì— ì¿¼ë¦¬ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ì²­í¬ ì°¾ê¸°
            if (query_lower in chunk.title.lower() or 
                query_lower in chunk.content.lower()):
                relevant_chunks.append(chunk)
        
        return relevant_chunks
    
    def export_chunks_to_json(self, chunks: List[DocumentChunk], filename: str):
        """ì²­í¬ë“¤ì„ JSON íŒŒì¼ë¡œ ë‚´ë³´ëƒ…ë‹ˆë‹¤."""
        chunks_data = []
        for chunk in chunks:
            chunks_data.append({
                'content': chunk.content,
                'level': chunk.level,
                'title': chunk.title,
                'full_path': chunk.full_path,
                'metadata': chunk.metadata,
                'start_line': chunk.start_line,
                'end_line': chunk.end_line
            })
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(chunks_data, f, ensure_ascii=False, indent=2)
    
    def print_chunk_summary(self, chunks: List[DocumentChunk]):
        """ì²­í¬ë“¤ì˜ ìš”ì•½ ì •ë³´ë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤."""
        print(f"ì´ {len(chunks)}ê°œì˜ ì²­í¬ê°€ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤.\n")
        
        for level in range(1, 5):
            level_chunks = self.get_chunks_by_level(chunks, level)
            if level_chunks:
                print(f"ë ˆë²¨ {level} í—¤ë”: {len(level_chunks)}ê°œ")
                for chunk in level_chunks[:3]:  # ì²˜ìŒ 3ê°œë§Œ í‘œì‹œ
                    print(f"  - {chunk.title} ({len(chunk.content)}ì)")
                if len(level_chunks) > 3:
                    print(f"  ... ë° {len(level_chunks) - 3}ê°œ ë”")
                print()


def demo_pns_query_test(chunks: List[DocumentChunk]):
    """PNS purchaseState ì§ˆì˜ì— ëŒ€í•œ í…ŒìŠ¤íŠ¸"""
    print("=== PNS purchaseState ì§ˆì˜ í…ŒìŠ¤íŠ¸ ===\n")
    
    query = "purchaseState"
    splitter = HierarchicalDocumentSplitter()
    relevant_chunks = splitter.find_relevant_chunks(chunks, query)
    
    print(f"'{query}' ê´€ë ¨ ì²­í¬ {len(relevant_chunks)}ê°œ ë°œê²¬:\n")
    
    # ë‹µë³€ ìƒì„±ì„ ìœ„í•œ ì •ë³´ ìˆ˜ì§‘
    answer_found = False
    purchase_state_info = []
    
    for i, chunk in enumerate(relevant_chunks):
        print(f"ì²­í¬ {i+1}:")
        print(f"  ì œëª©: {chunk.title}")
        print(f"  ê²½ë¡œ: {' > '.join(chunk.full_path)}")
        print(f"  ë ˆë²¨: {chunk.level}")
        
        # purchaseState ê´€ë ¨ ë‚´ìš© ì¶”ì¶œ (ê°œì„ ëœ ë¡œì§)
        lines = chunk.content.split('\n')
        found_purchase_state = False
        
        for line in lines:
            if 'purchaseState' in line.lower():
                print(f"  ê´€ë ¨ ë‚´ìš©: {line.strip()}")
                found_purchase_state = True
                
                # í…Œì´ë¸” í˜•íƒœë‚˜ ì„¤ëª… í˜•íƒœì—ì„œ COMPLETED/CANCELED ì¶”ì¶œ
                if 'COMPLETED' in line and 'CANCELED' in line:
                    # í…Œì´ë¸” í˜•íƒœ: | purchaseState | String | COMPLETED : ê²°ì œì™„ë£Œ / CANCELED : ì·¨ì†Œ |
                    if '|' in line:
                        parts = line.split('|')
                        for part in parts:
                            if 'COMPLETED' in part and 'CANCELED' in part:
                                purchase_state_info.append(part.strip())
                                answer_found = True
                    else:
                        purchase_state_info.append(line.strip())
                        answer_found = True
        
        if found_purchase_state:
            print()
    
    # ê°œì„ ëœ ë‹µë³€ ìƒì„±
    if answer_found:
        print("âœ… ì˜ˆìƒ ë‹µë³€:")
        print("PNSì˜ purchaseStateëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê°’ì´ ìˆìŠµë‹ˆë‹¤:")
        
        # COMPLETEDì™€ CANCELED ê°’ ì¶”ì¶œ
        for info in purchase_state_info:
            if 'COMPLETED' in info and 'CANCELED' in info:
                print("  - COMPLETED : ê²°ì œì™„ë£Œ")
                print("  - CANCELED : ì·¨ì†Œ")
                break
        else:
            for info in purchase_state_info:
                print(f"  - {info}")
    else:
        print("âŒ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        
        # ë””ë²„ê¹…ì„ ìœ„í•œ ì¶”ê°€ ê²€ìƒ‰
        print("\nğŸ” ë””ë²„ê¹…: ë” ë„“ì€ ë²”ìœ„ì—ì„œ ê²€ìƒ‰...")
        pns_chunks = [chunk for chunk in chunks if 'pns' in chunk.title.lower() or 'payment notification' in chunk.title.lower()]
        
        for chunk in pns_chunks:
            lines = chunk.content.split('\n')
            for line in lines:
                if 'purchaseState' in line.lower() and ('COMPLETED' in line or 'CANCELED' in line):
                    print(f"ğŸ¯ ë°œê²¬: {line.strip()}")
                    print(f"   ì²­í¬: {chunk.title}")
                    answer_found = True
        
        if not answer_found:
            print("   ì¶”ê°€ ê²€ìƒ‰ì—ì„œë„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
