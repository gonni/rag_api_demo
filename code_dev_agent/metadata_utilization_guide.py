"""
ë©”íƒ€ë°ì´í„° í™œìš© ê°€ì´ë“œ ë° ìœ í‹¸ë¦¬í‹°

ì´ ëª¨ë“ˆì€ RAG ì‹œìŠ¤í…œì—ì„œ ë©”íƒ€ë°ì´í„°ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ í™œìš©í•˜ëŠ” ë°©ë²•ì„
ì œì‹œí•˜ê³ , êµ¬ì²´ì ì¸ êµ¬í˜„ ì˜ˆì‹œë¥¼ ì œê³µí•©ë‹ˆë‹¤.
"""

import re
from typing import List, Dict, Any, Tuple, Optional
from langchain.docstore.document import Document
from dataclasses import dataclass
from enum import Enum


class ContentType(Enum):
    """ë¬¸ì„œ ë‚´ìš© íƒ€ì…"""
    MESSAGE_SPECIFICATION = "message_specification"
    PURCHASE_STATE_INFO = "purchase_state_info"
    SIGNATURE_VERIFICATION = "signature_verification"
    GENERAL_PNS = "general_pns"
    CODE_EXAMPLE = "code_example"
    ERROR_HANDLING = "error_handling"


class PriorityLevel(Enum):
    """ìš°ì„ ìˆœìœ„ ë ˆë²¨"""
    HIGH = "high"
    MEDIUM = "medium"
    LOW = "low"


@dataclass
class MetadataScore:
    """ë©”íƒ€ë°ì´í„° ì ìˆ˜"""
    relevance_score: float
    completeness_score: float
    context_score: float
    total_score: float


class MetadataAnalyzer:
    """ë©”íƒ€ë°ì´í„° ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.keyword_patterns = {
            'pns': [r'\bPNS\b', r'Payment Notification', r'ê²°ì œì•Œë¦¼'],
            'purchasestate': [r'\bpurchaseState\b', r'purchase.*?state', r'ê²°ì œ.*?ìƒíƒœ'],
            'signature': [r'\bsignature\b', r'ì„œëª…', r'ê²€ì¦'],
            'message': [r'\bmessage\b', r'ë©”ì‹œì§€', r'ê·œê²©'],
            'api': [r'\bAPI\b', r'endpoint', r'ìš”ì²­'],
            'error': [r'\berror\b', r'ì˜¤ë¥˜', r'ì—ëŸ¬', r'exception']
        }
    
    def analyze_document_metadata(self, doc: Document) -> Dict[str, Any]:
        """ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ë¶„ì„"""
        content = doc.page_content
        metadata = doc.metadata
        
        analysis = {
            'content_type': self._determine_content_type(content, metadata),
            'priority_level': self._determine_priority_level(metadata),
            'keyword_density': self._calculate_keyword_density(content),
            'completeness_score': self._calculate_completeness_score(metadata),
            'context_relevance': self._calculate_context_relevance(content, metadata),
            'search_boost_factors': self._identify_boost_factors(metadata)
        }
        
        return analysis
    
    def _determine_content_type(self, content: str, metadata: Dict) -> str:
        """ë‚´ìš© íƒ€ì… ê²°ì •"""
        content_lower = content.lower()
        
        # ë©”íƒ€ë°ì´í„°ì—ì„œ í™•ì¸
        if metadata.get('is_complete_spec', False):
            return ContentType.MESSAGE_SPECIFICATION.value
        
        # ë‚´ìš© ê¸°ë°˜ íŒë‹¨
        if 'purchasestate' in content_lower:
            return ContentType.PURCHASE_STATE_INFO.value
        elif 'signature' in content_lower:
            return ContentType.SIGNATURE_VERIFICATION.value
        elif '```' in content or 'code' in content_lower:
            return ContentType.CODE_EXAMPLE.value
        elif 'error' in content_lower or 'exception' in content_lower:
            return ContentType.ERROR_HANDLING.value
        else:
            return ContentType.GENERAL_PNS.value
    
    def _determine_priority_level(self, metadata: Dict) -> str:
        """ìš°ì„ ìˆœìœ„ ë ˆë²¨ ê²°ì •"""
        if metadata.get('is_complete_spec', False):
            return PriorityLevel.HIGH.value
        elif metadata.get('contains_purchasestate', False):
            return PriorityLevel.HIGH.value
        elif metadata.get('contains_pns', False):
            return PriorityLevel.MEDIUM.value
        else:
            return PriorityLevel.LOW.value
    
    def _calculate_keyword_density(self, content: str) -> Dict[str, float]:
        """í‚¤ì›Œë“œ ë°€ë„ ê³„ì‚°"""
        content_lower = content.lower()
        total_words = len(content.split())
        
        keyword_density = {}
        for keyword, patterns in self.keyword_patterns.items():
            count = 0
            for pattern in patterns:
                count += len(re.findall(pattern, content_lower, re.IGNORECASE))
            keyword_density[keyword] = count / total_words if total_words > 0 else 0
        
        return keyword_density
    
    def _calculate_completeness_score(self, metadata: Dict) -> float:
        """ì™„ì„±ë„ ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        
        # ì™„ì „í•œ ë©”ì‹œì§€ ê·œê²©
        if metadata.get('is_complete_spec', False):
            score += 1.0
        
        # PNS ê´€ë ¨ì„±
        if metadata.get('contains_pns', False):
            score += 0.3
        
        # purchaseState í¬í•¨
        if metadata.get('contains_purchasestate', False):
            score += 0.4
        
        # ì„¹ì…˜ ì •ë³´
        if metadata.get('section_name'):
            score += 0.2
        
        # í…Œì´ë¸” ì •ë³´
        if metadata.get('table_name'):
            score += 0.3
        
        return min(score, 1.0)
    
    def _calculate_context_relevance(self, content: str, metadata: Dict) -> float:
        """ì»¨í…ìŠ¤íŠ¸ ê´€ë ¨ì„± ì ìˆ˜"""
        score = 0.0
        
        # ê³„ì¸µ ì •ë³´
        if metadata.get('title_hierarchy'):
            hierarchy = metadata['title_hierarchy']
            if 'PNS' in hierarchy:
                score += 0.4
            if 'ë©”ì‹œì§€' in hierarchy or 'message' in hierarchy.lower():
                score += 0.3
        
        # ì„¹ì…˜ ì •ë³´
        if metadata.get('section_name'):
            section = metadata['section_name']
            if 'PNS' in section:
                score += 0.3
        
        return min(score, 1.0)
    
    def _identify_boost_factors(self, metadata: Dict) -> List[str]:
        """ë¶€ìŠ¤íŠ¸ íŒ©í„° ì‹ë³„"""
        boost_factors = []
        
        if metadata.get('is_complete_spec', False):
            boost_factors.append('complete_specification')
        
        if metadata.get('contains_purchasestate', False):
            boost_factors.append('purchase_state_related')
        
        if metadata.get('contains_pns', False):
            boost_factors.append('pns_related')
        
        if metadata.get('content_type') == 'message_specification':
            boost_factors.append('message_specification')
        
        return boost_factors


class MetadataBasedRetriever:
    """ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ê²€ìƒ‰ê¸°"""
    
    def __init__(self, documents: List[Document]):
        self.documents = documents
        self.metadata_analyzer = MetadataAnalyzer()
        self.document_analyses = self._analyze_all_documents()
    
    def _analyze_all_documents(self) -> Dict[str, Dict[str, Any]]:
        """ëª¨ë“  ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ë¶„ì„"""
        analyses = {}
        for i, doc in enumerate(self.documents):
            doc_id = f"doc_{i}"
            analyses[doc_id] = {
                'document': doc,
                'analysis': self.metadata_analyzer.analyze_document_metadata(doc)
            }
        return analyses
    
    def search_by_metadata(self, query: str, search_criteria: Dict[str, Any]) -> List[Tuple[float, Document]]:
        """ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ê²€ìƒ‰"""
        query_analysis = self._analyze_query(query)
        
        scored_docs = []
        for doc_id, doc_info in self.document_analyses.items():
            doc = doc_info['document']
            analysis = doc_info['analysis']
            
            # ë©”íƒ€ë°ì´í„° ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
            metadata_score = self._calculate_metadata_matching_score(
                query_analysis, analysis, search_criteria
            )
            
            if metadata_score > 0:
                scored_docs.append((metadata_score, doc))
        
        # ì ìˆ˜ìˆœ ì •ë ¬
        scored_docs.sort(key=lambda x: x[0], reverse=True)
        return scored_docs
    
    def _analyze_query(self, query: str) -> Dict[str, Any]:
        """ì§ˆì˜ ë¶„ì„"""
        query_lower = query.lower()
        
        analysis = {
            'target_content_type': self._identify_target_content_type(query_lower),
            'required_keywords': self._extract_required_keywords(query_lower),
            'priority_level': self._determine_query_priority(query_lower)
        }
        
        return analysis
    
    def _identify_target_content_type(self, query: str) -> str:
        """ëª©í‘œ ë‚´ìš© íƒ€ì… ì‹ë³„"""
        if 'purchasestate' in query or 'purchase state' in query:
            return ContentType.PURCHASE_STATE_INFO.value
        elif 'signature' in query:
            return ContentType.SIGNATURE_VERIFICATION.value
        elif 'message' in query or 'ë©”ì‹œì§€' in query:
            return ContentType.MESSAGE_SPECIFICATION.value
        elif 'code' in query or 'ì˜ˆì œ' in query:
            return ContentType.CODE_EXAMPLE.value
        else:
            return ContentType.GENERAL_PNS.value
    
    def _extract_required_keywords(self, query: str) -> List[str]:
        """í•„ìš”í•œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        required_keywords = []
        
        for keyword, patterns in self.metadata_analyzer.keyword_patterns.items():
            for pattern in patterns:
                if re.search(pattern, query, re.IGNORECASE):
                    required_keywords.append(keyword)
                    break
        
        return list(set(required_keywords))
    
    def _determine_query_priority(self, query: str) -> str:
        """ì§ˆì˜ ìš°ì„ ìˆœìœ„ ê²°ì •"""
        if 'purchasestate' in query or 'signature' in query:
            return PriorityLevel.HIGH.value
        elif 'message' in query or 'ë©”ì‹œì§€' in query:
            return PriorityLevel.MEDIUM.value
        else:
            return PriorityLevel.LOW.value
    
    def _calculate_metadata_matching_score(
        self, 
        query_analysis: Dict[str, Any], 
        doc_analysis: Dict[str, Any], 
        search_criteria: Dict[str, Any]
    ) -> float:
        """ë©”íƒ€ë°ì´í„° ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        
        # 1. ë‚´ìš© íƒ€ì… ë§¤ì¹­
        if query_analysis['target_content_type'] == doc_analysis['content_type']:
            score += 2.0
        
        # 2. ìš°ì„ ìˆœìœ„ ë§¤ì¹­
        if query_analysis['priority_level'] == doc_analysis['priority_level']:
            score += 1.5
        
        # 3. í‚¤ì›Œë“œ ë§¤ì¹­
        for keyword in query_analysis['required_keywords']:
            if keyword in doc_analysis['keyword_density']:
                density = doc_analysis['keyword_density'][keyword]
                score += density * 10  # í‚¤ì›Œë“œ ë°€ë„ì— ë¹„ë¡€í•œ ì ìˆ˜
        
        # 4. ì™„ì„±ë„ ì ìˆ˜
        score += doc_analysis['completeness_score'] * 2
        
        # 5. ì»¨í…ìŠ¤íŠ¸ ê´€ë ¨ì„±
        score += doc_analysis['context_relevance'] * 1.5
        
        # 6. ë¶€ìŠ¤íŠ¸ íŒ©í„° ì ìš©
        for boost_factor in doc_analysis['search_boost_factors']:
            if boost_factor in search_criteria.get('boost_factors', []):
                score *= 1.5
        
        return score


class MetadataEnhancer:
    """ë©”íƒ€ë°ì´í„° ê°•í™”ê¸°"""
    
    @staticmethod
    def enhance_document_metadata(doc: Document, additional_info: Dict[str, Any]) -> Document:
        """ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ê°•í™”"""
        enhanced_metadata = doc.metadata.copy()
        
        # ì¶”ê°€ ì •ë³´ ë³‘í•©
        enhanced_metadata.update(additional_info)
        
        # ê³„ì‚°ëœ í•„ë“œ ì¶”ê°€
        enhanced_metadata['enhanced_at'] = 'metadata_enhancer'
        enhanced_metadata['total_metadata_fields'] = len(enhanced_metadata)
        
        return Document(
            page_content=doc.page_content,
            metadata=enhanced_metadata
        )
    
    @staticmethod
    def create_search_metadata(query: str) -> Dict[str, Any]:
        """ê²€ìƒ‰ìš© ë©”íƒ€ë°ì´í„° ìƒì„±"""
        query_lower = query.lower()
        
        search_metadata = {
            'query_type': 'unknown',
            'target_keywords': [],
            'boost_factors': [],
            'priority_level': 'low'
        }
        
        # ì§ˆì˜ íƒ€ì… ì‹ë³„
        if 'purchasestate' in query_lower:
            search_metadata['query_type'] = 'purchase_state'
            search_metadata['target_keywords'].append('purchasestate')
            search_metadata['boost_factors'].append('purchase_state_related')
            search_metadata['priority_level'] = 'high'
        
        elif 'signature' in query_lower:
            search_metadata['query_type'] = 'signature_verification'
            search_metadata['target_keywords'].append('signature')
            search_metadata['boost_factors'].append('signature_related')
            search_metadata['priority_level'] = 'high'
        
        elif 'message' in query_lower or 'ë©”ì‹œì§€' in query:
            search_metadata['query_type'] = 'message_specification'
            search_metadata['target_keywords'].extend(['message', 'ë©”ì‹œì§€'])
            search_metadata['boost_factors'].append('message_specification')
            search_metadata['priority_level'] = 'medium'
        
        return search_metadata


class MetadataVisualizer:
    """ë©”íƒ€ë°ì´í„° ì‹œê°í™”ê¸°"""
    
    @staticmethod
    def print_metadata_summary(documents: List[Document]):
        """ë©”íƒ€ë°ì´í„° ìš”ì•½ ì¶œë ¥"""
        print("ğŸ“Š ë©”íƒ€ë°ì´í„° ë¶„ì„ ìš”ì•½")
        print("=" * 50)
        
        # í†µê³„ ê³„ì‚°
        total_docs = len(documents)
        pns_docs = sum(1 for doc in documents if doc.metadata.get('contains_pns', False))
        complete_specs = sum(1 for doc in documents if doc.metadata.get('is_complete_spec', False))
        purchase_state_docs = sum(1 for doc in documents if doc.metadata.get('contains_purchasestate', False))
        
        print(f"ğŸ“„ ì´ ë¬¸ì„œ ìˆ˜: {total_docs}")
        print(f"ğŸ”— PNS ê´€ë ¨ ë¬¸ì„œ: {pns_docs} ({pns_docs/total_docs*100:.1f}%)")
        print(f"ğŸ“‹ ì™„ì „í•œ ë©”ì‹œì§€ ê·œê²©: {complete_specs} ({complete_specs/total_docs*100:.1f}%)")
        print(f"ğŸ’° purchaseState í¬í•¨: {purchase_state_docs} ({purchase_state_docs/total_docs*100:.1f}%)")
        
        # ë‚´ìš© íƒ€ì…ë³„ ë¶„í¬
        content_types = {}
        for doc in documents:
            content_type = doc.metadata.get('content_type', 'unknown')
            content_types[content_type] = content_types.get(content_type, 0) + 1
        
        print(f"\nğŸ“‚ ë‚´ìš© íƒ€ì…ë³„ ë¶„í¬:")
        for content_type, count in content_types.items():
            print(f"  - {content_type}: {count}ê°œ ({count/total_docs*100:.1f}%)")
    
    @staticmethod
    def print_document_metadata(doc: Document, show_content: bool = False):
        """ê°œë³„ ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ì¶œë ¥"""
        print(f"ğŸ“„ ë¬¸ì„œ ë©”íƒ€ë°ì´í„°:")
        print(f"  - ì„¹ì…˜: {doc.metadata.get('section_name', 'N/A')}")
        print(f"  - ë‚´ìš© íƒ€ì…: {doc.metadata.get('content_type', 'N/A')}")
        print(f"  - PNS ê´€ë ¨: {doc.metadata.get('contains_pns', False)}")
        print(f"  - purchaseState í¬í•¨: {doc.metadata.get('contains_purchasestate', False)}")
        print(f"  - ì™„ì „í•œ ë©”ì‹œì§€ ê·œê²©: {doc.metadata.get('is_complete_spec', False)}")
        print(f"  - ì²­í¬ í¬ê¸°: {doc.metadata.get('chunk_size', 'N/A')}")
        
        if show_content:
            print(f"  - ë‚´ìš© ë¯¸ë¦¬ë³´ê¸°: {doc.page_content[:100]}...")


# ì‚¬ìš© ì˜ˆì‹œ
def demonstrate_metadata_utilization():
    """ë©”íƒ€ë°ì´í„° í™œìš© ë°ëª¨"""
    print("ğŸš€ ë©”íƒ€ë°ì´í„° í™œìš© ë°ëª¨")
    print("=" * 50)
    
    # ìƒ˜í”Œ ë¬¸ì„œ ìƒì„±
    sample_docs = [
        Document(
            page_content="PNS Payment Notification ë©”ì‹œì§€ì˜ purchaseState í•„ë“œëŠ” COMPLETED ë˜ëŠ” CANCELED ê°’ì„ ê°€ì§‘ë‹ˆë‹¤.",
            metadata={
                'section_name': 'PNS ë©”ì‹œì§€ ê·œê²©',
                'content_type': 'purchase_state_info',
                'contains_pns': True,
                'contains_purchasestate': True,
                'is_complete_spec': False
            }
        ),
        Document(
            page_content="| Element Name | Data Type | Description |\n| purchaseState | String | COMPLETED: ê²°ì œì™„ë£Œ / CANCELED: ì·¨ì†Œ |",
            metadata={
                'section_name': 'PNS ë©”ì‹œì§€ ê·œê²©',
                'content_type': 'message_specification',
                'contains_pns': True,
                'contains_purchasestate': True,
                'is_complete_spec': True
            }
        )
    ]
    
    # ë©”íƒ€ë°ì´í„° ë¶„ì„ê¸° ì‚¬ìš©
    analyzer = MetadataAnalyzer()
    for i, doc in enumerate(sample_docs):
        print(f"\nğŸ“Š ë¬¸ì„œ {i+1} ë¶„ì„:")
        analysis = analyzer.analyze_document_metadata(doc)
        print(f"  - ë‚´ìš© íƒ€ì…: {analysis['content_type']}")
        print(f"  - ìš°ì„ ìˆœìœ„: {analysis['priority_level']}")
        print(f"  - ì™„ì„±ë„ ì ìˆ˜: {analysis['completeness_score']:.2f}")
        print(f"  - ë¶€ìŠ¤íŠ¸ íŒ©í„°: {analysis['search_boost_factors']}")
    
    # ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ê²€ìƒ‰ê¸° ì‚¬ìš©
    retriever = MetadataBasedRetriever(sample_docs)
    query = "PNS ë©”ì‹œì§€ì˜ purchaseState ê°’ì€ ë¬´ì—‡ì´ ìˆë‚˜ìš”?"
    
    print(f"\nğŸ” ì§ˆì˜: {query}")
    search_criteria = {'boost_factors': ['purchase_state_related', 'complete_specification']}
    results = retriever.search_by_metadata(query, search_criteria)
    
    for score, doc in results:
        print(f"  - ì ìˆ˜: {score:.2f}, ë‚´ìš©: {doc.page_content[:50]}...")
    
    # ë©”íƒ€ë°ì´í„° ì‹œê°í™”
    print(f"\nğŸ“Š ë©”íƒ€ë°ì´í„° ìš”ì•½:")
    MetadataVisualizer.print_metadata_summary(sample_docs)


if __name__ == "__main__":
    demonstrate_metadata_utilization()
