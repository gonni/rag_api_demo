{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "da8ab2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Hierarchical Document Splitter for Technical Documentation\n",
    "계층적 기술 문서 분할기\n",
    "\n",
    "특징:\n",
    "1. 출처 URL 기반 1차 분할 (각 페이지별 독립성 보장)\n",
    "2. 헤더 기반 계층적 분할 (# → ## → ### → ####)\n",
    "3. 테이블과 코드 블록의 구조적 보존\n",
    "4. 섹션별 컨텍스트 메타데이터 강화\n",
    "5. 전문용어 추출 및 태깅\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "from typing import List, Dict, Tuple\n",
    "from dataclasses import dataclass\n",
    "from langchain.docstore.document import Document\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class DocumentSection:\n",
    "    \"\"\"문서 섹션 정보\"\"\"\n",
    "    content: str\n",
    "    header_path: List[str]  # [\"h1\", \"h2\", \"h3\", \"h4\"] 계층\n",
    "    level: int  # 헤더 레벨 (1-4)\n",
    "    source_url: str\n",
    "    section_id: str\n",
    "    start_pos: int\n",
    "    end_pos: int\n",
    "\n",
    "\n",
    "class TechnicalTermExtractor:\n",
    "    \"\"\"기술 문서 전용 용어 추출기\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.patterns = {\n",
    "            'api_codes': re.compile(r'\\b[A-Z]{2,}[A-Z0-9_]*\\b'),  # API, SDK, PNS 등\n",
    "            'camel_case': re.compile(r'\\b[a-z]+[A-Z][a-zA-Z0-9]*\\b'),  # purchaseState 등\n",
    "            'snake_case': re.compile(r'\\b[a-z]+_[a-z0-9_]+\\b'),  # query_purchases 등\n",
    "            'version_codes': re.compile(r'\\b[Vv]\\d+(\\.\\d+)*\\b'),  # V21, v1.0 등\n",
    "            'http_codes': re.compile(r'\\b[1-5]\\d{2}\\b'),  # 200, 404 등\n",
    "            'method_names': re.compile(r'\\b\\w+\\(\\)\\b'),  # launchPurchaseFlow() 등\n",
    "            'class_names': re.compile(r'\\b[A-Z][a-zA-Z0-9]*(?:Client|Listener|Params|Data|Result)\\b'),\n",
    "            'korean_tech': re.compile(r'(?:인앱|결제|구매|구독|월정액|상품|토큰|라이선스)'),\n",
    "        }\n",
    "        \n",
    "        # 제외할 일반적인 단어들 (노이즈 감소)\n",
    "        self.exclude_terms = {\n",
    "            'IF', 'OR', 'TO', 'IS', 'ON', 'IN', 'AT', 'BY', 'UP', 'NO', 'OK',\n",
    "            'GET', 'SET', 'PUT', 'POST', 'NEW', 'OLD', 'MAX', 'MIN', 'END'\n",
    "        }\n",
    "    \n",
    "    def extract_terms(self, text: str) -> Dict[str, List[str]]:\n",
    "        \"\"\"텍스트에서 기술 용어들을 카테고리별로 추출\"\"\"\n",
    "        terms = {}\n",
    "        for category, pattern in self.patterns.items():\n",
    "            matches = pattern.findall(text)\n",
    "            # 중복 제거 및 제외 단어 필터링\n",
    "            unique_matches = list(set(match for match in matches \n",
    "                                   if match.upper() not in self.exclude_terms))\n",
    "            if unique_matches:\n",
    "                terms[category] = unique_matches\n",
    "        return terms\n",
    "\n",
    "\n",
    "class HierarchicalSplitter:\n",
    "    \"\"\"계층적 문서 분할기\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 chunk_size: int = 1000,\n",
    "                 overlap_ratio: float = 0.1,\n",
    "                 preserve_tables: bool = True,\n",
    "                 preserve_code: bool = True):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.overlap_size = int(chunk_size * overlap_ratio)\n",
    "        self.preserve_tables = preserve_tables\n",
    "        self.preserve_code = preserve_code\n",
    "        self.term_extractor = TechnicalTermExtractor()\n",
    "        \n",
    "        # 출처 URL 패턴\n",
    "        self.source_pattern = re.compile(r'^출처:\\s*(https?://[^\\s]+)', re.MULTILINE)\n",
    "        \n",
    "        # 헤더 패턴 (마크다운)\n",
    "        self.header_patterns = {\n",
    "            1: re.compile(r'^#\\s+(.+)$', re.MULTILINE),\n",
    "            2: re.compile(r'^##\\s+(.+)$', re.MULTILINE),\n",
    "            3: re.compile(r'^###\\s+(.+)$', re.MULTILINE),\n",
    "            4: re.compile(r'^####\\s+(.+)$', re.MULTILINE),\n",
    "        }\n",
    "        \n",
    "        # 테이블 패턴\n",
    "        self.table_pattern = re.compile(r'^\\|.+\\|$', re.MULTILINE)\n",
    "        \n",
    "        # 코드 블록 패턴\n",
    "        self.code_pattern = re.compile(r'```(\\w+)?\\n(.*?)\\n```', re.DOTALL)\n",
    "    \n",
    "    def split_by_source(self, content: str) -> List[Tuple[str, str]]:\n",
    "        \"\"\"출처 URL 기준으로 문서를 1차 분할\"\"\"\n",
    "        sources = list(self.source_pattern.finditer(content))\n",
    "        sections = []\n",
    "        \n",
    "        for i, source_match in enumerate(sources):\n",
    "            start_pos = source_match.start()\n",
    "            end_pos = sources[i + 1].start() if i + 1 < len(sources) else len(content)\n",
    "            \n",
    "            source_url = source_match.group(1)\n",
    "            section_content = content[start_pos:end_pos].strip()\n",
    "            sections.append((source_url, section_content))\n",
    "        \n",
    "        return sections\n",
    "    \n",
    "    def extract_headers(self, content: str) -> List[Dict]:\n",
    "        \"\"\"텍스트에서 헤더 정보 추출\"\"\"\n",
    "        headers = []\n",
    "        \n",
    "        for level in range(1, 5):  # h1 ~ h4\n",
    "            pattern = self.header_patterns[level]\n",
    "            for match in pattern.finditer(content):\n",
    "                headers.append({\n",
    "                    'level': level,\n",
    "                    'title': match.group(1).strip(),\n",
    "                    'position': match.start(),\n",
    "                    'end_position': match.end()\n",
    "                })\n",
    "        \n",
    "        # 위치 순으로 정렬\n",
    "        headers.sort(key=lambda x: x['position'])\n",
    "        return headers\n",
    "    \n",
    "    def build_hierarchy_path(self, headers: List[Dict], current_pos: int) -> List[str]:\n",
    "        \"\"\"현재 위치까지의 헤더 계층 경로 구성\"\"\"\n",
    "        path = [''] * 4  # h1, h2, h3, h4\n",
    "        \n",
    "        for header in headers:\n",
    "            if header['position'] > current_pos:\n",
    "                break\n",
    "            level = header['level']\n",
    "            path[level - 1] = header['title']\n",
    "            # 하위 레벨 초기화\n",
    "            for i in range(level, 4):\n",
    "                if i > level - 1:\n",
    "                    path[i] = ''\n",
    "        \n",
    "        # 빈 문자열 제거하고 실제 경로만 반환\n",
    "        return [p for p in path if p]\n",
    "    \n",
    "    def extract_tables(self, content: str) -> List[Dict]:\n",
    "        \"\"\"테이블 구조 추출 및 파싱\"\"\"\n",
    "        tables = []\n",
    "        lines = content.split('\\n')\n",
    "        table_start = None\n",
    "        table_rows = []\n",
    "        \n",
    "        for i, line in enumerate(lines):\n",
    "            if self.table_pattern.match(line):\n",
    "                if table_start is None:\n",
    "                    table_start = i\n",
    "                table_rows.append(line)\n",
    "            else:\n",
    "                if table_start is not None:\n",
    "                    # 테이블 종료\n",
    "                    if len(table_rows) >= 2:  # 최소 헤더 + 데이터 1행\n",
    "                        tables.append({\n",
    "                            'start_line': table_start,\n",
    "                            'end_line': i,\n",
    "                            'rows': table_rows,\n",
    "                            'content': '\\n'.join(table_rows)\n",
    "                        })\n",
    "                    table_start = None\n",
    "                    table_rows = []\n",
    "        \n",
    "        # 마지막 테이블 처리\n",
    "        if table_start is not None and len(table_rows) >= 2:\n",
    "            tables.append({\n",
    "                'start_line': table_start,\n",
    "                'end_line': len(lines),\n",
    "                'rows': table_rows,\n",
    "                'content': '\\n'.join(table_rows)\n",
    "            })\n",
    "        \n",
    "        return tables\n",
    "    \n",
    "    def extract_code_blocks(self, content: str) -> List[Dict]:\n",
    "        \"\"\"코드 블록 추출\"\"\"\n",
    "        blocks = []\n",
    "        for match in self.code_pattern.finditer(content):\n",
    "            language = match.group(1) or 'text'\n",
    "            code_content = match.group(2)\n",
    "            blocks.append({\n",
    "                'language': language,\n",
    "                'content': code_content,\n",
    "                'full_match': match.group(0),\n",
    "                'start_pos': match.start(),\n",
    "                'end_pos': match.end()\n",
    "            })\n",
    "        return blocks\n",
    "    \n",
    "    def split_content_by_headers(self, content: str, source_url: str) -> List[DocumentSection]:\n",
    "        \"\"\"헤더 기준으로 내용을 계층적으로 분할\"\"\"\n",
    "        headers = self.extract_headers(content)\n",
    "        sections = []\n",
    "        \n",
    "        if not headers:\n",
    "            # 헤더가 없는 경우 전체를 하나의 섹션으로\n",
    "            section = DocumentSection(\n",
    "                content=content,\n",
    "                header_path=[],\n",
    "                level=0,\n",
    "                source_url=source_url,\n",
    "                section_id=f\"section_0\",\n",
    "                start_pos=0,\n",
    "                end_pos=len(content)\n",
    "            )\n",
    "            sections.append(section)\n",
    "            return sections\n",
    "        \n",
    "        # 첫 번째 헤더 이전 내용\n",
    "        if headers[0]['position'] > 0:\n",
    "            intro_content = content[:headers[0]['position']].strip()\n",
    "            if intro_content:\n",
    "                section = DocumentSection(\n",
    "                    content=intro_content,\n",
    "                    header_path=[],\n",
    "                    level=0,\n",
    "                    source_url=source_url,\n",
    "                    section_id=f\"section_intro\",\n",
    "                    start_pos=0,\n",
    "                    end_pos=headers[0]['position']\n",
    "                )\n",
    "                sections.append(section)\n",
    "        \n",
    "        # 헤더별 섹션 분할\n",
    "        for i, header in enumerate(headers):\n",
    "            start_pos = header['end_position']\n",
    "            end_pos = headers[i + 1]['position'] if i + 1 < len(headers) else len(content)\n",
    "            \n",
    "            section_content = content[start_pos:end_pos].strip()\n",
    "            if not section_content:\n",
    "                continue\n",
    "                \n",
    "            hierarchy_path = self.build_hierarchy_path(headers, header['position'])\n",
    "            \n",
    "            section = DocumentSection(\n",
    "                content=f\"# {header['title']}\\n\\n{section_content}\",\n",
    "                header_path=hierarchy_path,\n",
    "                level=header['level'],\n",
    "                source_url=source_url,\n",
    "                section_id=f\"section_{i}\",\n",
    "                start_pos=start_pos,\n",
    "                end_pos=end_pos\n",
    "            )\n",
    "            sections.append(section)\n",
    "        \n",
    "        return sections\n",
    "    \n",
    "    def chunk_large_section(self, section: DocumentSection) -> List[DocumentSection]:\n",
    "        \"\"\"큰 섹션을 청크로 분할\"\"\"\n",
    "        if len(section.content) <= self.chunk_size:\n",
    "            return [section]\n",
    "        \n",
    "        chunks = []\n",
    "        content = section.content\n",
    "        start_idx = 0\n",
    "        chunk_id = 0\n",
    "        \n",
    "        while start_idx < len(content):\n",
    "            end_idx = start_idx + self.chunk_size\n",
    "            \n",
    "            # 자연스러운 분할점 찾기 (문단, 문장 경계)\n",
    "            if end_idx < len(content):\n",
    "                # 문단 경계 찾기\n",
    "                para_break = content.rfind('\\n\\n', start_idx, end_idx)\n",
    "                if para_break > start_idx:\n",
    "                    end_idx = para_break + 2\n",
    "                else:\n",
    "                    # 문장 경계 찾기\n",
    "                    sent_break = content.rfind('. ', start_idx, end_idx)\n",
    "                    if sent_break > start_idx:\n",
    "                        end_idx = sent_break + 2\n",
    "            \n",
    "            chunk_content = content[start_idx:end_idx]\n",
    "            \n",
    "            chunk_section = DocumentSection(\n",
    "                content=chunk_content,\n",
    "                header_path=section.header_path,\n",
    "                level=section.level,\n",
    "                source_url=section.source_url,\n",
    "                section_id=f\"{section.section_id}_chunk_{chunk_id}\",\n",
    "                start_pos=section.start_pos + start_idx,\n",
    "                end_pos=section.start_pos + end_idx\n",
    "            )\n",
    "            chunks.append(chunk_section)\n",
    "            \n",
    "            # 다음 청크 시작점 (오버랩 적용)\n",
    "            start_idx = max(start_idx + 1, end_idx - self.overlap_size)\n",
    "            chunk_id += 1\n",
    "        \n",
    "        return chunks\n",
    "    \n",
    "    def create_document_with_metadata(self, section: DocumentSection) -> Document:\n",
    "        \"\"\"DocumentSection을 LangChain Document로 변환 (메타데이터 포함)\"\"\"\n",
    "        \n",
    "        # 기술 용어 추출\n",
    "        tech_terms = self.term_extractor.extract_terms(section.content)\n",
    "        \n",
    "        # 테이블 및 코드 블록 추출\n",
    "        tables = self.extract_tables(section.content)\n",
    "        code_blocks = self.extract_code_blocks(section.content)\n",
    "        \n",
    "        # 컨텐츠 타입 결정\n",
    "        content_types = []\n",
    "        if tables:\n",
    "            content_types.append('table')\n",
    "        if code_blocks:\n",
    "            content_types.append('code')\n",
    "        if not content_types:\n",
    "            content_types.append('text')\n",
    "        \n",
    "        # 메타데이터 구성\n",
    "        metadata = {\n",
    "            'source_url': section.source_url,\n",
    "            'header_path': section.header_path,\n",
    "            'section_hierarchy': ' > '.join(section.header_path),\n",
    "            'header_level': section.level,\n",
    "            'section_id': section.section_id,\n",
    "            'content_types': content_types,\n",
    "            'technical_terms': tech_terms,\n",
    "            'has_tables': len(tables) > 0,\n",
    "            'has_code': len(code_blocks) > 0,\n",
    "            'code_languages': [cb['language'] for cb in code_blocks],\n",
    "            'table_count': len(tables),\n",
    "            'code_block_count': len(code_blocks),\n",
    "            'content_length': len(section.content),\n",
    "            'start_pos': section.start_pos,\n",
    "            'end_pos': section.end_pos\n",
    "        }\n",
    "        \n",
    "        # 헤더 경로를 콘텐츠에 포함 (검색 개선)\n",
    "        content_with_context = section.content\n",
    "        if section.header_path:\n",
    "            context_header = f\"[{' > '.join(section.header_path)}]\\n\\n\"\n",
    "            content_with_context = context_header + content_with_context\n",
    "        \n",
    "        return Document(\n",
    "            page_content=content_with_context,\n",
    "            metadata=metadata\n",
    "        )\n",
    "    \n",
    "    def split_document(self, content: str) -> List[Document]:\n",
    "        \"\"\"메인 분할 메소드\"\"\"\n",
    "        documents = []\n",
    "        \n",
    "        # 1단계: 출처 URL 기준으로 분할\n",
    "        source_sections = self.split_by_source(content)\n",
    "        \n",
    "        for source_url, section_content in source_sections:\n",
    "            # 2단계: 헤더 기준으로 계층적 분할\n",
    "            doc_sections = self.split_content_by_headers(section_content, source_url)\n",
    "            \n",
    "            for section in doc_sections:\n",
    "                # 3단계: 큰 섹션은 청크로 분할\n",
    "                chunks = self.chunk_large_section(section)\n",
    "                \n",
    "                # 4단계: Document 객체 생성\n",
    "                for chunk in chunks:\n",
    "                    doc = self.create_document_with_metadata(chunk)\n",
    "                    documents.append(doc)\n",
    "        \n",
    "        return documents\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "697ed7db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "총 8개의 문서로 분할됨\n",
      "\n",
      "=== 문서 1 ===\n",
      "내용: 출처: https://onestore-dev.gitbook.io/dev/tools/billing/v21.md...\n",
      "출처: https://onestore-dev.gitbook.io/dev/tools/billing/v21.md\n",
      "계층: \n",
      "타입: ['text']\n",
      "기술용어: {'version_codes': ['']}\n",
      "\n",
      "=== 문서 2 ===\n",
      "내용: [원스토어 인앱결제 API V7(SDK V21) 연동 안내]\n",
      "\n",
      "# 원스토어 인앱결제 API V7(SDK V21) 연동 안내\n",
      "\n",
      "원스토어의 최신 인앱결제 API V7(SDK V21)이...\n",
      "출처: https://onestore-dev.gitbook.io/dev/tools/billing/v21.md\n",
      "계층: 원스토어 인앱결제 API V7(SDK V21) 연동 안내\n",
      "타입: ['text']\n",
      "기술용어: {'api_codes': ['API', 'SDK'], 'version_codes': [''], 'korean_tech': ['인앱', '결제']}\n",
      "\n",
      "=== 문서 3 ===\n",
      "내용: [원스토어 인앱결제 API V7(SDK V21) 연동 안내 > 개요]\n",
      "\n",
      "# 개요\n",
      "\n",
      "원스토어 인앱결제(IAP)는 안드로이드 앱 내에서 상품을 판매하는 서비스입니다....\n",
      "출처: https://onestore-dev.gitbook.io/dev/tools/billing/v21.md\n",
      "계층: 원스토어 인앱결제 API V7(SDK V21) 연동 안내 > 개요\n",
      "타입: ['text']\n",
      "기술용어: {'api_codes': ['IAP'], 'korean_tech': ['인앱', '상품', '결제']}\n",
      "\n",
      "=== 문서 4 ===\n",
      "내용: [원스토어 인앱결제 API V7(SDK V21) 연동 안내 > 개요 > PurchaseClient 초기화]\n",
      "\n",
      "# PurchaseClient 초기화\n",
      "\n",
      "```kotlin\n",
      "val pur...\n",
      "출처: https://onestore-dev.gitbook.io/dev/tools/billing/v21.md\n",
      "계층: 원스토어 인앱결제 API V7(SDK V21) 연동 안내 > 개요 > PurchaseClient 초기화\n",
      "타입: ['code']\n",
      "기술용어: {'camel_case': ['getClient', 'purchaseClient'], 'class_names': ['PurchaseClient']}\n",
      "\n",
      "=== 문서 5 ===\n",
      "내용: [원스토어 인앱결제 API V7(SDK V21) 연동 안내 > 개요 > 상품 구매 요청]\n",
      "\n",
      "# 상품 구매 요청\n",
      "\n",
      "launchPurchaseFlow() 메소드를 사용합니다.\n",
      "\n",
      "| 파...\n",
      "출처: https://onestore-dev.gitbook.io/dev/tools/billing/v21.md\n",
      "계층: 원스토어 인앱결제 API V7(SDK V21) 연동 안내 > 개요 > 상품 구매 요청\n",
      "타입: ['table']\n",
      "기술용어: {'api_codes': ['ID'], 'camel_case': ['launchPurchaseFlow', 'productId', 'purchaseParams'], 'class_names': ['PurchaseParams'], 'korean_tech': ['구매', '상품']}\n",
      "\n",
      "=== 문서 6 ===\n",
      "내용: 출처: https://onestore-dev.gitbook.io/dev/tools/billing/v21/pns.md...\n",
      "출처: https://onestore-dev.gitbook.io/dev/tools/billing/v21/pns.md\n",
      "계층: \n",
      "타입: ['text']\n",
      "기술용어: {'version_codes': ['']}\n",
      "\n",
      "=== 문서 7 ===\n",
      "내용: [PNS(Payment Notification Service) 이용하기]\n",
      "\n",
      "# PNS(Payment Notification Service) 이용하기\n",
      "\n",
      "PNS는 결제 상태를 서버로 ...\n",
      "출처: https://onestore-dev.gitbook.io/dev/tools/billing/v21/pns.md\n",
      "계층: PNS(Payment Notification Service) 이용하기\n",
      "타입: ['text']\n",
      "기술용어: {'api_codes': ['PNS'], 'korean_tech': ['결제']}\n",
      "\n",
      "=== 문서 8 ===\n",
      "내용: [PNS(Payment Notification Service) 이용하기 > PNS 설정]\n",
      "\n",
      "# PNS 설정\n",
      "\n",
      "개발사 서버에서 API를 구현해야 합니다....\n",
      "출처: https://onestore-dev.gitbook.io/dev/tools/billing/v21/pns.md\n",
      "계층: PNS(Payment Notification Service) 이용하기 > PNS 설정\n",
      "타입: ['text']\n",
      "기술용어: {'api_codes': ['PNS']}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## ONLY TEST ##\n",
    "\n",
    "# 사용 예제 및 테스트 함수\n",
    "def test_splitter():\n",
    "    \"\"\"분할기 테스트\"\"\"\n",
    "    \n",
    "    # 샘플 텍스트 (실제 문서 구조 모방)\n",
    "    sample_content = \"\"\"\n",
    "출처: https://onestore-dev.gitbook.io/dev/tools/billing/v21.md\n",
    "# 원스토어 인앱결제 API V7(SDK V21) 연동 안내\n",
    "\n",
    "원스토어의 최신 인앱결제 API V7(SDK V21)이 출시되었습니다.\n",
    "\n",
    "## 개요\n",
    "\n",
    "원스토어 인앱결제(IAP)는 안드로이드 앱 내에서 상품을 판매하는 서비스입니다.\n",
    "\n",
    "### PurchaseClient 초기화\n",
    "\n",
    "```kotlin\n",
    "val purchaseClient = PurchaseClient.getClient(activity)\n",
    "```\n",
    "\n",
    "### 상품 구매 요청\n",
    "\n",
    "launchPurchaseFlow() 메소드를 사용합니다.\n",
    "\n",
    "| 파라미터 | 타입 | 설명 |\n",
    "|---------|-----|-----|\n",
    "| productId | String | 상품 ID |\n",
    "| purchaseParams | PurchaseParams | 구매 파라미터 |\n",
    "\n",
    "출처: https://onestore-dev.gitbook.io/dev/tools/billing/v21/pns.md\n",
    "# PNS(Payment Notification Service) 이용하기\n",
    "\n",
    "PNS는 결제 상태를 서버로 전송하는 서비스입니다.\n",
    "\n",
    "## PNS 설정\n",
    "\n",
    "개발사 서버에서 API를 구현해야 합니다.\n",
    "\"\"\"\n",
    "    \n",
    "    splitter = HierarchicalSplitter(chunk_size=500)\n",
    "    documents = splitter.split_document(sample_content)\n",
    "    \n",
    "    print(f\"총 {len(documents)}개의 문서로 분할됨\\n\")\n",
    "    \n",
    "    for i, doc in enumerate(documents):\n",
    "        print(f\"=== 문서 {i+1} ===\")\n",
    "        print(f\"내용: {doc.page_content[:100]}...\")\n",
    "        print(f\"출처: {doc.metadata['source_url']}\")\n",
    "        print(f\"계층: {doc.metadata['section_hierarchy']}\")\n",
    "        print(f\"타입: {doc.metadata['content_types']}\")\n",
    "        if doc.metadata['technical_terms']:\n",
    "            print(f\"기술용어: {doc.metadata['technical_terms']}\")\n",
    "        print()\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     test_splitter()\n",
    "\n",
    "test_splitter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "010cc4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Context-Aware Retriever for Technical Documentation\n",
    "컨텍스트 인식 기술문서 검색기\n",
    "\n",
    "특징:\n",
    "1. 전문용어의 맥락적 의미 고려\n",
    "2. 섹션별 컨텍스트 보존\n",
    "3. 다단계 리랭킹 (관련성, 맥락, 기술정확성)\n",
    "4. 쿼리 확장 (동의어, 약어)\n",
    "5. 노이즈 필터링 (중요도 기반)\n",
    "\"\"\"\n",
    "\n",
    "import re\n",
    "from typing import List, Dict, Any, Tuple, Set\n",
    "from dataclasses import dataclass\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.schema import BaseRetriever\n",
    "from langchain_core.callbacks.manager import CallbackManagerForRetrieverRun\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class SearchContext:\n",
    "    \"\"\"검색 컨텍스트 정보\"\"\"\n",
    "    query: str\n",
    "    query_terms: List[str]\n",
    "    tech_terms: Dict[str, List[str]]\n",
    "    query_type: str  # 'how_to', 'what_is', 'api_spec', 'error_debug'\n",
    "    context_keywords: Set[str]\n",
    "\n",
    "\n",
    "class QueryAnalyzer:\n",
    "    \"\"\"쿼리 분석 및 확장\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # 기술 용어 동의어 사전\n",
    "        self.tech_synonyms = {\n",
    "            'IAP': ['인앱결제', '인앱', '결제'],\n",
    "            'SDK': ['소프트웨어개발키트', '개발킷'],\n",
    "            'API': ['인터페이스', '연동'],\n",
    "            'PNS': ['결제알림서비스', 'Payment Notification Service'],\n",
    "            'PurchaseClient': ['구매클라이언트', '결제클라이언트'],\n",
    "            'purchaseState': ['구매상태', '결제상태'],\n",
    "            'purchaseToken': ['구매토큰', '결제토큰'],\n",
    "            'acknowledge': ['구매확인', '승인'],\n",
    "            'consume': ['소비', '사용완료'],\n",
    "            'subscription': ['구독', '정기결제'],\n",
    "            'recurring': ['월정액', '자동결제', '반복결제']\n",
    "        }\n",
    "        \n",
    "        # 쿼리 타입 패턴\n",
    "        self.query_patterns = {\n",
    "            'how_to': re.compile(r'(?:어떻게|방법|사용|구현|적용|설정|연동)'),\n",
    "            'what_is': re.compile(r'(?:무엇|뭔가요|란|이란|설명|개념)'),\n",
    "            'api_spec': re.compile(r'(?:파라미터|리턴|응답|요청|스펙|명세)'),\n",
    "            'error_debug': re.compile(r'(?:에러|오류|문제|실패|안됨|해결)')\n",
    "        }\n",
    "        \n",
    "        # 한국어 조사 패턴\n",
    "        self.particle_pattern = re.compile(r'([가-힣]+)(?:은|는|이|가|을|를|의|에|와|과|도|로|으로|부터|까지|만)')\n",
    "    \n",
    "    def expand_query(self, query: str) -> List[str]:\n",
    "        \"\"\"쿼리 확장 (동의어, 약어)\"\"\"\n",
    "        expanded = [query]\n",
    "        \n",
    "        for term, synonyms in self.tech_synonyms.items():\n",
    "            if term.lower() in query.lower():\n",
    "                for synonym in synonyms:\n",
    "                    expanded.append(query.replace(term, synonym))\n",
    "            else:\n",
    "                for synonym in synonyms:\n",
    "                    if synonym in query:\n",
    "                        expanded.append(query.replace(synonym, term))\n",
    "        \n",
    "        return list(set(expanded))\n",
    "    \n",
    "    def extract_query_terms(self, query: str) -> List[str]:\n",
    "        \"\"\"쿼리에서 핵심 용어 추출\"\"\"\n",
    "        # 조사 제거\n",
    "        query_cleaned = self.particle_pattern.sub(r'\\1', query)\n",
    "        \n",
    "        terms = []\n",
    "        \n",
    "        # 기술 용어 추출\n",
    "        for term in self.tech_synonyms.keys():\n",
    "            if term.lower() in query_cleaned.lower():\n",
    "                terms.append(term)\n",
    "        \n",
    "        # 영어 용어 추출\n",
    "        english_terms = re.findall(r'\\b[A-Za-z][A-Za-z0-9_]*\\b', query_cleaned)\n",
    "        terms.extend([t for t in english_terms if len(t) > 2])\n",
    "        \n",
    "        # 한글 명사 추출 (간단한 휴리스틱)\n",
    "        korean_terms = re.findall(r'[가-힣]{2,}', query_cleaned)\n",
    "        terms.extend(korean_terms)\n",
    "        \n",
    "        return list(set(terms))\n",
    "    \n",
    "    def classify_query_type(self, query: str) -> str:\n",
    "        \"\"\"쿼리 타입 분류\"\"\"\n",
    "        for query_type, pattern in self.query_patterns.items():\n",
    "            if pattern.search(query):\n",
    "                return query_type\n",
    "        return 'general'\n",
    "    \n",
    "    def analyze_query(self, query: str) -> SearchContext:\n",
    "        \"\"\"쿼리 종합 분석\"\"\"\n",
    "        terms = self.extract_query_terms(query)\n",
    "        query_type = self.classify_query_type(query)\n",
    "        \n",
    "        # 기술 용어 분류\n",
    "        tech_terms: Dict[str, List[str]] = {\n",
    "            'api_terms': [],\n",
    "            'method_terms': [],\n",
    "            'concept_terms': []\n",
    "        }\n",
    "        \n",
    "        for term in terms:\n",
    "            if term in self.tech_synonyms or term.upper() in self.tech_synonyms:\n",
    "                tech_terms['api_terms'].append(term)\n",
    "            elif '()' in term or 'Client' in term or 'Listener' in term:\n",
    "                tech_terms['method_terms'].append(term)\n",
    "            else:\n",
    "                tech_terms['concept_terms'].append(term)\n",
    "        \n",
    "        context_keywords = set(terms)\n",
    "        \n",
    "        return SearchContext(\n",
    "            query=query,\n",
    "            query_terms=terms,\n",
    "            tech_terms=tech_terms,\n",
    "            query_type=query_type,\n",
    "            context_keywords=context_keywords\n",
    "        )\n",
    "\n",
    "\n",
    "class ContextScorer:\n",
    "    \"\"\"문맥 기반 점수 계산\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # 섹션 타입별 가중치\n",
    "        self.section_weights = {\n",
    "            'api_spec': {'how_to': 0.8, 'what_is': 0.6, 'api_spec': 1.0, 'error_debug': 0.7},\n",
    "            'code': {'how_to': 1.0, 'what_is': 0.5, 'api_spec': 0.8, 'error_debug': 0.9},\n",
    "            'table': {'how_to': 0.7, 'what_is': 0.8, 'api_spec': 1.0, 'error_debug': 0.6},\n",
    "            'text': {'how_to': 0.9, 'what_is': 1.0, 'api_spec': 0.7, 'error_debug': 0.8}\n",
    "        }\n",
    "        \n",
    "        # 기술 용어 가중치\n",
    "        self.term_weights = {\n",
    "            'api_terms': 1.0,\n",
    "            'method_terms': 0.9,\n",
    "            'concept_terms': 0.7\n",
    "        }\n",
    "    \n",
    "    def calculate_term_relevance(self, doc: Document, search_context: SearchContext) -> float:\n",
    "        \"\"\"용어 관련성 점수\"\"\"\n",
    "        content = doc.page_content.lower()\n",
    "        metadata = doc.metadata\n",
    "        \n",
    "        score = 0.0\n",
    "        total_weight = 0.0\n",
    "        \n",
    "        # 쿼리 용어 매칭\n",
    "        for term in search_context.query_terms:\n",
    "            if term.lower() in content:\n",
    "                score += 1.0\n",
    "                total_weight += 1.0\n",
    "        \n",
    "        # 기술 용어 매칭 (가중치 적용)\n",
    "        for term_type, terms in search_context.tech_terms.items():\n",
    "            weight = self.term_weights.get(term_type, 0.5)\n",
    "            for term in terms:\n",
    "                if term.lower() in content:\n",
    "                    score += weight\n",
    "                    total_weight += weight\n",
    "        \n",
    "        # 메타데이터의 기술 용어 매칭\n",
    "        doc_tech_terms = metadata.get('technical_terms', {})\n",
    "        for term_type, terms in doc_tech_terms.items():\n",
    "            for term in terms:\n",
    "                if term.lower() in search_context.query.lower():\n",
    "                    score += 0.5\n",
    "                    total_weight += 0.5\n",
    "        \n",
    "        return score / max(total_weight, 1.0)\n",
    "    \n",
    "    def calculate_context_relevance(self, doc: Document, search_context: SearchContext) -> float:\n",
    "        \"\"\"문맥 관련성 점수\"\"\"\n",
    "        metadata = doc.metadata\n",
    "        content_types = metadata.get('content_types', ['text'])\n",
    "        query_type = search_context.query_type\n",
    "        \n",
    "        # 컨텐츠 타입과 쿼리 타입 매칭\n",
    "        type_score = 0.0\n",
    "        for content_type in content_types:\n",
    "            if content_type in self.section_weights:\n",
    "                type_score += self.section_weights[content_type].get(query_type, 0.5)\n",
    "        \n",
    "        type_score = type_score / len(content_types)\n",
    "        \n",
    "        # 섹션 계층 관련성\n",
    "        hierarchy = metadata.get('section_hierarchy', '')\n",
    "        hierarchy_score = 0.0\n",
    "        \n",
    "        for keyword in search_context.context_keywords:\n",
    "            if keyword.lower() in hierarchy.lower():\n",
    "                hierarchy_score += 0.2\n",
    "        \n",
    "        hierarchy_score = min(hierarchy_score, 1.0)\n",
    "        \n",
    "        return (type_score + hierarchy_score) / 2.0\n",
    "    \n",
    "    def calculate_technical_accuracy(self, doc: Document, search_context: SearchContext) -> float:\n",
    "        \"\"\"기술적 정확성 점수\"\"\"\n",
    "        metadata = doc.metadata\n",
    "        content = doc.page_content.lower()\n",
    "        \n",
    "        # 코드 블록과 테이블의 가중치\n",
    "        accuracy_score = 0.5  # 기본 점수\n",
    "        \n",
    "        if metadata.get('has_code'):\n",
    "            accuracy_score += 0.2\n",
    "        \n",
    "        if metadata.get('has_tables'):\n",
    "            accuracy_score += 0.15\n",
    "        \n",
    "        # API 관련 키워드 밀도\n",
    "        api_keywords = ['api', 'sdk', 'client', 'params', 'response', 'request']\n",
    "        keyword_count = sum(1 for kw in api_keywords if kw in content)\n",
    "        accuracy_score += min(keyword_count * 0.05, 0.25)\n",
    "        \n",
    "        return min(accuracy_score, 1.0)\n",
    "\n",
    "\n",
    "class NoiseFilter:\n",
    "    \"\"\"노이즈 필터링\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # 일반적인 노이즈 용어들\n",
    "        self.noise_terms = {\n",
    "            'common_words': {'the', 'and', 'or', 'is', 'are', 'was', 'were', 'for', 'to', 'in', 'on'},\n",
    "            'filler_korean': {'입니다', '있습니다', '합니다', '때문에', '그리고', '하지만', '또한'},\n",
    "            'generic_tech': {'data', 'info', 'value', 'result', 'success', 'error'}\n",
    "        }\n",
    "    \n",
    "    def calculate_noise_score(self, doc: Document, search_context: SearchContext) -> float:\n",
    "        \"\"\"노이즈 점수 계산 (낮을수록 좋음)\"\"\"\n",
    "        content = doc.page_content.lower()\n",
    "        content_length = len(content)\n",
    "        \n",
    "        if content_length == 0:\n",
    "            return 1.0\n",
    "        \n",
    "        noise_count = 0\n",
    "        total_terms = len(content.split())\n",
    "        \n",
    "        # 일반적인 노이즈 용어 카운트\n",
    "        for noise_category, terms in self.noise_terms.items():\n",
    "            for term in terms:\n",
    "                noise_count += content.count(term.lower())\n",
    "        \n",
    "        # 반복되는 문구 패널티\n",
    "        sentences = content.split('.')\n",
    "        unique_sentences = set(sentences)\n",
    "        repetition_penalty = 1 - (len(unique_sentences) / max(len(sentences), 1))\n",
    "        \n",
    "        # 노이즈 비율 계산\n",
    "        noise_ratio = noise_count / max(total_terms, 1)\n",
    "        total_noise_score = min((noise_ratio + repetition_penalty) / 2, 1.0)\n",
    "        \n",
    "        return total_noise_score\n",
    "    \n",
    "    def filter_by_relevance_threshold(self, docs: List[Document], \n",
    "                                    scores: List[float], \n",
    "                                    threshold: float = 0.3) -> Tuple[List[Document], List[float]]:\n",
    "        \"\"\"관련성 임계값으로 필터링\"\"\"\n",
    "        filtered_docs = []\n",
    "        filtered_scores = []\n",
    "        \n",
    "        for doc, score in zip(docs, scores):\n",
    "            if score >= threshold:\n",
    "                filtered_docs.append(doc)\n",
    "                filtered_scores.append(score)\n",
    "        \n",
    "        return filtered_docs, filtered_scores\n",
    "\n",
    "\n",
    "class ContextAwareRetriever(BaseRetriever):\n",
    "    \"\"\"컨텍스트 인식 검색기\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 base_retriever: BaseRetriever,\n",
    "                 rerank_top_k: int = 20,\n",
    "                 final_top_k: int = 5,\n",
    "                 enable_query_expansion: bool = True,\n",
    "                 noise_threshold: float = 0.7):\n",
    "        \n",
    "        self.base_retriever = base_retriever\n",
    "        self.rerank_top_k = rerank_top_k\n",
    "        self.final_top_k = final_top_k\n",
    "        self.enable_query_expansion = enable_query_expansion\n",
    "        self.noise_threshold = noise_threshold\n",
    "        \n",
    "        self.query_analyzer = QueryAnalyzer()\n",
    "        self.context_scorer = ContextScorer()\n",
    "        self.noise_filter = NoiseFilter()\n",
    "    \n",
    "    def _get_relevant_documents(self, \n",
    "                              query: str, \n",
    "                              *, \n",
    "                              run_manager: CallbackManagerForRetrieverRun) -> List[Document]:\n",
    "        \"\"\"메인 검색 메소드\"\"\"\n",
    "        \n",
    "        # 1. 쿼리 분석\n",
    "        search_context = self.query_analyzer.analyze_query(query)\n",
    "        \n",
    "        # 2. 쿼리 확장 (옵션)\n",
    "        queries_to_search = [query]\n",
    "        if self.enable_query_expansion:\n",
    "            expanded_queries = self.query_analyzer.expand_query(query)\n",
    "            queries_to_search.extend(expanded_queries[:3])  # 최대 3개 확장\n",
    "        \n",
    "        # 3. 기본 검색 수행\n",
    "        all_docs = []\n",
    "        for q in queries_to_search:\n",
    "            docs = self.base_retriever.get_relevant_documents(q, run_manager=run_manager)\n",
    "            all_docs.extend(docs)\n",
    "        \n",
    "        # 중복 제거 (page_content 기준)\n",
    "        unique_docs = []\n",
    "        seen_contents = set()\n",
    "        for doc in all_docs:\n",
    "            content_hash = hash(doc.page_content)\n",
    "            if content_hash not in seen_contents:\n",
    "                unique_docs.append(doc)\n",
    "                seen_contents.add(content_hash)\n",
    "        \n",
    "        # 상위 K개만 리랭킹 대상으로\n",
    "        docs_to_rerank = unique_docs[:self.rerank_top_k]\n",
    "        \n",
    "        # 4. 다단계 리랭킹\n",
    "        reranked_docs = self._rerank_documents(docs_to_rerank, search_context)\n",
    "        \n",
    "        # 5. 최종 결과 반환\n",
    "        return reranked_docs[:self.final_top_k]\n",
    "    \n",
    "    def _rerank_documents(self, docs: List[Document], search_context: SearchContext) -> List[Document]:\n",
    "        \"\"\"다단계 리랭킹\"\"\"\n",
    "        \n",
    "        scored_docs = []\n",
    "        \n",
    "        for doc in docs:\n",
    "            # 점수 계산\n",
    "            term_score = self.context_scorer.calculate_term_relevance(doc, search_context)\n",
    "            context_score = self.context_scorer.calculate_context_relevance(doc, search_context)\n",
    "            accuracy_score = self.context_scorer.calculate_technical_accuracy(doc, search_context)\n",
    "            noise_score = self.noise_filter.calculate_noise_score(doc, search_context)\n",
    "            \n",
    "            # 종합 점수 (노이즈는 패널티)\n",
    "            final_score = (term_score * 0.4 + \n",
    "                          context_score * 0.3 + \n",
    "                          accuracy_score * 0.2 + \n",
    "                          (1 - noise_score) * 0.1)\n",
    "            \n",
    "            scored_docs.append((doc, final_score))\n",
    "        \n",
    "        # 점수순 정렬\n",
    "        scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 노이즈 임계값으로 필터링\n",
    "        filtered_docs = []\n",
    "        for doc, score in scored_docs:\n",
    "            noise_score = self.noise_filter.calculate_noise_score(doc, search_context)\n",
    "            if noise_score <= self.noise_threshold:\n",
    "                filtered_docs.append(doc)\n",
    "        \n",
    "        return filtered_docs if filtered_docs else [doc for doc, _ in scored_docs]\n",
    "    \n",
    "    def get_search_analytics(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"검색 분석 정보 반환 (디버깅용)\"\"\"\n",
    "        search_context = self.query_analyzer.analyze_query(query)\n",
    "        \n",
    "        return {\n",
    "            'query_terms': search_context.query_terms,\n",
    "            'query_type': search_context.query_type,\n",
    "            'tech_terms': search_context.tech_terms,\n",
    "            'context_keywords': list(search_context.context_keywords)\n",
    "        }\n",
    "\n",
    "\n",
    "# 사용 예제\n",
    "def create_context_aware_retriever_example():\n",
    "    \"\"\"컨텍스트 인식 검색기 생성 예제\"\"\"\n",
    "    \n",
    "    # 가상의 기본 검색기 (실제로는 FAISS, BM25 등을 사용)\n",
    "    class MockRetriever(BaseRetriever):\n",
    "        def __init__(self, documents):\n",
    "            self.documents = documents\n",
    "        \n",
    "        def _get_relevant_documents(self, query, *, run_manager):\n",
    "            # 간단한 키워드 매칭 검색\n",
    "            results = []\n",
    "            for doc in self.documents:\n",
    "                if any(term.lower() in doc.page_content.lower() \n",
    "                      for term in query.split()):\n",
    "                    results.append(doc)\n",
    "            return results[:10]\n",
    "    \n",
    "    # 샘플 문서들\n",
    "    sample_docs = [\n",
    "        Document(\n",
    "            page_content=\"PurchaseClient를 사용하여 인앱결제를 구현하는 방법\",\n",
    "            metadata={'content_types': ['text'], 'technical_terms': {'api_terms': ['PurchaseClient']}}\n",
    "        ),\n",
    "        Document(\n",
    "            page_content=\"purchaseState 값으로 구매 상태를 확인할 수 있습니다\",\n",
    "            metadata={'content_types': ['text'], 'technical_terms': {'method_terms': ['purchaseState']}}\n",
    "        )\n",
    "    ]\n",
    "    \n",
    "    base_retriever = MockRetriever(sample_docs)\n",
    "    context_retriever = ContextAwareRetriever(base_retriever)\n",
    "    \n",
    "    return context_retriever\n",
    "\n",
    "\n",
    "# 테스트 함수\n",
    "def test_retriever():\n",
    "    \"\"\"검색기 테스트\"\"\"\n",
    "    retriever = create_context_aware_retriever_example()\n",
    "    \n",
    "    queries = [\n",
    "        \"PurchaseClient 사용법이 뭔가요?\",\n",
    "        \"purchaseState 값은 무엇인가요?\",\n",
    "        \"인앱결제 에러 해결 방법\"\n",
    "    ]\n",
    "    \n",
    "    for query in queries:\n",
    "        print(f\"쿼리: {query}\")\n",
    "        analytics = retriever.get_search_analytics(query)\n",
    "        print(f\"분석: {analytics}\")\n",
    "        \n",
    "        # results = retriever.get_relevant_documents(query)\n",
    "        # print(f\"결과: {len(results)}개 문서\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     test_retriever()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "cae4419e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Optimized RAG Pipeline for Technical Documentation\n",
    "기술문서 최적화 RAG 파이프라인\n",
    "\n",
    "특징:\n",
    "1. Ollama 기반 환경 (bge-m3, exaone3.5) 지원\n",
    "2. 계층적 문서 분할 + 컨텍스트 인식 검색\n",
    "3. 하이브리드 검색 (FAISS + BM25 + 메타데이터 필터링)\n",
    "4. 다단계 리랭킹 및 노이즈 필터링\n",
    "5. 스트리밍 응답 지원\n",
    "6. 검색 분석 및 디버깅 기능\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "from typing import List, Dict, Any, Optional, Union\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.callbacks import StreamingStdOutCallbackHandler\n",
    "\n",
    "# # 로컬 모듈 imports\n",
    "# from hierarchical_splitter import HierarchicalSplitter\n",
    "# from context_aware_retriever import ContextAwareRetriever\n",
    "\n",
    "\n",
    "class OptimizedRAGPipeline:\n",
    "    \"\"\"최적화된 RAG 파이프라인\"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 embed_model: str = \"bge-m3:latest\",\n",
    "                 llm_model: str = \"exaone3.5:latest\",\n",
    "                 data_file: str = \"data/dev_center_guide_allmd_touched.md\",\n",
    "                 vector_store_path: str = \"models/faiss_optimized\",\n",
    "                 chunk_size: int = 1000,\n",
    "                 overlap_ratio: float = 0.1,\n",
    "                 final_top_k: int = 5):\n",
    "        \n",
    "        # 모델 설정\n",
    "        self.embed_model = embed_model\n",
    "        self.llm_model = llm_model\n",
    "        self.data_file = data_file\n",
    "        self.vector_store_path = vector_store_path\n",
    "        \n",
    "        # 분할 설정\n",
    "        self.chunk_size = chunk_size\n",
    "        self.overlap_ratio = overlap_ratio\n",
    "        self.final_top_k = final_top_k\n",
    "        \n",
    "        # 구성요소 초기화\n",
    "        self.embeddings: Optional[OllamaEmbeddings] = None\n",
    "        self.vector_store: Optional[FAISS] = None\n",
    "        self.splitter: Optional[HierarchicalSplitter] = None\n",
    "        self.retriever: Optional[ContextAwareRetriever] = None\n",
    "        self.llm: Optional[ChatOllama] = None\n",
    "        self.documents: List[Document] = []\n",
    "        \n",
    "        # 성능 통계\n",
    "        self.stats: Dict[str, Union[int, float]] = {\n",
    "            'total_documents': 0,\n",
    "            'total_chunks': 0,\n",
    "            'index_build_time': 0.0,\n",
    "            'last_query_time': 0.0,\n",
    "            'queries_processed': 0\n",
    "        }\n",
    "        \n",
    "        print(\"RAG Pipeline 초기화:\")\n",
    "        print(f\"  - 임베딩 모델: {embed_model}\")\n",
    "        print(f\"  - LLM 모델: {llm_model}\")\n",
    "        print(f\"  - 데이터 파일: {data_file}\")\n",
    "        print(f\"  - 벡터 저장소: {vector_store_path}\")\n",
    "    \n",
    "    def initialize_models(self):\n",
    "        \"\"\"모델 초기화\"\"\"\n",
    "        print(\"모델 초기화 중...\")\n",
    "        \n",
    "        # 임베딩 모델\n",
    "        self.embeddings = OllamaEmbeddings(model=self.embed_model)\n",
    "        \n",
    "        # LLM 모델 (스트리밍 지원)\n",
    "        self.llm = ChatOllama(\n",
    "            model=self.llm_model,\n",
    "            temperature=0.1,\n",
    "            streaming=True,\n",
    "            callbacks=[StreamingStdOutCallbackHandler()]\n",
    "        )\n",
    "        \n",
    "        # 문서 분할기\n",
    "        self.splitter = HierarchicalSplitter(\n",
    "            chunk_size=self.chunk_size,\n",
    "            overlap_ratio=self.overlap_ratio,\n",
    "            preserve_tables=True,\n",
    "            preserve_code=True\n",
    "        )\n",
    "        \n",
    "        print(\"✓ 모델 초기화 완료\")\n",
    "    \n",
    "    def load_and_process_documents(self, force_rebuild: bool = False):\n",
    "        \"\"\"문서 로드 및 처리\"\"\"\n",
    "        print(\"문서 처리 시작...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # 기존 벡터 스토어 확인\n",
    "        if not force_rebuild and os.path.exists(self.vector_store_path) and self.embeddings is not None:\n",
    "            try:\n",
    "                print(\"기존 벡터 스토어 로드 중...\")\n",
    "                self.vector_store = FAISS.load_local(\n",
    "                    self.vector_store_path, \n",
    "                    self.embeddings, \n",
    "                    allow_dangerous_deserialization=True\n",
    "                )\n",
    "                \n",
    "                # 메타데이터 로드\n",
    "                metadata_file = f\"{self.vector_store_path}/metadata.json\"\n",
    "                if os.path.exists(metadata_file):\n",
    "                    with open(metadata_file, 'r', encoding='utf-8') as f:\n",
    "                        self.stats.update(json.load(f))\n",
    "                \n",
    "                print(f\"✓ 기존 벡터 스토어 로드 완료 (문서: {self.stats.get('total_chunks', 0)}개)\")\n",
    "                return\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"기존 벡터 스토어 로드 실패: {e}\")\n",
    "                print(\"새로 생성합니다...\")\n",
    "        \n",
    "        # 문서 로드\n",
    "        if not os.path.exists(self.data_file):\n",
    "            raise FileNotFoundError(f\"데이터 파일을 찾을 수 없습니다: {self.data_file}\")\n",
    "        \n",
    "        with open(self.data_file, 'r', encoding='utf-8') as f:\n",
    "            content = f.read()\n",
    "        \n",
    "        print(f\"원본 문서 크기: {len(content):,} 문자\")\n",
    "        \n",
    "        # 계층적 분할\n",
    "        if self.splitter is None:\n",
    "            raise ValueError(\"문서 분할기가 초기화되지 않았습니다. initialize_models()를 먼저 호출하세요.\")\n",
    "        \n",
    "        print(\"계층적 문서 분할 중...\")\n",
    "        self.documents = self.splitter.split_document(content)\n",
    "        \n",
    "        self.stats['total_documents'] = 1\n",
    "        self.stats['total_chunks'] = len(self.documents)\n",
    "        \n",
    "        print(f\"✓ 분할 완료: {len(self.documents)}개 청크\")\n",
    "        \n",
    "        # 벡터 스토어 구축\n",
    "        if self.embeddings is None:\n",
    "            raise ValueError(\"임베딩 모델이 초기화되지 않았습니다. initialize_models()를 먼저 호출하세요.\")\n",
    "            \n",
    "        print(\"벡터 인덱스 구축 중...\")\n",
    "        self.vector_store = FAISS.from_documents(self.documents, self.embeddings)\n",
    "        \n",
    "        # 벡터 스토어 저장\n",
    "        os.makedirs(self.vector_store_path, exist_ok=True)\n",
    "        self.vector_store.save_local(self.vector_store_path)\n",
    "        \n",
    "        # 메타데이터 저장\n",
    "        self.stats['index_build_time'] = time.time() - start_time\n",
    "        metadata_file = f\"{self.vector_store_path}/metadata.json\"\n",
    "        with open(metadata_file, 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.stats, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        print(f\"✓ 벡터 인덱스 구축 완료 (소요시간: {self.stats['index_build_time']:.2f}초)\")\n",
    "    \n",
    "    def build_retriever(self):\n",
    "        \"\"\"검색기 구축\"\"\"\n",
    "        print(\"검색기 구축 중...\")\n",
    "        \n",
    "        if not self.vector_store:\n",
    "            raise ValueError(\"벡터 스토어가 초기화되지 않았습니다. load_and_process_documents()를 먼저 호출하세요.\")\n",
    "        \n",
    "        # FAISS 검색기\n",
    "        vector_retriever = self.vector_store.as_retriever(\n",
    "            search_type=\"mmr\",\n",
    "            search_kwargs={\"k\": 15, \"fetch_k\": 50, \"lambda_mult\": 0.7}\n",
    "        )\n",
    "        \n",
    "        # BM25 검색기\n",
    "        if not self.documents:\n",
    "            # 벡터 스토어에서 문서 복원 (간소화된 버전)\n",
    "            print(\"문서를 벡터 스토어에서 복원 중...\")\n",
    "            # 실제 구현에서는 문서 메타데이터를 별도 저장하는 것이 좋습니다\n",
    "            self.documents = [Document(page_content=\"임시 문서\", metadata={})]\n",
    "        \n",
    "        bm25_retriever = BM25Retriever.from_documents(\n",
    "            self.documents, \n",
    "            bm25_params={\"k1\": 1.5, \"b\": 0.75}\n",
    "        )\n",
    "        bm25_retriever.k = 20\n",
    "        \n",
    "        # 앙상블 검색기\n",
    "        ensemble_retriever = EnsembleRetriever(\n",
    "            retrievers=[bm25_retriever, vector_retriever],\n",
    "            weights=[0.4, 0.6]\n",
    "        )\n",
    "        \n",
    "        # 컨텍스트 인식 검색기로 래핑\n",
    "        self.retriever = ContextAwareRetriever(\n",
    "            base_retriever=ensemble_retriever,\n",
    "            rerank_top_k=20,\n",
    "            final_top_k=self.final_top_k,\n",
    "            enable_query_expansion=True,\n",
    "            noise_threshold=0.7\n",
    "        )\n",
    "        \n",
    "        print(\"✓ 검색기 구축 완료\")\n",
    "    \n",
    "    def create_prompt_template(self) -> PromptTemplate:\n",
    "        \"\"\"프롬프트 템플릿 생성\"\"\"\n",
    "        \n",
    "        template = \"\"\"당신은 원스토어 인앱결제 기술문서 전문가입니다. \n",
    "주어진 문서를 바탕으로 사용자의 질문에 정확하고 자세하게 답변해주세요.\n",
    "\n",
    "**답변 지침:**\n",
    "1. 기술적 정확성을 최우선으로 합니다\n",
    "2. 코드 예제가 있다면 반드시 포함합니다  \n",
    "3. API 파라미터나 응답값은 정확히 기술합니다\n",
    "4. 단계별 설명이 필요한 경우 번호를 매겨 설명합니다\n",
    "5. 불확실한 내용은 \"문서에서 확인되지 않음\"이라고 명시합니다\n",
    "\n",
    "**참고 문서:**\n",
    "{context}\n",
    "\n",
    "**질문:** {question}\n",
    "\n",
    "**답변:**\"\"\"\n",
    "\n",
    "        return PromptTemplate(\n",
    "            template=template,\n",
    "            input_variables=[\"context\", \"question\"]\n",
    "        )\n",
    "    \n",
    "    def query(self, question: str, return_sources: bool = True, stream: bool = True) -> Dict[str, Any]:\n",
    "        \"\"\"쿼리 실행\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if not self.retriever or not self.llm:\n",
    "            raise ValueError(\"파이프라인이 완전히 초기화되지 않았습니다.\")\n",
    "        \n",
    "        print(f\"\\n🔍 질문: {question}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # 1. 문서 검색\n",
    "        print(\"관련 문서 검색 중...\")\n",
    "        retrieved_docs = self.retriever.get_relevant_documents(question)\n",
    "        \n",
    "        print(f\"✓ {len(retrieved_docs)}개 문서 검색됨\")\n",
    "        \n",
    "        # 2. 컨텍스트 구성\n",
    "        context = \"\\n\\n\".join([\n",
    "            f\"[문서 {i+1}] {doc.metadata.get('section_hierarchy', 'N/A')}\\n{doc.page_content}\"\n",
    "            for i, doc in enumerate(retrieved_docs)\n",
    "        ])\n",
    "        \n",
    "        # 3. 프롬프트 생성\n",
    "        prompt_template = self.create_prompt_template()\n",
    "        prompt = prompt_template.format(context=context, question=question)\n",
    "        \n",
    "        # 4. LLM 실행\n",
    "        print(\"\\n💭 답변 생성 중...\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        response_text = \"\"\n",
    "        if stream:\n",
    "            # 스트리밍 응답\n",
    "            response_generator = self.llm.stream(prompt)\n",
    "            for chunk in response_generator:\n",
    "                if hasattr(chunk, 'content'):\n",
    "                    # chunk.content는 다양한 타입이 될 수 있으므로 str로 안전하게 변환\n",
    "                    response_text += str(chunk.content)\n",
    "            print()  # 줄바꿈\n",
    "        else:\n",
    "            # 일반 응답\n",
    "            response = self.llm.invoke(prompt)\n",
    "            if hasattr(response, 'content'):\n",
    "                # response.content도 str로 안전하게 변환\n",
    "                response_text = str(response.content)\n",
    "            else:\n",
    "                response_text = str(response)\n",
    "        \n",
    "        # 5. 통계 업데이트\n",
    "        query_time = time.time() - start_time\n",
    "        self.stats['last_query_time'] = query_time\n",
    "        self.stats['queries_processed'] += 1\n",
    "        \n",
    "        print(\"-\" * 60)\n",
    "        print(f\"⏱️  응답 시간: {query_time:.2f}초\")\n",
    "        \n",
    "        # 6. 결과 구성\n",
    "        result = {\n",
    "            'question': question,\n",
    "            'answer': response_text,\n",
    "            'query_time': query_time,\n",
    "            'retrieved_docs_count': len(retrieved_docs)\n",
    "        }\n",
    "        \n",
    "        if return_sources:\n",
    "            result['sources'] = [\n",
    "                {\n",
    "                    'content': doc.page_content[:200] + \"...\",\n",
    "                    'metadata': doc.metadata\n",
    "                }\n",
    "                for doc in retrieved_docs\n",
    "            ]\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def batch_query(self, questions: List[str]) -> List[Dict[str, Any]]:\n",
    "        \"\"\"배치 쿼리 실행\"\"\"\n",
    "        print(f\"📋 배치 쿼리 실행: {len(questions)}개 질문\")\n",
    "        \n",
    "        results = []\n",
    "        for i, question in enumerate(questions, 1):\n",
    "            print(f\"\\n[{i}/{len(questions)}]\")\n",
    "            result = self.query(question, stream=False)\n",
    "            results.append(result)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def get_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"파이프라인 통계 반환\"\"\"\n",
    "        return {\n",
    "            **self.stats,\n",
    "            'models': {\n",
    "                'embedding': self.embed_model,\n",
    "                'llm': self.llm_model\n",
    "            },\n",
    "            'config': {\n",
    "                'chunk_size': self.chunk_size,\n",
    "                'overlap_ratio': self.overlap_ratio,\n",
    "                'final_top_k': self.final_top_k\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def analyze_document_structure(self) -> Dict[str, Any]:\n",
    "        \"\"\"문서 구조 분석\"\"\"\n",
    "        if not self.documents:\n",
    "            return {\"error\": \"문서가 로드되지 않았습니다\"}\n",
    "        \n",
    "        # 출처별 통계\n",
    "        source_stats: Dict[str, int] = {}\n",
    "        content_type_stats: Dict[str, int] = {}\n",
    "        tech_term_stats: Dict[str, Dict[str, int]] = {}\n",
    "        \n",
    "        for doc in self.documents:\n",
    "            # 출처 통계\n",
    "            source_url = doc.metadata.get('source_url', 'Unknown')\n",
    "            source_domain = source_url.split('/')[-1] if '/' in source_url else source_url\n",
    "            source_stats[source_domain] = source_stats.get(source_domain, 0) + 1\n",
    "            \n",
    "            # 콘텐츠 타입 통계\n",
    "            content_types = doc.metadata.get('content_types', ['text'])\n",
    "            for ct in content_types:\n",
    "                content_type_stats[ct] = content_type_stats.get(ct, 0) + 1\n",
    "            \n",
    "            # 기술 용어 통계\n",
    "            tech_terms = doc.metadata.get('technical_terms', {})\n",
    "            for category, terms in tech_terms.items():\n",
    "                if category not in tech_term_stats:\n",
    "                    tech_term_stats[category] = {}\n",
    "                for term in terms:\n",
    "                    tech_term_stats[category][term] = tech_term_stats[category].get(term, 0) + 1\n",
    "        \n",
    "        return {\n",
    "            'total_chunks': len(self.documents),\n",
    "            'source_distribution': source_stats,\n",
    "            'content_type_distribution': content_type_stats,\n",
    "            'tech_term_distribution': {\n",
    "                category: dict(list(sorted(terms.items(), key=lambda x: x[1], reverse=True))[:10])\n",
    "                for category, terms in tech_term_stats.items()\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def debug_search(self, question: str) -> Dict[str, Any]:\n",
    "        \"\"\"검색 디버깅 정보\"\"\"\n",
    "        if not self.retriever:\n",
    "            return {\"error\": \"검색기가 초기화되지 않았습니다\"}\n",
    "        \n",
    "        # 쿼리 분석\n",
    "        analytics = self.retriever.get_search_analytics(question)\n",
    "        \n",
    "        # 검색 실행\n",
    "        retrieved_docs = self.retriever.get_relevant_documents(question)\n",
    "        \n",
    "        # 검색 결과 분석\n",
    "        result_analysis = []\n",
    "        for i, doc in enumerate(retrieved_docs):\n",
    "            result_analysis.append({\n",
    "                'rank': i + 1,\n",
    "                'content_preview': doc.page_content[:150] + \"...\",\n",
    "                'section_hierarchy': doc.metadata.get('section_hierarchy', 'N/A'),\n",
    "                'content_types': doc.metadata.get('content_types', []),\n",
    "                'technical_terms': doc.metadata.get('technical_terms', {}),\n",
    "                'content_length': len(doc.page_content)\n",
    "            })\n",
    "        \n",
    "        return {\n",
    "            'query_analysis': analytics,\n",
    "            'retrieved_count': len(retrieved_docs),\n",
    "            'results': result_analysis\n",
    "        }\n",
    "\n",
    "\n",
    "# 편의 함수들\n",
    "def create_pipeline(data_file: str = \"data/dev_center_guide_allmd_touched.md\",\n",
    "                   force_rebuild: bool = False) -> OptimizedRAGPipeline:\n",
    "    \"\"\"파이프라인 생성 및 초기화\"\"\"\n",
    "    \n",
    "    pipeline = OptimizedRAGPipeline(data_file=data_file)\n",
    "    \n",
    "    # 초기화 단계\n",
    "    pipeline.initialize_models()\n",
    "    pipeline.load_and_process_documents(force_rebuild=force_rebuild)\n",
    "    pipeline.build_retriever()\n",
    "    \n",
    "    print(\"\\n🚀 RAG 파이프라인 준비 완료!\")\n",
    "    print(f\"   총 문서: {pipeline.stats['total_chunks']}개\")\n",
    "    print(f\"   인덱싱 시간: {pipeline.stats['index_build_time']:.2f}초\")\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "\n",
    "def interactive_mode(pipeline: OptimizedRAGPipeline):\n",
    "    \"\"\"대화형 모드\"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"🤖 원스토어 IAP 기술문서 QA 시스템\")\n",
    "    print(\"=\"*60)\n",
    "    print(\"질문을 입력하세요 (종료: 'quit' 또는 'exit')\")\n",
    "    print(\"특수 명령어:\")\n",
    "    print(\"  - 'stats': 파이프라인 통계\")\n",
    "    print(\"  - 'analyze': 문서 구조 분석\")\n",
    "    print(\"  - 'debug <질문>': 검색 디버깅\")\n",
    "    print(\"-\"*60)\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            question = input(\"\\n❓ 질문: \").strip()\n",
    "            \n",
    "            if question.lower() in ['quit', 'exit', '종료']:\n",
    "                print(\"👋 시스템을 종료합니다.\")\n",
    "                break\n",
    "            \n",
    "            if question == 'stats':\n",
    "                stats = pipeline.get_statistics()\n",
    "                print(\"\\n📊 파이프라인 통계:\")\n",
    "                print(json.dumps(stats, ensure_ascii=False, indent=2))\n",
    "                continue\n",
    "            \n",
    "            if question == 'analyze':\n",
    "                analysis = pipeline.analyze_document_structure()\n",
    "                print(\"\\n📈 문서 구조 분석:\")\n",
    "                print(json.dumps(analysis, ensure_ascii=False, indent=2))\n",
    "                continue\n",
    "            \n",
    "            if question.startswith('debug '):\n",
    "                debug_query = question[6:]\n",
    "                debug_info = pipeline.debug_search(debug_query)\n",
    "                print(\"\\n🔍 검색 디버깅:\")\n",
    "                print(json.dumps(debug_info, ensure_ascii=False, indent=2))\n",
    "                continue\n",
    "            \n",
    "            if not question:\n",
    "                continue\n",
    "            \n",
    "            # 일반 질문 처리\n",
    "            pipeline.query(question)\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\n👋 시스템을 종료합니다.\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\n❌ 오류 발생: {e}\")\n",
    "\n",
    "\n",
    "# 테스트 함수\n",
    "def test_pipeline():\n",
    "    \"\"\"파이프라인 테스트\"\"\"\n",
    "    \n",
    "    test_questions = [\n",
    "        \"PurchaseClient 초기화 방법이 뭔가요?\",\n",
    "        \"PNS의 purchaseState 값은 무엇인가요?\", \n",
    "        \"PNS 서비스 설정 방법을 알려주세요\",\n",
    "        \"인앱결제 에러가 발생했을 때 해결 방법은?\",\n",
    "        \"구독형 상품과 관리형 상품의 차이점은?\"\n",
    "    ]\n",
    "    \n",
    "    print(\"🧪 파이프라인 테스트 시작...\")\n",
    "    \n",
    "    # 테스트용 더미 파이프라인 (실제 파일 없이)\n",
    "    # 실제 사용시에는 create_pipeline()을 사용\n",
    "    \n",
    "    for question in test_questions:\n",
    "        print(f\"\\n질문: {question}\")\n",
    "        # result = pipeline.query(question)\n",
    "        print(\"답변: [테스트 모드 - 실제 구현에서는 답변이 생성됩니다]\")\n",
    "        print(\"-\" * 60)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # 테스트 실행\n",
    "#     test_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "a249d374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 빠른 테스트 데모 시작\n",
      "============================================================\n",
      "파이프라인 초기화 중...\n",
      "RAG Pipeline 초기화:\n",
      "  - 임베딩 모델: bge-m3:latest\n",
      "  - LLM 모델: exaone3.5:latest\n",
      "  - 데이터 파일: data/dev_center_guide_allmd_touched.md\n",
      "  - 벡터 저장소: models/faiss_optimized\n",
      "모델 초기화 중...\n",
      "✓ 모델 초기화 완료\n",
      "문서 처리 시작...\n",
      "기존 벡터 스토어 로드 중...\n",
      "✓ 기존 벡터 스토어 로드 완료 (문서: 5912개)\n",
      "검색기 구축 중...\n",
      "문서를 벡터 스토어에서 복원 중...\n",
      "❌ 오류 발생: \"ContextAwareRetriever\" object has no field \"base_retriever\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"/tmp/ipykernel_5434/1986687736.py\", line 27, in demo_quick_test\n",
      "    pipeline.build_retriever()\n",
      "  File \"/tmp/ipykernel_5434/2927573387.py\", line 206, in build_retriever\n",
      "    self.retriever = ContextAwareRetriever(\n",
      "                     ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/tmp/ipykernel_5434/4081965214.py\", line 298, in __init__\n",
      "    self.base_retriever = base_retriever\n",
      "    ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/gpu/dev/jupyter_root/venv/lib/python3.11/site-packages/pydantic/main.py\", line 884, in __setattr__\n",
      "    raise ValueError(f'\"{self.__class__.__name__}\" object has no field \"{name}\"')\n",
      "ValueError: \"ContextAwareRetriever\" object has no field \"base_retriever\"\n"
     ]
    }
   ],
   "source": [
    "def demo_quick_test():\n",
    "    \"\"\"빠른 테스트 데모\"\"\"\n",
    "    print(\"🚀 빠른 테스트 데모 시작\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 설정\n",
    "    data_file = \"data/dev_center_guide_allmd_touched.md\"\n",
    "    \n",
    "    # 데이터 파일 존재 확인\n",
    "    if not os.path.exists(data_file):\n",
    "        print(f\"❌ 데이터 파일을 찾을 수 없습니다: {data_file}\")\n",
    "        print(\"경로를 수정하거나 파일을 확인해주세요.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # 파이프라인 생성\n",
    "        print(\"파이프라인 초기화 중...\")\n",
    "        pipeline = OptimizedRAGPipeline(\n",
    "            data_file=data_file,\n",
    "            chunk_size=800,\n",
    "            final_top_k=3\n",
    "        )\n",
    "        \n",
    "        # 단계별 초기화\n",
    "        pipeline.initialize_models()\n",
    "        pipeline.load_and_process_documents(force_rebuild=False)\n",
    "        pipeline.build_retriever()\n",
    "        \n",
    "        print(\"\\n✅ 파이프라인 준비 완료!\")\n",
    "        \n",
    "        # 테스트 질문들\n",
    "        test_questions = [\n",
    "            \"PurchaseClient를 어떻게 초기화하나요?\",\n",
    "            \"PNS의 purchaseState 값의 종류는 무엇인가요?\",\n",
    "            \"PNS 서비스란 무엇인가요?\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\n📝 테스트 질문 실행:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for i, question in enumerate(test_questions, 1):\n",
    "            print(f\"\\n[질문 {i}] {question}\")\n",
    "            print(\"🤔 답변 생성 중...\")\n",
    "            \n",
    "            start_time = time.time()\n",
    "            result = pipeline.query(question, stream=False)\n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            print(\"\\n💡 답변:\")\n",
    "            print(result['answer'])\n",
    "            print(f\"\\n⏱️  응답 시간: {elapsed:.2f}초\")\n",
    "            print(\"-\" * 40)\n",
    "        \n",
    "        # 통계 출력\n",
    "        stats = pipeline.get_statistics()\n",
    "        print(\"\\n📊 파이프라인 통계:\")\n",
    "        print(f\"  - 총 문서 청크: {stats['total_chunks']}\")\n",
    "        print(f\"  - 인덱싱 시간: {stats['index_build_time']:.2f}초\")\n",
    "        print(f\"  - 처리된 쿼리: {stats['queries_processed']}\")\n",
    "        print(f\"  - 마지막 쿼리 시간: {stats['last_query_time']:.2f}초\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 오류 발생: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "demo_quick_test()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b70e8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "caa6d636",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 수정된 ContextAwareRetriever 클래스 정의 완료\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "수정된 ContextAwareRetriever 클래스\n",
    "Pydantic 우회 방식으로 해결\n",
    "\"\"\"\n",
    "\n",
    "from typing import List, Dict, Any, Optional, Union\n",
    "from langchain.schema import BaseRetriever\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_core.callbacks.manager import CallbackManagerForRetrieverRun\n",
    "\n",
    "\n",
    "class FixedContextAwareRetriever(BaseRetriever):\n",
    "    \"\"\"컨텍스트 인식 검색기 - 간단한 버전\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 base_retriever: BaseRetriever,\n",
    "                 rerank_top_k: int = 20,\n",
    "                 final_top_k: int = 5,\n",
    "                 enable_query_expansion: bool = True,\n",
    "                 noise_threshold: float = 0.7,\n",
    "                 **kwargs):\n",
    "        \n",
    "        # 기본 초기화 (kwargs는 부모 클래스에 전달)\n",
    "        super().__init__(**kwargs)\n",
    "        \n",
    "        # 속성 직접 설정 (Pydantic 검증 우회)\n",
    "        object.__setattr__(self, 'base_retriever', base_retriever)\n",
    "        object.__setattr__(self, 'rerank_top_k', rerank_top_k)\n",
    "        object.__setattr__(self, 'final_top_k', final_top_k)\n",
    "        object.__setattr__(self, 'enable_query_expansion', enable_query_expansion)\n",
    "        object.__setattr__(self, 'noise_threshold', noise_threshold)\n",
    "        \n",
    "        # 내부 객체 초기화\n",
    "        object.__setattr__(self, 'query_analyzer', QueryAnalyzer())\n",
    "        object.__setattr__(self, 'context_scorer', ContextScorer())\n",
    "        object.__setattr__(self, 'noise_filter', NoiseFilter())\n",
    "    \n",
    "    def _get_relevant_documents(self, \n",
    "                              query: str, \n",
    "                              *, \n",
    "                              run_manager: CallbackManagerForRetrieverRun) -> List[Document]:\n",
    "        \"\"\"메인 검색 메소드\"\"\"\n",
    "        \n",
    "        # 1. 쿼리 분석\n",
    "        search_context = self.query_analyzer.analyze_query(query)\n",
    "        \n",
    "        # 2. 쿼리 확장 (옵션)\n",
    "        queries_to_search = [query]\n",
    "        if self.enable_query_expansion:\n",
    "            expanded_queries = self.query_analyzer.expand_query(query)\n",
    "            queries_to_search.extend(expanded_queries[:3])  # 최대 3개 확장\n",
    "        \n",
    "        # 3. 기본 검색 수행\n",
    "        all_docs = []\n",
    "        for q in queries_to_search:\n",
    "            docs = self.base_retriever.get_relevant_documents(q, run_manager=run_manager)\n",
    "            all_docs.extend(docs)\n",
    "        \n",
    "        # 중복 제거 (page_content 기준)\n",
    "        unique_docs = []\n",
    "        seen_contents = set()\n",
    "        for doc in all_docs:\n",
    "            content_hash = hash(doc.page_content)\n",
    "            if content_hash not in seen_contents:\n",
    "                unique_docs.append(doc)\n",
    "                seen_contents.add(content_hash)\n",
    "        \n",
    "        # 상위 K개만 리랭킹 대상으로\n",
    "        docs_to_rerank = unique_docs[:self.rerank_top_k]\n",
    "        \n",
    "        # 4. 다단계 리랭킹\n",
    "        reranked_docs = self._rerank_documents(docs_to_rerank, search_context)\n",
    "        \n",
    "        # 5. 최종 결과 반환\n",
    "        return reranked_docs[:self.final_top_k]\n",
    "    \n",
    "    def _rerank_documents(self, docs: List[Document], search_context: SearchContext) -> List[Document]:\n",
    "        \"\"\"다단계 리랭킹\"\"\"\n",
    "        \n",
    "        scored_docs = []\n",
    "        \n",
    "        for doc in docs:\n",
    "            # 점수 계산\n",
    "            term_score = self.context_scorer.calculate_term_relevance(doc, search_context)\n",
    "            context_score = self.context_scorer.calculate_context_relevance(doc, search_context)\n",
    "            accuracy_score = self.context_scorer.calculate_technical_accuracy(doc, search_context)\n",
    "            noise_score = self.noise_filter.calculate_noise_score(doc, search_context)\n",
    "            \n",
    "            # 종합 점수 (노이즈는 패널티)\n",
    "            final_score = (term_score * 0.4 + \n",
    "                          context_score * 0.3 + \n",
    "                          accuracy_score * 0.2 + \n",
    "                          (1 - noise_score) * 0.1)\n",
    "            \n",
    "            scored_docs.append((doc, final_score))\n",
    "        \n",
    "        # 점수순 정렬\n",
    "        scored_docs.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 노이즈 임계값으로 필터링\n",
    "        filtered_docs = []\n",
    "        for doc, score in scored_docs:\n",
    "            noise_score = self.noise_filter.calculate_noise_score(doc, search_context)\n",
    "            if noise_score <= self.noise_threshold:\n",
    "                filtered_docs.append(doc)\n",
    "        \n",
    "        return filtered_docs if filtered_docs else [doc for doc, _ in scored_docs]\n",
    "    \n",
    "    def get_search_analytics(self, query: str) -> Dict[str, Any]:\n",
    "        \"\"\"검색 분석 정보 반환 (디버깅용)\"\"\"\n",
    "        search_context = self.query_analyzer.analyze_query(query)\n",
    "        \n",
    "        return {\n",
    "            'query_terms': search_context.query_terms,\n",
    "            'query_type': search_context.query_type,\n",
    "            'tech_terms': search_context.tech_terms,\n",
    "            'context_keywords': list(search_context.context_keywords)\n",
    "        }\n",
    "\n",
    "print(\"✓ 수정된 ContextAwareRetriever 클래스 정의 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8e6bb1c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ 수정된 OptimizedRAGPipeline 클래스 정의 완료\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "수정된 OptimizedRAGPipeline 클래스\n",
    "FixedContextAwareRetriever 사용\n",
    "\"\"\"\n",
    "\n",
    "# OptimizedRAGPipeline 클래스의 build_retriever 메소드만 수정\n",
    "class FixedOptimizedRAGPipeline(OptimizedRAGPipeline):\n",
    "    \"\"\"수정된 RAG 파이프라인 - 고정된 검색기 사용\"\"\"\n",
    "    \n",
    "    def build_retriever(self):\n",
    "        \"\"\"검색기 구축 - 수정된 버전\"\"\"\n",
    "        print(\"검색기 구축 중...\")\n",
    "        \n",
    "        if not self.vector_store:\n",
    "            raise ValueError(\"벡터 스토어가 초기화되지 않았습니다. load_and_process_documents()를 먼저 호출하세요.\")\n",
    "        \n",
    "        # FAISS 검색기\n",
    "        vector_retriever = self.vector_store.as_retriever(\n",
    "            search_type=\"mmr\",\n",
    "            search_kwargs={\"k\": 15, \"fetch_k\": 50, \"lambda_mult\": 0.7}\n",
    "        )\n",
    "        \n",
    "        # BM25 검색기\n",
    "        if not self.documents:\n",
    "            # 벡터 스토어에서 문서 복원 (간소화된 버전)\n",
    "            print(\"문서를 벡터 스토어에서 복원 중...\")\n",
    "            # 실제 구현에서는 문서 메타데이터를 별도 저장하는 것이 좋습니다\n",
    "            self.documents = [Document(page_content=\"임시 문서\", metadata={})]\n",
    "        \n",
    "        bm25_retriever = BM25Retriever.from_documents(\n",
    "            self.documents, \n",
    "            bm25_params={\"k1\": 1.5, \"b\": 0.75}\n",
    "        )\n",
    "        bm25_retriever.k = 20\n",
    "        \n",
    "        # 앙상블 검색기\n",
    "        ensemble_retriever = EnsembleRetriever(\n",
    "            retrievers=[bm25_retriever, vector_retriever],\n",
    "            weights=[0.4, 0.6]\n",
    "        )\n",
    "        \n",
    "        # 컨텍스트 인식 검색기로 래핑 - 수정된 클래스 사용\n",
    "        self.retriever = FixedContextAwareRetriever(\n",
    "            base_retriever=ensemble_retriever,\n",
    "            rerank_top_k=20,\n",
    "            final_top_k=self.final_top_k,\n",
    "            enable_query_expansion=True,\n",
    "            noise_threshold=0.7\n",
    "        )\n",
    "        \n",
    "        print(\"✓ 검색기 구축 완료\")\n",
    "\n",
    "# 편의 함수도 수정\n",
    "def create_fixed_pipeline(data_file: str = \"data/dev_center_guide_allmd_touched.md\",\n",
    "                         force_rebuild: bool = False) -> FixedOptimizedRAGPipeline:\n",
    "    \"\"\"수정된 파이프라인 생성 및 초기화\"\"\"\n",
    "    \n",
    "    pipeline = FixedOptimizedRAGPipeline(data_file=data_file)\n",
    "    \n",
    "    # 초기화 단계\n",
    "    pipeline.initialize_models()\n",
    "    pipeline.load_and_process_documents(force_rebuild=force_rebuild)\n",
    "    pipeline.build_retriever()\n",
    "    \n",
    "    print(\"\\n🚀 RAG 파이프라인 준비 완료!\")\n",
    "    print(f\"   총 문서: {pipeline.stats['total_chunks']}개\")\n",
    "    print(f\"   인덱싱 시간: {pipeline.stats['index_build_time']:.2f}초\")\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "print(\"✓ 수정된 OptimizedRAGPipeline 클래스 정의 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "276db313",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 수정된 클래스로 다시 테스트...\n",
      "🚀 수정된 테스트 데모 시작\n",
      "============================================================\n",
      "파이프라인 초기화 중...\n",
      "RAG Pipeline 초기화:\n",
      "  - 임베딩 모델: bge-m3:latest\n",
      "  - LLM 모델: exaone3.5:latest\n",
      "  - 데이터 파일: data/dev_center_guide_allmd_touched.md\n",
      "  - 벡터 저장소: models/faiss_optimized\n",
      "모델 초기화 중...\n",
      "✓ 모델 초기화 완료\n",
      "문서 처리 시작...\n",
      "기존 벡터 스토어 로드 중...\n",
      "✓ 기존 벡터 스토어 로드 완료 (문서: 5912개)\n",
      "검색기 구축 중...\n",
      "문서를 벡터 스토어에서 복원 중...\n",
      "✓ 검색기 구축 완료\n",
      "\n",
      "🚀 RAG 파이프라인 준비 완료!\n",
      "   총 문서: 5912개\n",
      "   인덱싱 시간: 73.25초\n",
      "\n",
      "✅ 파이프라인 준비 완료!\n",
      "\n",
      "📝 테스트 질문 실행:\n",
      "----------------------------------------\n",
      "\n",
      "[질문 1] PurchaseClient를 어떻게 초기화하나요?\n",
      "🤔 답변 생성 중...\n",
      "\n",
      "🔍 질문: PurchaseClient를 어떻게 초기화하나요?\n",
      "============================================================\n",
      "관련 문서 검색 중...\n",
      "✓ 5개 문서 검색됨\n",
      "\n",
      "💭 답변 생성 중...\n",
      "------------------------------------------------------------\n",
      "PurchaseClient를 초기화하는 방법은 다음과 같습니다. 주로 Kotlin과 Java 언어를 사용하여 구현하는 방법을 설명드리겠습니다. 주어진 문서를 바탕으로 단계별로 설명드리겠습니다.\n",
      "\n",
      "### Kotlin 구현 예시\n",
      "\n",
      "1. **PurchasesUpdatedListener 구현**: 구매 결과를 처리할 리스너를 먼저 정의합니다.\n",
      "2. **PurchaseClient Builder 사용**: `PurchaseClient.newBuilder()`를 사용하여 PurchaseClient 인스턴스를 생성하고 필요한 설정을 적용합니다.\n",
      "\n",
      "```kotlin\n",
      "// 1. 구매 결과 리스너 정의\n",
      "private val listener = PurchasesUpdatedListener { iapResult, purchases ->\n",
      "    // 구매 결과 처리 로직\n",
      "    when (iapResult.status) {\n",
      "        IapResult.Status.SUCCESS -> {\n",
      "            // 구매 성공 시 처리 로직\n",
      "            Log.d(\"PurchaseClient\", \"Purchase successful: ${purchases.firstOrNull()?.purchaseToken}\")\n",
      "        }\n",
      "        IapResult.Status.ERROR -> {\n",
      "            // 구매 실패 시 처리 로직\n",
      "            Log.e(\"PurchaseClient\", \"Purchase failed: ${iapResult.errorMessage}\")\n",
      "        }\n",
      "        else -> {\n",
      "            // 기타 상태 처리 로직\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "// 2. PurchaseClient 초기화\n",
      "private var purchaseClient = PurchaseClient.newBuilder(activity)\n",
      "    .setListener(listener)          // 리스너 설정\n",
      "    .setBase64PublicKey(\"YOUR_PUBLIC_KEY\")  // 공개키 설정 (선택 사항)\n",
      "    .build()\n",
      "\n",
      "// 구매 플로우 시작\n",
      "fun startPurchaseFlow() {\n",
      "    val purchaseFlowParams = PurchaseFlowParams.newBuilder()\n",
      "        .setProductId(\"YOUR_PRODUCT_ID\")  // 상품 ID 설정\n",
      "        .build()\n",
      "\n",
      "    purchaseClient.launchPurchaseFlow(activity, purchaseFlowParams)\n",
      "}\n",
      "```\n",
      "\n",
      "### Java 구현 예시\n",
      "\n",
      "1. **PurchasesUpdatedListener 구현**: 구매 결과를 처리할 리스너를 정의합니다.\n",
      "2. **PurchaseClient Builder 사용**: `PurchaseClient.newBuilder()`를 사용하여 PurchaseClient 인스턴스를 생성하고 필요한 설정을 적용합니다.\n",
      "\n",
      "```java\n",
      "// 1. 구매 결과 리스너 정의\n",
      "private final PurchasesUpdatedListener listener = new PurchasesUpdatedListener() {\n",
      "    @Override\n",
      "    public void onPurchasesUpdated(IapResult iapResult, List<PurchaseData> purchases) {\n",
      "        switch (iapResult.getStatus()) {\n",
      "            case SUCCESS:\n",
      "                // 구매 성공 시 처리 로직\n",
      "                Log.d(\"PurchaseClient\", \"Purchase successful: \" + purchases.get(0).getPurchaseToken());\n",
      "                break;\n",
      "            case ERROR:\n",
      "                // 구매 실패 시 처리 로직\n",
      "                Log.e(\"PurchaseClient\", \"Purchase failed: \" + iapResult.getErrorMessage());\n",
      "                break;\n",
      "            default:\n",
      "                // 기타 상태 처리 로직\n",
      "                break;\n",
      "        }\n",
      "    }\n",
      "};\n",
      "\n",
      "// 2. PurchaseClient 초기화\n",
      "private final PurchaseClient purchaseClient;\n",
      "\n",
      "@Override\n",
      "protected void onCreate(Bundle savedInstanceState) {\n",
      "    super.onCreate(savedInstanceState);\n",
      "    setContentView(R.layout.activity_main);\n",
      "\n",
      "    purchaseClient = PurchaseClient.newBuilder(this)\n",
      "        .setListener(listener)          // 리스너 설정\n",
      "        .setBase64PublicKey(\"YOUR_PUBLIC_KEY\")  // 공개키 설정 (선택 사항)\n",
      "        .build();\n",
      "\n",
      "    // 구매 플로우 시작 예시\n",
      "    startPurchaseFlow();\n",
      "}\n",
      "\n",
      "// 구매 플로우 시작 메서드\n",
      "private void startPurchaseFlow() {\n",
      "    PurchaseFlowParams params = new PurchaseFlowParams.Builder()\n",
      "        .setProductId(\"YOUR_PRODUCT_ID\")  // 상품 ID 설정\n",
      "        .build();\n",
      "\n",
      "    purchaseClient.launchPurchaseFlow(this, params);\n",
      "}\n",
      "```\n",
      "\n",
      "### 주요 포인트\n",
      "1. **리스너 설정**: `setListener()` 메서드를 사용하여 구매 결과를 처리할 `PurchasesUpdatedListener`를 설정합니다.\n",
      "2. **공개키 설정**: 보안을 위해 앱 서버에서 전달받은 공개키를 설정하는 것이 권장됩니다 (`setBase64PublicKey`).\n",
      "3. **구매 플로우 시작**: `launchPurchaseFlow()` 메서드를 호출하여 구매 프로세스를 시작합니다. 이때 `Activity`와 `PurchaseFlowParams`를 인자로 전달합니다.\n",
      "\n",
      "**참고**: `YOUR_PUBLIC_KEY`, `YOUR_PRODUCT_ID` 등의 플레이스홀더는 실제 앱에서 사용할 키와 상품 ID로 대체해야 합니다. 또한, 보안을 위해 공개키는 서버를 통해 안전하게 전달받는 것이 좋습니다.------------------------------------------------------------\n",
      "⏱️  응답 시간: 9.16초\n",
      "\n",
      "💡 답변:\n",
      "PurchaseClient를 초기화하는 방법은 다음과 같습니다. 주로 Kotlin과 Java 언어를 사용하여 구현하는 방법을 설명드리겠습니다. 주어진 문서를 바탕으로 단계별로 설명드리겠습니다.\n",
      "\n",
      "### Kotlin 구현 예시\n",
      "\n",
      "1. **PurchasesUpdatedListener 구현**: 구매 결과를 처리할 리스너를 먼저 정의합니다.\n",
      "2. **PurchaseClient Builder 사용**: `PurchaseClient.newBuilder()`를 사용하여 PurchaseClient 인스턴스를 생성하고 필요한 설정을 적용합니다.\n",
      "\n",
      "```kotlin\n",
      "// 1. 구매 결과 리스너 정의\n",
      "private val listener = PurchasesUpdatedListener { iapResult, purchases ->\n",
      "    // 구매 결과 처리 로직\n",
      "    when (iapResult.status) {\n",
      "        IapResult.Status.SUCCESS -> {\n",
      "            // 구매 성공 시 처리 로직\n",
      "            Log.d(\"PurchaseClient\", \"Purchase successful: ${purchases.firstOrNull()?.purchaseToken}\")\n",
      "        }\n",
      "        IapResult.Status.ERROR -> {\n",
      "            // 구매 실패 시 처리 로직\n",
      "            Log.e(\"PurchaseClient\", \"Purchase failed: ${iapResult.errorMessage}\")\n",
      "        }\n",
      "        else -> {\n",
      "            // 기타 상태 처리 로직\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "// 2. PurchaseClient 초기화\n",
      "private var purchaseClient = PurchaseClient.newBuilder(activity)\n",
      "    .setListener(listener)          // 리스너 설정\n",
      "    .setBase64PublicKey(\"YOUR_PUBLIC_KEY\")  // 공개키 설정 (선택 사항)\n",
      "    .build()\n",
      "\n",
      "// 구매 플로우 시작\n",
      "fun startPurchaseFlow() {\n",
      "    val purchaseFlowParams = PurchaseFlowParams.newBuilder()\n",
      "        .setProductId(\"YOUR_PRODUCT_ID\")  // 상품 ID 설정\n",
      "        .build()\n",
      "\n",
      "    purchaseClient.launchPurchaseFlow(activity, purchaseFlowParams)\n",
      "}\n",
      "```\n",
      "\n",
      "### Java 구현 예시\n",
      "\n",
      "1. **PurchasesUpdatedListener 구현**: 구매 결과를 처리할 리스너를 정의합니다.\n",
      "2. **PurchaseClient Builder 사용**: `PurchaseClient.newBuilder()`를 사용하여 PurchaseClient 인스턴스를 생성하고 필요한 설정을 적용합니다.\n",
      "\n",
      "```java\n",
      "// 1. 구매 결과 리스너 정의\n",
      "private final PurchasesUpdatedListener listener = new PurchasesUpdatedListener() {\n",
      "    @Override\n",
      "    public void onPurchasesUpdated(IapResult iapResult, List<PurchaseData> purchases) {\n",
      "        switch (iapResult.getStatus()) {\n",
      "            case SUCCESS:\n",
      "                // 구매 성공 시 처리 로직\n",
      "                Log.d(\"PurchaseClient\", \"Purchase successful: \" + purchases.get(0).getPurchaseToken());\n",
      "                break;\n",
      "            case ERROR:\n",
      "                // 구매 실패 시 처리 로직\n",
      "                Log.e(\"PurchaseClient\", \"Purchase failed: \" + iapResult.getErrorMessage());\n",
      "                break;\n",
      "            default:\n",
      "                // 기타 상태 처리 로직\n",
      "                break;\n",
      "        }\n",
      "    }\n",
      "};\n",
      "\n",
      "// 2. PurchaseClient 초기화\n",
      "private final PurchaseClient purchaseClient;\n",
      "\n",
      "@Override\n",
      "protected void onCreate(Bundle savedInstanceState) {\n",
      "    super.onCreate(savedInstanceState);\n",
      "    setContentView(R.layout.activity_main);\n",
      "\n",
      "    purchaseClient = PurchaseClient.newBuilder(this)\n",
      "        .setListener(listener)          // 리스너 설정\n",
      "        .setBase64PublicKey(\"YOUR_PUBLIC_KEY\")  // 공개키 설정 (선택 사항)\n",
      "        .build();\n",
      "\n",
      "    // 구매 플로우 시작 예시\n",
      "    startPurchaseFlow();\n",
      "}\n",
      "\n",
      "// 구매 플로우 시작 메서드\n",
      "private void startPurchaseFlow() {\n",
      "    PurchaseFlowParams params = new PurchaseFlowParams.Builder()\n",
      "        .setProductId(\"YOUR_PRODUCT_ID\")  // 상품 ID 설정\n",
      "        .build();\n",
      "\n",
      "    purchaseClient.launchPurchaseFlow(this, params);\n",
      "}\n",
      "```\n",
      "\n",
      "### 주요 포인트\n",
      "1. **리스너 설정**: `setListener()` 메서드를 사용하여 구매 결과를 처리할 `PurchasesUpdatedListener`를 설정합니다.\n",
      "2. **공개키 설정**: 보안을 위해 앱 서버에서 전달받은 공개키를 설정하는 것이 권장됩니다 (`setBase64PublicKey`).\n",
      "3. **구매 플로우 시작**: `launchPurchaseFlow()` 메서드를 호출하여 구매 프로세스를 시작합니다. 이때 `Activity`와 `PurchaseFlowParams`를 인자로 전달합니다.\n",
      "\n",
      "**참고**: `YOUR_PUBLIC_KEY`, `YOUR_PRODUCT_ID` 등의 플레이스홀더는 실제 앱에서 사용할 키와 상품 ID로 대체해야 합니다. 또한, 보안을 위해 공개키는 서버를 통해 안전하게 전달받는 것이 좋습니다.\n",
      "\n",
      "⏱️  응답 시간: 9.16초\n",
      "\n",
      "\n",
      "\n",
      "********************************************************************************************************************************************************************************************************\n",
      "\n",
      "[질문 2] PNS의 purchaseState 값의 종류는 무엇인가요?\n",
      "🤔 답변 생성 중...\n",
      "\n",
      "🔍 질문: PNS의 purchaseState 값의 종류는 무엇인가요?\n",
      "============================================================\n",
      "관련 문서 검색 중...\n",
      "✓ 5개 문서 검색됨\n",
      "\n",
      "💭 답변 생성 중...\n",
      "------------------------------------------------------------\n",
      "PNS의 `purchaseState` 값은 결제 트랜잭션의 상태를 나타내며 다음과 같은 두 가지 주요 종류가 있습니다:\n",
      "\n",
      "1. **COMPLETED**: 결제가 성공적으로 완료된 상태를 나타냅니다. 사용자가 인앱 상품이나 서비스를 구매하고 결제가 정상적으로 처리된 경우 이 상태가 반환됩니다.\n",
      "\n",
      "2. **CANCELED**: 결제가 취소된 상태를 나타냅니다. 사용자가 결제를 취소하거나 결제 프로세스 중 문제가 발생하여 결제가 완료되지 않은 경우 이 상태가 반환됩니다.\n",
      "\n",
      "### 코드 예제\n",
      "\n",
      "개발사 서버에서 `purchaseState` 값을 확인하는 예제 코드는 다음과 같습니다:\n",
      "\n",
      "```java\n",
      "// Java 예제 코드\n",
      "public class PaymentNotificationHandler {\n",
      "    public void handlePaymentNotification(String purchaseId, String purchaseState) {\n",
      "        switch (purchaseState) {\n",
      "            case \"COMPLETED\":\n",
      "                System.out.println(\"결제 완료 알림: 구매 ID - \" + purchaseId);\n",
      "                // 결제 완료에 따른 후속 처리 로직\n",
      "                break;\n",
      "            case \"CANCELED\":\n",
      "                System.out.println(\"결제 취소 알림: 구매 ID - \" + purchaseId);\n",
      "                // 결제 취소에 따른 후속 처리 로직\n",
      "                break;\n",
      "            default:\n",
      "                System.out.println(\"알 수 없는 결제 상태: \" + purchaseState + \", 구매 ID - \" + purchaseId);\n",
      "                // 예상치 못한 상태에 대한 처리 로직\n",
      "                break;\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "// 사용 예시\n",
      "PaymentNotificationHandler handler = new PaymentNotificationHandler();\n",
      "handler.handlePaymentNotification(\"구매ID12345\", \"COMPLETED\");\n",
      "handler.handlePaymentNotification(\"구매ID67890\", \"CANCELED\");\n",
      "```\n",
      "\n",
      "### 단계별 설명\n",
      "\n",
      "1. **메시지 수신**: 개발사 서버는 원스토어로부터 `purchaseState` 메시지를 수신합니다.\n",
      "2. **상태 확인**: 수신된 `purchaseState` 값을 확인합니다.\n",
      "   - `COMPLETED`인 경우 결제가 성공적으로 완료되었음을 인지합니다.\n",
      "   - `CANCELED`인 경우 결제가 취소되었음을 인지합니다.\n",
      "3. **후속 처리**: 각 상태에 따라 적절한 비즈니스 로직을 실행합니다 (예: 결제 완료 시 상품 제공, 결제 취소 시 취소 처리 등).\n",
      "\n",
      "**참고**: 문서에서 명시된 이외의 상태 값이 수신될 경우, 이는 예상치 못한 상황이므로 추가적인 검증이나 오류 처리가 필요할 수 있습니다. 문서에서 확인되지 않음: 추가 상태 값의 존재 여부.------------------------------------------------------------\n",
      "⏱️  응답 시간: 5.18초\n",
      "\n",
      "💡 답변:\n",
      "PNS의 `purchaseState` 값은 결제 트랜잭션의 상태를 나타내며 다음과 같은 두 가지 주요 종류가 있습니다:\n",
      "\n",
      "1. **COMPLETED**: 결제가 성공적으로 완료된 상태를 나타냅니다. 사용자가 인앱 상품이나 서비스를 구매하고 결제가 정상적으로 처리된 경우 이 상태가 반환됩니다.\n",
      "\n",
      "2. **CANCELED**: 결제가 취소된 상태를 나타냅니다. 사용자가 결제를 취소하거나 결제 프로세스 중 문제가 발생하여 결제가 완료되지 않은 경우 이 상태가 반환됩니다.\n",
      "\n",
      "### 코드 예제\n",
      "\n",
      "개발사 서버에서 `purchaseState` 값을 확인하는 예제 코드는 다음과 같습니다:\n",
      "\n",
      "```java\n",
      "// Java 예제 코드\n",
      "public class PaymentNotificationHandler {\n",
      "    public void handlePaymentNotification(String purchaseId, String purchaseState) {\n",
      "        switch (purchaseState) {\n",
      "            case \"COMPLETED\":\n",
      "                System.out.println(\"결제 완료 알림: 구매 ID - \" + purchaseId);\n",
      "                // 결제 완료에 따른 후속 처리 로직\n",
      "                break;\n",
      "            case \"CANCELED\":\n",
      "                System.out.println(\"결제 취소 알림: 구매 ID - \" + purchaseId);\n",
      "                // 결제 취소에 따른 후속 처리 로직\n",
      "                break;\n",
      "            default:\n",
      "                System.out.println(\"알 수 없는 결제 상태: \" + purchaseState + \", 구매 ID - \" + purchaseId);\n",
      "                // 예상치 못한 상태에 대한 처리 로직\n",
      "                break;\n",
      "        }\n",
      "    }\n",
      "}\n",
      "\n",
      "// 사용 예시\n",
      "PaymentNotificationHandler handler = new PaymentNotificationHandler();\n",
      "handler.handlePaymentNotification(\"구매ID12345\", \"COMPLETED\");\n",
      "handler.handlePaymentNotification(\"구매ID67890\", \"CANCELED\");\n",
      "```\n",
      "\n",
      "### 단계별 설명\n",
      "\n",
      "1. **메시지 수신**: 개발사 서버는 원스토어로부터 `purchaseState` 메시지를 수신합니다.\n",
      "2. **상태 확인**: 수신된 `purchaseState` 값을 확인합니다.\n",
      "   - `COMPLETED`인 경우 결제가 성공적으로 완료되었음을 인지합니다.\n",
      "   - `CANCELED`인 경우 결제가 취소되었음을 인지합니다.\n",
      "3. **후속 처리**: 각 상태에 따라 적절한 비즈니스 로직을 실행합니다 (예: 결제 완료 시 상품 제공, 결제 취소 시 취소 처리 등).\n",
      "\n",
      "**참고**: 문서에서 명시된 이외의 상태 값이 수신될 경우, 이는 예상치 못한 상황이므로 추가적인 검증이나 오류 처리가 필요할 수 있습니다. 문서에서 확인되지 않음: 추가 상태 값의 존재 여부.\n",
      "\n",
      "⏱️  응답 시간: 5.18초\n",
      "\n",
      "\n",
      "\n",
      "********************************************************************************************************************************************************************************************************\n",
      "\n",
      "[질문 3] PNS 서비스란 무엇인가요?\n",
      "🤔 답변 생성 중...\n",
      "\n",
      "🔍 질문: PNS 서비스란 무엇인가요?\n",
      "============================================================\n",
      "관련 문서 검색 중...\n",
      "✓ 5개 문서 검색됨\n",
      "\n",
      "💭 답변 생성 중...\n",
      "------------------------------------------------------------\n",
      "PNS (Payment Notification Service)는 **원스토어에서 제공하는 서버 간 결제 알림 서비스**입니다. 주요 기능과 특징은 다음과 같습니다:\n",
      "\n",
      "1. **목적**:\n",
      "   - 모바일 네트워크 연결의 불안정성을 보완하여 사용자의 결제 상태 (결제 완료, 결제 취소 등)를 안정적으로 전달합니다.\n",
      "   - 개발사가 지정한 서버로 원스토어의 결제 상태 알림 메시지를 전송합니다.\n",
      "\n",
      "2. **작동 방식**:\n",
      "   - **서버 간 통신 (Server to Server)**: 원스토어 서버가 개발사 서버로 결제 트랜잭션 상태 메시지를 전송합니다.\n",
      "   - **반복 전송 메커니즘**: 네트워크 문제로 인해 메시지 전송이 실패할 경우, 자동으로 재전송됩니다.\n",
      "   - **응답 확인**: 개발사 서버는 메시지 수신 후 정의된 응답을 통해 원스토어에 정상 수신을 확인합니다.\n",
      "\n",
      "3. **알림 유형**:\n",
      "   - **결제 알림**: 인앱 상품 결제 또는 결제 취소 시 알림 전송.\n",
      "   - **구독 알림**: 구독 상태 변경 시 알림 전송 (SNS - Subscription Notification Service와 관련).\n",
      "\n",
      "4. **주의사항**:\n",
      "   - **지연 및 유실 가능성**: 네트워크 상태에 따라 알림 지연 또는 유실 가능성이 있으므로, 알림 수신을 기준으로 상품 제공은 권장되지 않습니다.\n",
      "   - **대체 방법**: 결제 상태 확인을 위해서는 직접 서버 API를 통해 조회하는 것이 더 안정적입니다.\n",
      "\n",
      "### 코드 예제 (PNS 메시지 발송)\n",
      "\n",
      "아래는 개발사 서버에서 원스토어 서버로 PNS 메시지를 발송하는 예시 코드입니다 (Python 기반):\n",
      "\n",
      "```python\n",
      "import requests\n",
      "import json\n",
      "\n",
      "# 설정된 Payment Notification URL\n",
      "payment_notification_url = \"https://your-payment-notification-url.com\"\n",
      "\n",
      "# 예시 결제 알림 메시지 데이터 (JSON 형식)\n",
      "payment_notification_data = {\n",
      "    \"transaction_id\": \"TX123456789\",\n",
      "    \"status\": \"completed\",  # \"completed\", \"canceled\" 등\n",
      "    \"user_id\": \"user123\",\n",
      "    \"timestamp\": \"2023-10-05T14:48:00Z\"\n",
      "}\n",
      "\n",
      "# 요청 헤더 설정\n",
      "headers = {\n",
      "    \"Content-Type\": \"application/json\"\n",
      "}\n",
      "\n",
      "# POST 요청 발송\n",
      "response = requests.post(\n",
      "    payment_notification_url,\n",
      "    data=json.dumps(payment_notification_data),\n",
      "    headers=headers\n",
      ")\n",
      "\n",
      "# 응답 확인\n",
      "if response.status_code == 200:\n",
      "    print(\"알림 메시지 성공적으로 전송되었습니다.\")\n",
      "else:\n",
      "    print(f\"알림 메시지 전송 실패: HTTP 상태 코드 {response.status_code}, 응답: {response.text}\")\n",
      "```\n",
      "\n",
      "### 단계별 설명:\n",
      "1. **URL 설정**: 개발사 서버에서 설정된 `Payment Notification URL`을 지정합니다.\n",
      "2. **데이터 준비**: 결제 알림 메시지 데이터를 JSON 형식으로 준비합니다.\n",
      "3. **헤더 설정**: `Content-Type` 헤더를 `application/json`으로 설정합니다.\n",
      "4. **POST 요청 발송**: `requests.post`를 사용하여 원스토어 서버로 메시지를 전송합니다.\n",
      "5. **응답 확인**: 응답 상태 코드를 확인하여 성공 여부를 판단합니다.\n",
      "\n",
      "이 방식을 통해 개발사는 사용자 결제 상태에 대한 안정적인 알림을 받을 수 있습니다.------------------------------------------------------------\n",
      "⏱️  응답 시간: 6.48초\n",
      "\n",
      "💡 답변:\n",
      "PNS (Payment Notification Service)는 **원스토어에서 제공하는 서버 간 결제 알림 서비스**입니다. 주요 기능과 특징은 다음과 같습니다:\n",
      "\n",
      "1. **목적**:\n",
      "   - 모바일 네트워크 연결의 불안정성을 보완하여 사용자의 결제 상태 (결제 완료, 결제 취소 등)를 안정적으로 전달합니다.\n",
      "   - 개발사가 지정한 서버로 원스토어의 결제 상태 알림 메시지를 전송합니다.\n",
      "\n",
      "2. **작동 방식**:\n",
      "   - **서버 간 통신 (Server to Server)**: 원스토어 서버가 개발사 서버로 결제 트랜잭션 상태 메시지를 전송합니다.\n",
      "   - **반복 전송 메커니즘**: 네트워크 문제로 인해 메시지 전송이 실패할 경우, 자동으로 재전송됩니다.\n",
      "   - **응답 확인**: 개발사 서버는 메시지 수신 후 정의된 응답을 통해 원스토어에 정상 수신을 확인합니다.\n",
      "\n",
      "3. **알림 유형**:\n",
      "   - **결제 알림**: 인앱 상품 결제 또는 결제 취소 시 알림 전송.\n",
      "   - **구독 알림**: 구독 상태 변경 시 알림 전송 (SNS - Subscription Notification Service와 관련).\n",
      "\n",
      "4. **주의사항**:\n",
      "   - **지연 및 유실 가능성**: 네트워크 상태에 따라 알림 지연 또는 유실 가능성이 있으므로, 알림 수신을 기준으로 상품 제공은 권장되지 않습니다.\n",
      "   - **대체 방법**: 결제 상태 확인을 위해서는 직접 서버 API를 통해 조회하는 것이 더 안정적입니다.\n",
      "\n",
      "### 코드 예제 (PNS 메시지 발송)\n",
      "\n",
      "아래는 개발사 서버에서 원스토어 서버로 PNS 메시지를 발송하는 예시 코드입니다 (Python 기반):\n",
      "\n",
      "```python\n",
      "import requests\n",
      "import json\n",
      "\n",
      "# 설정된 Payment Notification URL\n",
      "payment_notification_url = \"https://your-payment-notification-url.com\"\n",
      "\n",
      "# 예시 결제 알림 메시지 데이터 (JSON 형식)\n",
      "payment_notification_data = {\n",
      "    \"transaction_id\": \"TX123456789\",\n",
      "    \"status\": \"completed\",  # \"completed\", \"canceled\" 등\n",
      "    \"user_id\": \"user123\",\n",
      "    \"timestamp\": \"2023-10-05T14:48:00Z\"\n",
      "}\n",
      "\n",
      "# 요청 헤더 설정\n",
      "headers = {\n",
      "    \"Content-Type\": \"application/json\"\n",
      "}\n",
      "\n",
      "# POST 요청 발송\n",
      "response = requests.post(\n",
      "    payment_notification_url,\n",
      "    data=json.dumps(payment_notification_data),\n",
      "    headers=headers\n",
      ")\n",
      "\n",
      "# 응답 확인\n",
      "if response.status_code == 200:\n",
      "    print(\"알림 메시지 성공적으로 전송되었습니다.\")\n",
      "else:\n",
      "    print(f\"알림 메시지 전송 실패: HTTP 상태 코드 {response.status_code}, 응답: {response.text}\")\n",
      "```\n",
      "\n",
      "### 단계별 설명:\n",
      "1. **URL 설정**: 개발사 서버에서 설정된 `Payment Notification URL`을 지정합니다.\n",
      "2. **데이터 준비**: 결제 알림 메시지 데이터를 JSON 형식으로 준비합니다.\n",
      "3. **헤더 설정**: `Content-Type` 헤더를 `application/json`으로 설정합니다.\n",
      "4. **POST 요청 발송**: `requests.post`를 사용하여 원스토어 서버로 메시지를 전송합니다.\n",
      "5. **응답 확인**: 응답 상태 코드를 확인하여 성공 여부를 판단합니다.\n",
      "\n",
      "이 방식을 통해 개발사는 사용자 결제 상태에 대한 안정적인 알림을 받을 수 있습니다.\n",
      "\n",
      "⏱️  응답 시간: 6.48초\n",
      "\n",
      "\n",
      "\n",
      "********************************************************************************************************************************************************************************************************\n",
      "\n",
      "📊 파이프라인 통계:\n",
      "  - 총 문서 청크: 5912\n",
      "  - 인덱싱 시간: 73.25초\n",
      "  - 처리된 쿼리: 3\n",
      "  - 마지막 쿼리 시간: 6.48초\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "수정된 테스트 함수\n",
    "\"\"\"\n",
    "\n",
    "def demo_fixed_test():\n",
    "    \"\"\"수정된 빠른 테스트 데모\"\"\"\n",
    "    print(\"🚀 수정된 테스트 데모 시작\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # 설정\n",
    "    data_file = \"data/dev_center_guide_allmd_touched.md\"\n",
    "    \n",
    "    # 데이터 파일 존재 확인\n",
    "    if not os.path.exists(data_file):\n",
    "        print(f\"❌ 데이터 파일을 찾을 수 없습니다: {data_file}\")\n",
    "        print(\"경로를 수정하거나 파일을 확인해주세요.\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        # 수정된 파이프라인 생성\n",
    "        print(\"파이프라인 초기화 중...\")\n",
    "        pipeline = create_fixed_pipeline(\n",
    "            data_file=data_file,\n",
    "            force_rebuild=False  # 기존 인덱스 재사용\n",
    "        )\n",
    "        \n",
    "        print(\"\\n✅ 파이프라인 준비 완료!\")\n",
    "        \n",
    "        # 테스트 질문들\n",
    "        test_questions = [\n",
    "            \"PurchaseClient를 어떻게 초기화하나요?\",\n",
    "            \"PNS의 purchaseState 값의 종류는 무엇인가요?\",\n",
    "            \"PNS 서비스란 무엇인가요?\"\n",
    "        ]\n",
    "        \n",
    "        print(\"\\n📝 테스트 질문 실행:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        for i, question in enumerate(test_questions, 1):\n",
    "            print(f\"\\n[질문 {i}] {question}\")\n",
    "            print(\"🤔 답변 생성 중...\")\n",
    "            \n",
    "            start_time = time.time()\n",
    "            result = pipeline.query(question, stream=False)\n",
    "            elapsed = time.time() - start_time\n",
    "            \n",
    "            print(\"\\n💡 답변:\")\n",
    "            print(result['answer'])\n",
    "            print(f\"\\n⏱️  응답 시간: {elapsed:.2f}초\")\n",
    "            print(\"\\n\\n\")\n",
    "            print(\"*\" * 200)\n",
    "        \n",
    "        # 통계 출력\n",
    "        stats = pipeline.get_statistics()\n",
    "        print(\"\\n📊 파이프라인 통계:\")\n",
    "        print(f\"  - 총 문서 청크: {stats['total_chunks']}\")\n",
    "        print(f\"  - 인덱싱 시간: {stats['index_build_time']:.2f}초\")\n",
    "        print(f\"  - 처리된 쿼리: {stats['queries_processed']}\")\n",
    "        print(f\"  - 마지막 쿼리 시간: {stats['last_query_time']:.2f}초\")\n",
    "        \n",
    "        return pipeline\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ 오류 발생: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return None\n",
    "\n",
    "# 실행 - 다시 시도\n",
    "print(\"🔄 수정된 클래스로 다시 테스트...\")\n",
    "pipeline = demo_fixed_test()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
