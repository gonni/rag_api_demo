{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b95b910c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import pickle\n",
    "from typing import List\n",
    "from pathlib import Path\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_ollama import OllamaEmbeddings\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "\n",
    "class CommonRAGUtil:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def save_documents(self, docs: List[Document], output_path: str):\n",
    "        \"\"\"\n",
    "        List[Document]ë¥¼ pickle íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "        \n",
    "        Args:\n",
    "            docs (List[Document]): ì €ì¥í•  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
    "            output_path (str): ì €ì¥í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ\n",
    "        \"\"\"\n",
    "        os.makedirs(output_path, exist_ok=True)\n",
    "        docs_file_path = os.path.join(output_path, \"documents.pkl\")\n",
    "        \n",
    "        with open(docs_file_path, \"wb\") as f:\n",
    "            pickle.dump(docs, f)\n",
    "        \n",
    "        print(f\"âœ… ë¬¸ì„œ ì €ì¥ ì™„ë£Œ: {docs_file_path}\")\n",
    "        print(f\"ğŸ“„ ì €ì¥ëœ ë¬¸ì„œ ìˆ˜: {len(docs)}\")\n",
    "\n",
    "\n",
    "    def load_documents(self, input_path: str) -> List[Document]:\n",
    "        \"\"\"\n",
    "        ì €ì¥ëœ pickle íŒŒì¼ì—ì„œ List[Document]ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "        \n",
    "        Args:\n",
    "            input_path (str): ë¬¸ì„œê°€ ì €ì¥ëœ ë””ë ‰í† ë¦¬ ê²½ë¡œ ë˜ëŠ” íŒŒì¼ ê²½ë¡œ\n",
    "            \n",
    "        Returns:\n",
    "            List[Document]: ë¡œë“œëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
    "        \"\"\"\n",
    "        # ë””ë ‰í† ë¦¬ ê²½ë¡œì¸ ê²½ìš° documents.pkl íŒŒì¼ì„ ì°¾ìŒ\n",
    "        if os.path.isdir(input_path):\n",
    "            docs_file_path = os.path.join(input_path, \"documents.pkl\")\n",
    "        else:\n",
    "            docs_file_path = input_path\n",
    "        \n",
    "        if not os.path.exists(docs_file_path):\n",
    "            raise FileNotFoundError(f\"ë¬¸ì„œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {docs_file_path}\")\n",
    "        \n",
    "        with open(docs_file_path, \"rb\") as f:\n",
    "            docs = pickle.load(f)\n",
    "        \n",
    "        print(f\"âœ… ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ: {docs_file_path}\")\n",
    "        print(f\"ğŸ“„ ë¡œë“œëœ ë¬¸ì„œ ìˆ˜: {len(docs)}\")\n",
    "        \n",
    "        return docs\n",
    "\n",
    "\n",
    "    def embed_and_save_with_docs(self, docs: List[Document], output_path: str, model_name: str = \"bge-m3:latest\"):\n",
    "        \"\"\"\n",
    "        ë¬¸ì„œë¥¼ ì„ë² ë”©í•˜ê³  FAISS ë°ì´í„°ë² ì´ìŠ¤ë¡œ ì €ì¥í•˜ë©°, ë™ì‹œì— ì›ë³¸ ë¬¸ì„œë„ ì €ì¥í•©ë‹ˆë‹¤.\n",
    "        \n",
    "        Args:\n",
    "            docs (List[Document]): ì²˜ë¦¬í•  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸\n",
    "            output_path (str): ì €ì¥í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ\n",
    "            model_name (str): ì„ë² ë”© ëª¨ë¸ëª…\n",
    "        \"\"\"\n",
    "        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”\n",
    "        embedding_model = OllamaEmbeddings(model=model_name)\n",
    "        \n",
    "        # FAISS ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ë° ì €ì¥\n",
    "        db = FAISS.from_documents(docs, embedding_model)\n",
    "        db.save_local(output_path)\n",
    "        print(f\"âœ… ì„ë² ë”© ì €ì¥ ì™„ë£Œ: {output_path}\")\n",
    "        \n",
    "        # ì›ë³¸ ë¬¸ì„œë„ í•¨ê»˜ ì €ì¥\n",
    "        self.save_documents(docs, output_path)\n",
    "        \n",
    "\n",
    "    def load_both_faiss_and_docs(self, folder_path: str, model_name: str = \"bge-m3:latest\") -> tuple[FAISS, List[Document]]:\n",
    "        \"\"\"\n",
    "        FAISS ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì™€ ì›ë³¸ ë¬¸ì„œë¥¼ ëª¨ë‘ ë¡œë“œí•©ë‹ˆë‹¤.\n",
    "        \n",
    "        Args:\n",
    "            folder_path (str): ë°ì´í„°ê°€ ì €ì¥ëœ ë””ë ‰í† ë¦¬ ê²½ë¡œ\n",
    "            model_name (str): ì„ë² ë”© ëª¨ë¸ëª…\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (FAISS ë°ì´í„°ë² ì´ìŠ¤, List[Document])\n",
    "        \"\"\"\n",
    "        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”\n",
    "        embedding_model = OllamaEmbeddings(model=model_name)\n",
    "        \n",
    "        # FAISS ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ\n",
    "        loaded_db = FAISS.load_local(\n",
    "            folder_path=folder_path,\n",
    "            embeddings=embedding_model,\n",
    "            allow_dangerous_deserialization=True,\n",
    "        )\n",
    "        print(f\"âœ… FAISS ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì™„ë£Œ: {folder_path}\")\n",
    "        \n",
    "        # ì›ë³¸ ë¬¸ì„œ ë¡œë“œ\n",
    "        docs = self.load_documents(folder_path)\n",
    "        \n",
    "        return loaded_db, docs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
