# Enhanced RAG Pipeline for Long Documents and Complex Concept Systems
# - Hierarchical document analysis
# - Concept mapping and categorization  
# - Multi-stage retrieval strategy
# - Context window expansion
# - Overview-first approach for comprehensive understanding

import os, re, json
from typing import List, Dict, Any, Tuple, Optional, Set
from dataclasses import dataclass
from collections import Counter, defaultdict
import networkx as nx

from langchain.docstore.document import Document
from langchain_ollama import OllamaEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from langchain_ollama import ChatOllama
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

try:
    from langchain_text_splitters import MarkdownHeaderTextSplitter
    HAS_MD_SPLITTER = True
except Exception:
    HAS_MD_SPLITTER = False

# Configuration
DATA_FILE = "../data/dev_center_guide_allmd_touched.md"
EMBED_MODEL = "bge-m3:latest"
LLM_MODEL = "exaone3.5:latest"
FAISS_DIR = "../models/faiss_rag_enhanced_bge-m3_hierarchical"

print("Enhanced RAG Config:")
print(f"- DATA_FILE: {DATA_FILE}")
print(f"- EMBED_MODEL: {EMBED_MODEL}")
print(f"- LLM_MODEL: {LLM_MODEL}")
print(f"- FAISS_DIR:  {FAISS_DIR}")

# ============================================================================
# Hierarchical Document Analysis
# ============================================================================

@dataclass
class DocumentNode:
    """ê³„ì¸µì  ë¬¸ì„œ êµ¬ì¡°ë¥¼ í‘œí˜„í•˜ëŠ” ë…¸ë“œ"""
    id: str
    title: str
    content: str
    level: int
    parent_id: Optional[str] = None
    children: List[str] = None
    metadata: Dict[str, Any] = None
    concept_type: str = "section"  # section, concept, overview, detail
    
    def __post_init__(self):
        if self.children is None:
            self.children = []
        if self.metadata is None:
            self.metadata = {}

class HierarchicalDocumentAnalyzer:
    """ê¸´ ë¬¸ì„œì˜ ê³„ì¸µì  êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ê³  ê°œë…ì„ ë§¤í•‘í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self):
        self.concept_patterns = {
            'numbered_concept': re.compile(r'([A-Z]{2,})_(\d+)'),  # PNS_1, API_2 ë“±
            'versioned_concept': re.compile(r'([A-Z]{2,})\s*V?(\d+(?:\.\d+)*)'),  # PNS V1, API V2.1 ë“±
            'sectioned_concept': re.compile(r'(\d+)\.\s*([A-Z][A-Za-z\s]+)'),  # 1. PNS ê°œìš”, 2. API ì„¤ëª… ë“±
        }
        self.overview_keywords = {
            'ê°œìš”', 'overview', 'introduction', 'ê°œë…', 'concept', 'ì •ì˜', 'definition',
            'ì„¤ëª…', 'description', 'ì†Œê°œ', 'introduction', 'ê¸°ë³¸', 'basic', 'fundamental'
        }
        self.detail_keywords = {
            'ìƒì„¸', 'detail', 'êµ¬ì²´', 'specific', 'ì˜ˆì‹œ', 'example', 'ì‚¬ìš©ë²•', 'usage',
            'êµ¬í˜„', 'implementation', 'ì½”ë“œ', 'code', 'ë§¤ê°œë³€ìˆ˜', 'parameter'
        }
    
    def analyze_document_structure(self, docs: List[Document]) -> Dict[str, DocumentNode]:
        """ë¬¸ì„œë“¤ì˜ ê³„ì¸µì  êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ì—¬ ë…¸ë“œ íŠ¸ë¦¬ ìƒì„±"""
        nodes = {}
        
        for doc in docs:
            metadata = doc.metadata or {}
            full_path = metadata.get('full_path', [])
            
            if not full_path:
                continue
                
            # ë…¸ë“œ ID ìƒì„±
            node_id = "_".join(full_path)
            
            # ë ˆë²¨ ê²°ì • (í—¤ë” ê¹Šì´)
            level = len(full_path)
            
            # ë¶€ëª¨ ë…¸ë“œ ì°¾ê¸°
            parent_id = None
            if level > 1:
                parent_path = full_path[:-1]
                parent_id = "_".join(parent_path)
            
            # ê°œë… íƒ€ì… ê²°ì •
            concept_type = self._determine_concept_type(full_path[-1], doc.page_content)
            
            # ë…¸ë“œ ìƒì„±
            node = DocumentNode(
                id=node_id,
                title=full_path[-1],
                content=doc.page_content,
                level=level,
                parent_id=parent_id,
                concept_type=concept_type,
                metadata=metadata
            )
            
            nodes[node_id] = node
        
        # ë¶€ëª¨-ìì‹ ê´€ê³„ ì„¤ì •
        for node_id, node in nodes.items():
            if node.parent_id and node.parent_id in nodes:
                nodes[node.parent_id].children.append(node_id)
        
        return nodes
    
    def _determine_concept_type(self, title: str, content: str) -> str:
        """ì œëª©ê³¼ ë‚´ìš©ì„ ê¸°ë°˜ìœ¼ë¡œ ê°œë… íƒ€ì… ê²°ì •"""
        title_lower = title.lower()
        content_lower = content.lower()
        
        # ê°œìš” íƒ€ì… í™•ì¸
        if any(keyword in title_lower for keyword in self.overview_keywords):
            return "overview"
        
        # ìƒì„¸ íƒ€ì… í™•ì¸
        if any(keyword in title_lower for keyword in self.detail_keywords):
            return "detail"
        
        # ë²ˆí˜¸ê°€ ìˆëŠ” ê°œë… í™•ì¸
        if self.concept_patterns['numbered_concept'].search(title):
            return "concept"
        
        # ë²„ì „ì´ ìˆëŠ” ê°œë… í™•ì¸
        if self.concept_patterns['versioned_concept'].search(title):
            return "concept"
        
        return "section"
    
    def extract_concept_hierarchy(self, nodes: Dict[str, DocumentNode]) -> Dict[str, List[str]]:
        """ê°œë… ê³„ì¸µ êµ¬ì¡° ì¶”ì¶œ (ì˜ˆ: PNS_1, PNS_2, ...)"""
        concept_groups = defaultdict(list)
        
        for node_id, node in nodes.items():
            # ë²ˆí˜¸ê°€ ìˆëŠ” ê°œë… ì°¾ê¸°
            match = self.concept_patterns['numbered_concept'].search(node.title)
            if match:
                base_concept = match.group(1)  # PNS
                concept_groups[base_concept].append(node_id)
        
        # ê° ê·¸ë£¹ ë‚´ì—ì„œ ë²ˆí˜¸ ìˆœìœ¼ë¡œ ì •ë ¬
        for base_concept in concept_groups:
            concept_groups[base_concept].sort(
                key=lambda x: int(self.concept_patterns['numbered_concept'].search(nodes[x].title).group(2))
            )
        
        return dict(concept_groups)
    
    def find_overview_sections(self, nodes: Dict[str, DocumentNode]) -> List[str]:
        """ê°œìš” ì„¹ì…˜ë“¤ ì°¾ê¸°"""
        overview_nodes = []
        for node_id, node in nodes.items():
            if node.concept_type == "overview":
                overview_nodes.append(node_id)
        return overview_nodes
    
    def get_related_sections(self, nodes: Dict[str, DocumentNode], target_node_id: str, max_distance: int = 2) -> List[str]:
        """ê´€ë ¨ ì„¹ì…˜ë“¤ì„ ì°¾ê¸° (ê³„ì¸µì  ê±°ë¦¬ ê¸°ë°˜)"""
        target_node = nodes.get(target_node_id)
        if not target_node:
            return []
        
        related = []
        for node_id, node in nodes.items():
            if node_id == target_node_id:
                continue
            
            # ê³„ì¸µì  ê±°ë¦¬ ê³„ì‚°
            distance = self._calculate_hierarchical_distance(nodes, target_node_id, node_id)
            if distance <= max_distance:
                related.append(node_id)
        
        return related
    
    def _calculate_hierarchical_distance(self, nodes: Dict[str, DocumentNode], node1_id: str, node2_id: str) -> int:
        """ë‘ ë…¸ë“œ ê°„ì˜ ê³„ì¸µì  ê±°ë¦¬ ê³„ì‚°"""
        node1 = nodes.get(node1_id)
        node2 = nodes.get(node2_id)
        
        if not node1 or not node2:
            return float('inf')
        
        # ê³µí†µ ì¡°ìƒ ì°¾ê¸°
        ancestors1 = self._get_ancestors(nodes, node1_id)
        ancestors2 = self._get_ancestors(nodes, node2_id)
        
        # ê³µí†µ ì¡°ìƒì´ ìˆìœ¼ë©´ ê±°ë¦¬ ê³„ì‚°
        for ancestor in ancestors1:
            if ancestor in ancestors2:
                return len(ancestors1) + len(ancestors2) - 2 * len([a for a in ancestors1 if a in ancestors2])
        
        # ê³µí†µ ì¡°ìƒì´ ì—†ìœ¼ë©´ ë ˆë²¨ ì°¨ì´
        return abs(node1.level - node2.level)
    
    def _get_ancestors(self, nodes: Dict[str, DocumentNode], node_id: str) -> List[str]:
        """ë…¸ë“œì˜ ëª¨ë“  ì¡°ìƒë“¤ ì°¾ê¸°"""
        ancestors = []
        current = nodes.get(node_id)
        
        while current and current.parent_id:
            ancestors.append(current.parent_id)
            current = nodes.get(current.parent_id)
        
        return ancestors

# ============================================================================
# Multi-Stage Retrieval Strategy
# ============================================================================

class MultiStageRetriever:
    """ë‹¤ë‹¨ê³„ ê²€ìƒ‰ ì „ëµ: ê°œìš” â†’ ê°œë… â†’ ì„¸ë¶€ì‚¬í•­"""
    
    def __init__(self, analyzer: HierarchicalDocumentAnalyzer, embeddings, docs: List[Document]):
        self.analyzer = analyzer
        self.embeddings = embeddings
        self.docs = docs
        
        # ë¬¸ì„œ êµ¬ì¡° ë¶„ì„
        self.nodes = analyzer.analyze_document_structure(docs)
        self.concept_hierarchy = analyzer.extract_concept_hierarchy(self.nodes)
        self.overview_sections = analyzer.find_overview_sections(self.nodes)
        
        # ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        self.vectorstore = FAISS.from_documents(docs, embeddings)
        
        print(f"ğŸ“Š ë¬¸ì„œ êµ¬ì¡° ë¶„ì„ ì™„ë£Œ:")
        print(f"   - ì´ ë…¸ë“œ: {len(self.nodes)}")
        print(f"   - ê°œë… ê·¸ë£¹: {len(self.concept_hierarchy)}")
        print(f"   - ê°œìš” ì„¹ì…˜: {len(self.overview_sections)}")
    
    def retrieve_comprehensive_context(self, query: str, top_k: int = 15) -> List[Document]:
        """ì „ì²´ì ì¸ ë§¥ë½ì„ íŒŒì•…í•˜ê¸° ìœ„í•œ ë‹¤ë‹¨ê³„ ê²€ìƒ‰"""
        
        # 1ë‹¨ê³„: ê°œìš” ì„¹ì…˜ì—ì„œ ê²€ìƒ‰
        overview_docs = self._retrieve_overview_context(query)
        
        # 2ë‹¨ê³„: ê´€ë ¨ ê°œë…ë“¤ ì°¾ê¸°
        concept_docs = self._retrieve_concept_context(query)
        
        # 3ë‹¨ê³„: ì„¸ë¶€ì‚¬í•­ ê²€ìƒ‰
        detail_docs = self._retrieve_detail_context(query, top_k // 2)
        
        # ê²°ê³¼ í†µí•© ë° ì¤‘ë³µ ì œê±°
        all_docs = overview_docs + concept_docs + detail_docs
        unique_docs = self._deduplicate_docs(all_docs)
        
        return unique_docs[:top_k]
    
    def _retrieve_overview_context(self, query: str) -> List[Document]:
        """ê°œìš” ì„¹ì…˜ì—ì„œ ê²€ìƒ‰"""
        if not self.overview_sections:
            return []
        
        overview_content = []
        for node_id in self.overview_sections:
            node = self.nodes[node_id]
            overview_content.append(node.content)
        
        # ê°œìš” ë‚´ìš©ì„ í•˜ë‚˜ì˜ ë¬¸ì„œë¡œ ê²°í•©
        combined_overview = "\n\n".join(overview_content)
        overview_doc = Document(
            page_content=combined_overview,
            metadata={'type': 'overview_summary', 'sections': self.overview_sections}
        )
        
        return [overview_doc]
    
    def _retrieve_concept_context(self, query: str) -> List[Document]:
        """ê´€ë ¨ ê°œë…ë“¤ ì°¾ê¸°"""
        concept_docs = []
        
        # ì¿¼ë¦¬ì—ì„œ ê°œë… í‚¤ì›Œë“œ ì¶”ì¶œ
        query_keywords = self._extract_concept_keywords(query)
        
        for base_concept, concept_nodes in self.concept_hierarchy.items():
            if any(keyword.lower() in base_concept.lower() for keyword in query_keywords):
                # í•´ë‹¹ ê°œë… ê·¸ë£¹ì˜ ëª¨ë“  ë…¸ë“œ í¬í•¨
                for node_id in concept_nodes:
                    node = self.nodes[node_id]
                    concept_docs.append(Document(
                        page_content=node.content,
                        metadata={'type': 'concept_group', 'concept': base_concept, 'node_id': node_id}
                    ))
        
        return concept_docs
    
    def _retrieve_detail_context(self, query: str, top_k: int) -> List[Document]:
        """ì„¸ë¶€ì‚¬í•­ ê²€ìƒ‰"""
        retriever = self.vectorstore.as_retriever(search_kwargs={"k": top_k * 2})
        docs = retriever.invoke(query)
        return docs[:top_k]
    
    def _extract_concept_keywords(self, query: str) -> List[str]:
        """ì¿¼ë¦¬ì—ì„œ ê°œë… í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = []
        
        # ëŒ€ë¬¸ì ì•½ì–´ íŒ¨í„´
        acronym_pattern = re.compile(r'\b[A-Z]{2,}\b')
        acronyms = acronym_pattern.findall(query)
        keywords.extend(acronyms)
        
        # ì¼ë°˜ì ì¸ ê¸°ìˆ  ìš©ì–´
        tech_terms = ['API', 'SDK', 'PNS', 'IAP', 'ê²°ì œ', 'ì¸ì•±', 'ì•Œë¦¼', 'ì„œë¹„ìŠ¤']
        for term in tech_terms:
            if term.lower() in query.lower():
                keywords.append(term)
        
        return keywords
    
    def _deduplicate_docs(self, docs: List[Document]) -> List[Document]:
        """ì¤‘ë³µ ë¬¸ì„œ ì œê±°"""
        seen = set()
        unique_docs = []
        
        for doc in docs:
            content_hash = hash(doc.page_content[:200])
            if content_hash not in seen:
                seen.add(content_hash)
                unique_docs.append(doc)
        
        return unique_docs

# ============================================================================
# Enhanced Context Window Expansion
# ============================================================================

class ContextWindowExpander:
    """ì»¨í…ìŠ¤íŠ¸ ìœˆë„ìš° í™•ì¥ì„ ìœ„í•œ í´ë˜ìŠ¤"""
    
    def __init__(self, analyzer: HierarchicalDocumentAnalyzer):
        self.analyzer = analyzer
    
    def expand_context(self, docs: List[Document], nodes: Dict[str, DocumentNode], 
                      expansion_level: int = 1) -> List[Document]:
        """ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì˜ ì»¨í…ìŠ¤íŠ¸ë¥¼ í™•ì¥"""
        expanded_docs = []
        
        for doc in docs:
            expanded_docs.append(doc)
            
            # ë¬¸ì„œì˜ ë…¸ë“œ ID ì°¾ê¸°
            node_id = self._find_node_id_for_doc(doc, nodes)
            if not node_id:
                continue
            
            # ê´€ë ¨ ì„¹ì…˜ë“¤ ì°¾ê¸°
            related_sections = self.analyzer.get_related_sections(
                nodes, node_id, max_distance=expansion_level
            )
            
            # ê´€ë ¨ ì„¹ì…˜ë“¤ì˜ ë‚´ìš© ì¶”ê°€
            for related_id in related_sections:
                related_node = nodes[related_id]
                related_doc = Document(
                    page_content=related_node.content,
                    metadata={'type': 'expanded_context', 'original_node': node_id, 'related_node': related_id}
                )
                expanded_docs.append(related_doc)
        
        return expanded_docs
    
    def _find_node_id_for_doc(self, doc: Document, nodes: Dict[str, DocumentNode]) -> Optional[str]:
        """ë¬¸ì„œì— í•´ë‹¹í•˜ëŠ” ë…¸ë“œ ID ì°¾ê¸°"""
        doc_content = doc.page_content[:100]  # ì²« 100ìë§Œ ë¹„êµ
        
        for node_id, node in nodes.items():
            if doc_content in node.content[:200]:
                return node_id
        
        return None

# ============================================================================
# Enhanced Prompt Templates
# ============================================================================

COMPREHENSIVE_OVERVIEW_PROMPT = PromptTemplate.from_template(
    """ë‹¹ì‹ ì€ ê¸´ ê¸°ìˆ  ë¬¸ì„œì˜ ì „ì²´ì ì¸ ë§¥ë½ì„ íŒŒì•…í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë¶„ì„í•˜ì—¬ ë‹¤ìŒì„ ìˆ˜í–‰í•˜ì„¸ìš”:

1. **ì „ì²´ êµ¬ì¡° íŒŒì•…**: ë¬¸ì„œì˜ ì£¼ìš” ì„¹ì…˜ë“¤ê³¼ ê·¸ë“¤ì˜ ê´€ê³„ë¥¼ íŒŒì•…
2. **í•µì‹¬ ê°œë… ì •ë¦¬**: ì£¼ìš” ê°œë…ë“¤(PNS_1, PNS_2 ë“±)ì„ ì‹ë³„í•˜ê³  ë¶„ë¥˜
3. **ê¸°ëŠ¥ ìš”ì•½**: ìš”ì²­ëœ ê¸°ëŠ¥ì˜ ì „ë°˜ì ì¸ ê°œìš” ì œê³µ
4. **ê´€ë ¨ ì„¹ì…˜ ì•ˆë‚´**: ë” ìì„¸í•œ ì •ë³´ê°€ í•„ìš”í•œ ì„¹ì…˜ë“¤ ì œì‹œ

ì§ˆë¬¸: {question}

ì»¨í…ìŠ¤íŠ¸:
{context}

**ë¶„ì„ ê²°ê³¼:**

### ğŸ“‹ ì „ì²´ êµ¬ì¡°
(ë¬¸ì„œì˜ ì£¼ìš” ì„¹ì…˜ë“¤ê³¼ ê³„ì¸µ êµ¬ì¡°)

### ğŸ¯ í•µì‹¬ ê°œë…
(ì‹ë³„ëœ ì£¼ìš” ê°œë…ë“¤ê³¼ ê·¸ë“¤ì˜ ì—­í• )

### ğŸ“– ê¸°ëŠ¥ ìš”ì•½
(ìš”ì²­ëœ ê¸°ëŠ¥ì˜ ì „ë°˜ì ì¸ ì„¤ëª…)

### ğŸ” ì¶”ê°€ ì •ë³´
(ë” ìì„¸í•œ ì •ë³´ê°€ í•„ìš”í•œ ì„¹ì…˜ë“¤)

ë‹µë³€:"""
)

DETAILED_ANALYSIS_PROMPT = PromptTemplate.from_template(
    """ë‹¹ì‹ ì€ ê¸°ìˆ  ë¬¸ì„œì˜ ì„¸ë¶€ì‚¬í•­ì„ ì •í™•íˆ ë¶„ì„í•˜ëŠ” ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ì—ì„œ ë‹¤ìŒì„ ìˆ˜í–‰í•˜ì„¸ìš”:

1. **ì •í™•í•œ ê°’ ì¶”ì¶œ**: í•„ë“œê°’, ì½”ë“œ, ë§¤ê°œë³€ìˆ˜ ë“±ì„ ì •í™•íˆ ë‚˜ì—´
2. **êµ¬ì²´ì  ì˜ˆì‹œ ì œê³µ**: ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œë‚˜ ì½”ë“œ ìƒ˜í”Œ ì œì‹œ
3. **ê´€ë ¨ ì •ë³´ ì—°ê²°**: ë‹¤ë¥¸ ì„¹ì…˜ê³¼ì˜ ì—°ê´€ì„± ì„¤ëª…
4. **ì‹¤ìš©ì  ê°€ì´ë“œ**: ì‹¤ì œ êµ¬í˜„ ì‹œ ì£¼ì˜ì‚¬í•­ì´ë‚˜ íŒ ì œê³µ

ì§ˆë¬¸: {question}

ì»¨í…ìŠ¤íŠ¸:
{context}

**ìƒì„¸ ë¶„ì„:**

### ğŸ“Š ì •í™•í•œ ê°’ë“¤
(í•„ë“œê°’, ì½”ë“œ, ìƒìˆ˜ ë“±)

### ğŸ’¡ êµ¬ì²´ì  ì˜ˆì‹œ
(ì‹¤ì œ ì‚¬ìš© ì˜ˆì‹œ)

### ğŸ”— ê´€ë ¨ ì •ë³´
(ë‹¤ë¥¸ ì„¹ì…˜ê³¼ì˜ ì—°ê´€ì„±)

### âš ï¸ ì£¼ì˜ì‚¬í•­
(êµ¬í˜„ ì‹œ ê³ ë ¤ì‚¬í•­)

ë‹µë³€:"""
)

# ============================================================================
# Main Enhanced RAG System
# ============================================================================

class EnhancedRAGSystem:
    """ê°œì„ ëœ RAG ì‹œìŠ¤í…œ - ê¸´ ë¬¸ì„œì™€ ë³µì¡í•œ ê°œë… ì²´ê³„ ì²˜ë¦¬"""
    
    def __init__(self, data_file: str, embed_model: str, llm_model: str):
        self.data_file = data_file
        self.embeddings = OllamaEmbeddings(model=embed_model)
        self.llm = ChatOllama(model=llm_model)
        self.parser = StrOutputParser()
        
        # ë¬¸ì„œ ë¡œë“œ ë° ë¶„ì„
        self.docs = self._load_and_process_documents()
        self.analyzer = HierarchicalDocumentAnalyzer()
        self.multi_stage_retriever = MultiStageRetriever(self.analyzer, self.embeddings, self.docs)
        self.context_expander = ContextWindowExpander(self.analyzer)
        
        print("ğŸš€ Enhanced RAG System ì´ˆê¸°í™” ì™„ë£Œ!")
    
    def _load_and_process_documents(self) -> List[Document]:
        """ë¬¸ì„œ ë¡œë“œ ë° ì „ì²˜ë¦¬"""
        with open(self.data_file, 'r', encoding='utf-8') as f:
            md_text = f.read()
        
        # ë§ˆí¬ë‹¤ìš´ í—¤ë” ê¸°ë°˜ ë¶„í• 
        if HAS_MD_SPLITTER:
            splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[
                ("#", "h1"), ("##","h2"), ("###","h3"), ("####","h4")
            ])
            splits = splitter.split_text(md_text)
            docs = []
            for s in splits:
                path = []
                for k in ['h1','h2','h3','h4']:
                    if s.metadata.get(k):
                        path.append(s.metadata[k])
                full_path = " > ".join(path) if path else ""
                content = (f"[{full_path}] " if full_path else "") + s.page_content
                docs.append(Document(page_content=content, metadata={'full_path': path, 'type': 'section'}))
        else:
            # ê¸°ë³¸ ë¶„í• 
            parts = re.split(r'(?m)^#{1,4}\s+', md_text)
            docs = []
            for p in parts:
                p = p.strip()
                if not p: continue
                first_line = p.splitlines()[0] if '\n' in p else p
                content = f"[{first_line}] " + p
                docs.append(Document(page_content=content, metadata={'full_path': [first_line], 'type': 'section'}))
        
        return docs
    
    def answer_comprehensive(self, query: str, include_overview: bool = True) -> str:
        """ì „ì²´ì ì¸ ë§¥ë½ì„ ê³ ë ¤í•œ ì¢…í•©ì  ë‹µë³€"""
        
        # 1. ë‹¤ë‹¨ê³„ ê²€ìƒ‰ìœ¼ë¡œ ê´€ë ¨ ë¬¸ì„œ ìˆ˜ì§‘
        relevant_docs = self.multi_stage_retriever.retrieve_comprehensive_context(query, top_k=20)
        
        # 2. ì»¨í…ìŠ¤íŠ¸ í™•ì¥
        expanded_docs = self.context_expander.expand_context(
            relevant_docs, self.multi_stage_retriever.nodes, expansion_level=1
        )
        
        # 3. ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…
        context = self._format_context(expanded_docs)
        
        # 4. ì ì ˆí•œ í”„ë¡¬í”„íŠ¸ ì„ íƒ
        if include_overview or "ì „ë°˜ì " in query or "ê°œìš”" in query or "ìš”ì•½" in query:
            prompt = COMPREHENSIVE_OVERVIEW_PROMPT
        else:
            prompt = DETAILED_ANALYSIS_PROMPT
        
        # 5. LLM í˜¸ì¶œ
        chain = prompt | self.llm | self.parser
        response = chain.invoke({"question": query, "context": context})
        
        return response
    
    def _format_context(self, docs: List[Document]) -> str:
        """ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…"""
        parts = []
        for i, d in enumerate(docs, 1):
            md = d.metadata or {}
            doc_type = md.get('type', 'section')
            concept = md.get('concept', '')
            
            prefix = f"[{doc_type}"
            if concept:
                prefix += f":{concept}"
            prefix += "] "
            
            parts.append(f"({i}) {prefix}{d.page_content}")
        
        return "\n\n".join(parts)

# ============================================================================
# Test the Enhanced System
# ============================================================================

if __name__ == "__main__":
    print("ğŸ§ª Enhanced RAG System í…ŒìŠ¤íŠ¸")
    print("="*80)
    
    # ì‹œìŠ¤í…œ ì´ˆê¸°í™”
    enhanced_system = EnhancedRAGSystem(DATA_FILE, EMBED_MODEL, LLM_MODEL)
    
    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë“¤
    test_queries = [
        "PNSì˜ ì „ë°˜ì ì¸ ê¸°ëŠ¥ì„ ìš”ì•½í•´ì£¼ì„¸ìš”",
        "PNS_1, PNS_2 ë“±ì˜ ì„¸ë¶€ ê°œë…ë“¤ì´ ë¬´ì—‡ì¸ì§€ ì•Œë ¤ì£¼ì„¸ìš”",
        "ì›ìŠ¤í† ì–´ ì¸ì•±ê²°ì œì˜ ì „ì²´ì ì¸ êµ¬ì¡°ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”"
    ]
    
    for i, query in enumerate(test_queries, 1):
        print(f"\nğŸ” í…ŒìŠ¤íŠ¸ {i}: {query}")
        print("-" * 60)
        
        try:
            answer = enhanced_system.answer_comprehensive(query, include_overview=True)
            print(f"ë‹µë³€:\n{answer}")
        except Exception as e:
            print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        print("\n" + "="*80)
