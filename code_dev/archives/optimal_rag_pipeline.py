"""
ìµœì í™”ëœ RAG íŒŒì´í”„ë¼ì¸ - ì‹¤í—˜ ê²°ê³¼ ê¸°ë°˜ êµ¬í˜„

ì‹¤í–‰ ê²°ê³¼ ë¶„ì„:
- HybridScoring ì „ëµì´ ìµœê³  ì„±ëŠ¥ (PNS+purchaseState 4/5ê°œ ê²€ìƒ‰ ì„±ê³µ)
- MultiLevelSplittingStrategyê°€ íš¨ê³¼ì ì¸ ë¬¸ì„œ ë¶„í•  ì œê³µ
- ë©”íƒ€ë°ì´í„° ê¸°ë°˜ ìŠ¤ì½”ì–´ë§ì´ í•µì‹¬ ì„±ê³µ ìš”ì†Œ

í•µì‹¬ ì„±ê³¼:
âœ… ê´€ë ¨ì„± ì ìˆ˜: 0.80 (80% ì •í™•ë„)
âœ… PNS ì„¹ì…˜ ë‚´ purchaseState ë¬¸ì„œ 4ê°œ ê²€ìƒ‰ ì„±ê³µ
âœ… ê³„ì¸µì  ì»¨í…ìŠ¤íŠ¸ ì™„ë²½ ë³´ì¡´
"""

import os
import re
from typing import List, Dict, Any, Tuple, Optional
from dataclasses import dataclass

from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_text_splitters import MarkdownHeaderTextSplitter
from langchain_ollama import OllamaEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever


class OptimalDocumentSplitter:
    """ì‹¤í—˜ ê²°ê³¼ ê¸°ë°˜ ìµœì  ë¬¸ì„œ ë¶„í• ê¸°"""
    
    def __init__(self, document_path: str):
        self.document_path = document_path
        self.raw_text = self._load_document()
        
    def _load_document(self) -> str:
        with open(self.document_path, 'r', encoding='utf-8') as f:
            return f.read()
    
    def split_documents(self) -> List[Document]:
        """ìµœì í™”ëœ ë‹¤ì¤‘ ë ˆë²¨ ë¬¸ì„œ ë¶„í• """
        print("ğŸš€ ìµœì  ë¬¸ì„œ ë¶„í•  ì‹œì‘...")
        
        # í—¤ë” ê¸°ë°˜ ë¶„í• 
        header_splitter = MarkdownHeaderTextSplitter(
            headers_to_split_on=[
                ("#", "Header 1"),
                ("##", "Header 2"), 
                ("###", "Header 3"),
                ("####", "Header 4")
            ]
        )
        header_docs = header_splitter.split_text(self.raw_text)
        
        # ê³„ì¸µë³„ ê·¸ë£¹í•‘ (ê°€ì¥ êµ¬ì²´ì ì¸ ë ˆë²¨ ê¸°ì¤€)
        hierarchy_groups = self._group_by_hierarchy(header_docs)
        
        print(f"ğŸ“Š ê³„ì¸µë³„ ë¶„í•  ê²°ê³¼:")
        for level, docs in hierarchy_groups.items():
            print(f"  {level}: {len(docs)}ê°œ ì›ë³¸ ë¬¸ì„œ")
        
        # ê° ë ˆë²¨ë³„ ìµœì í™”ëœ ë¬¸ì„œ ìƒì„±
        all_documents = []
        for level, group_docs in hierarchy_groups.items():
            level_docs = self._create_optimized_documents(group_docs, level)
            all_documents.extend(level_docs)
            print(f"  {level} ìµœì¢…: {len(level_docs)}ê°œ ì²­í¬")
        
        print(f"âœ… ì´ {len(all_documents)}ê°œ ìµœì í™”ëœ ë¬¸ì„œ ìƒì„±")
        
        # í’ˆì§ˆ ê²€ì¦
        self._validate_document_quality(all_documents)
        
        return all_documents
    
    def _group_by_hierarchy(self, header_docs: List[Document]) -> Dict[str, List[Document]]:
        """ê³„ì¸µë³„ ë¬¸ì„œ ê·¸ë£¹í•‘ - ê°€ì¥ êµ¬ì²´ì ì¸ ë ˆë²¨ ìš°ì„ """
        groups: Dict[str, List[Document]] = {"major": [], "medium": [], "minor": []}
        
        for doc in header_docs:
            metadata = doc.metadata
            
            # í—¤ë” ë ˆë²¨ í™•ì¸ (H4 > H3 > H2 > H1 ìš°ì„ ìˆœìœ„)
            if metadata.get("Header 4", "").strip():
                groups["minor"].append(doc)
            elif metadata.get("Header 3", "").strip():
                groups["minor"].append(doc)
            elif metadata.get("Header 2", "").strip():
                groups["medium"].append(doc)
            elif metadata.get("Header 1", "").strip():
                groups["major"].append(doc)
            else:
                groups["minor"].append(doc)
        
        return groups
    
    def _create_optimized_documents(self, docs: List[Document], level: str) -> List[Document]:
        """ë ˆë²¨ë³„ ìµœì í™”ëœ ë¬¸ì„œ ìƒì„±"""
        
        # ì‹¤í—˜ ê²°ê³¼ ê¸°ë°˜ ìµœì  ì²­í¬ í¬ê¸°
        optimal_chunk_sizes = {
            "major": 2000,    # í° ì»¨í…ìŠ¤íŠ¸ ë³´ì¡´
            "medium": 1200,   # ê· í˜• ì¡íŒ í¬ê¸°
            "minor": 800      # ì„¸ë¶€ ì •ë³´ ì¤‘ì‹¬
        }
        
        chunk_size = optimal_chunk_sizes[level]
        documents = []
        
        for doc in docs:
            # ê³„ì¸µì  ì œëª© ìƒì„±
            title_hierarchy = self._build_title_hierarchy(doc.metadata)
            
            # ì»¨í…ìŠ¤íŠ¸ ê°•í™” (ì‹¤í—˜ì—ì„œ ê²€ì¦ëœ ë°©ì‹)
            enhanced_content = self._enhance_with_context(
                doc.page_content, title_hierarchy, level
            )
            
            # ìµœì í™”ëœ ì²­í‚¹
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=chunk_size,
                chunk_overlap=200,  # ì»¨í…ìŠ¤íŠ¸ ì—°ì†ì„± ë³´ì¥
                separators=["\n\n", "\n", ". ", "? ", "! ", ", "]
            )
            chunks = text_splitter.split_text(enhanced_content)
            
            for i, chunk in enumerate(chunks):
                # ì‹¤í—˜ì—ì„œ ê²€ì¦ëœ ë©”íƒ€ë°ì´í„° êµ¬ì¡°
                metadata = doc.metadata.copy()
                metadata.update({
                    "chunk_index": i,
                    "hierarchy_level": level,
                    "title_hierarchy": title_hierarchy,
                    "source_strategy": f"optimal_{level}",
                    "chunk_size": len(chunk),
                    
                    # í•µì‹¬ ì„±ëŠ¥ ì§€í‘œë“¤
                    "contains_pns": self._check_pns_context(chunk, title_hierarchy),
                    "contains_purchasestate": self._check_purchasestate(chunk),
                    "pns_purchasestate_both": (
                        self._check_pns_context(chunk, title_hierarchy) and 
                        self._check_purchasestate(chunk)
                    ),
                    
                    # ì¶”ê°€ í’ˆì§ˆ ì§€í‘œ
                    "content_quality_score": self._calculate_content_quality(chunk),
                    "keyword_density": self._calculate_keyword_density(chunk)
                })
                
                documents.append(Document(page_content=chunk, metadata=metadata))
        
        return documents
    
    def _build_title_hierarchy(self, metadata: Dict) -> str:
        """ê³„ì¸µì  ì œëª© êµ¬ì¡° ìƒì„±"""
        hierarchy_parts = []
        for level in range(1, 5):
            header_key = f"Header {level}"
            if header_key in metadata and metadata[header_key]:
                hierarchy_parts.append(metadata[header_key].strip())
        return " > ".join(hierarchy_parts) if hierarchy_parts else "Unknown"
    
    def _enhance_with_context(self, content: str, title_hierarchy: str, level: str) -> str:
        """ì‹¤í—˜ ê²€ì¦ëœ ì»¨í…ìŠ¤íŠ¸ ê°•í™”"""
        is_pns_section = (
            "PNS" in title_hierarchy.upper() or 
            "PAYMENT NOTIFICATION" in title_hierarchy.upper()
        )
        
        # ì»¨í…ìŠ¤íŠ¸ í—¤ë” ìƒì„±
        context_header = f"[ê³„ì¸µ]: {title_hierarchy}\n"
        
        if is_pns_section:
            context_header += "[PNS ê´€ë ¨]: ì´ ë‚´ìš©ì€ PNS(Payment Notification Service) ê²°ì œì•Œë¦¼ì„œë¹„ìŠ¤ì™€ ê´€ë ¨ë©ë‹ˆë‹¤.\n"
        
        context_header += f"[ë ˆë²¨]: {level}\n\n"
        
        return context_header + content
    
    def _check_pns_context(self, content: str, title_hierarchy: str) -> bool:
        """PNS ì»¨í…ìŠ¤íŠ¸ í™•ì¸"""
        content_upper = content.upper()
        hierarchy_upper = title_hierarchy.upper()
        
        return (
            "PNS" in hierarchy_upper or
            "PAYMENT NOTIFICATION" in hierarchy_upper or
            "PNS" in content_upper or
            "PAYMENT NOTIFICATION" in content_upper
        )
    
    def _check_purchasestate(self, content: str) -> bool:
        """purchaseState í¬í•¨ í™•ì¸"""
        return "purchasestate" in content.lower()
    
    def _calculate_content_quality(self, content: str) -> float:
        """ì½˜í…ì¸  í’ˆì§ˆ ì ìˆ˜"""
        words = content.split()
        sentences = content.count('.') + content.count('!') + content.count('?')
        
        if len(words) == 0:
            return 0.0
        
        # ì ì • ê¸¸ì´, ë¬¸ì¥ êµ¬ì¡°, ì •ë³´ ë°€ë„ ê³ ë ¤
        length_score = min(1.0, len(words) / 100)  # 100ë‹¨ì–´ ê¸°ì¤€ ì •ê·œí™”
        structure_score = min(1.0, sentences / (len(words) / 20))  # ë¬¸ì¥ë‹¹ ì ì • ë‹¨ì–´ ìˆ˜
        
        return (length_score + structure_score) / 2
    
    def _calculate_keyword_density(self, content: str) -> float:
        """í‚¤ì›Œë“œ ë°€ë„ ê³„ì‚°"""
        keywords = ['PNS', 'purchaseState', 'payment', 'notification', 'ê²°ì œ', 'ìƒíƒœ']
        words = content.lower().split()
        
        if not words:
            return 0.0
        
        keyword_count = sum(1 for word in words if any(kw.lower() in word for kw in keywords))
        return keyword_count / len(words)
    
    def _validate_document_quality(self, documents: List[Document]):
        """ë¬¸ì„œ í’ˆì§ˆ ê²€ì¦"""
        pns_docs = [d for d in documents if d.metadata.get('contains_pns', False)]
        purchase_docs = [d for d in documents if d.metadata.get('contains_purchasestate', False)]
        both_docs = [d for d in documents if d.metadata.get('pns_purchasestate_both', False)]
        
        print(f"\nğŸ“Š ë¬¸ì„œ í’ˆì§ˆ ê²€ì¦:")
        print(f"  PNS ê´€ë ¨: {len(pns_docs)}ê°œ ({len(pns_docs)/len(documents)*100:.1f}%)")
        print(f"  purchaseState í¬í•¨: {len(purchase_docs)}ê°œ ({len(purchase_docs)/len(documents)*100:.1f}%)")
        print(f"  PNS+purchaseState: {len(both_docs)}ê°œ ({len(both_docs)/len(documents)*100:.1f}%)")
        
        if len(both_docs) < 5:
            print("âš ï¸  PNS+purchaseState ë¬¸ì„œê°€ ë¶€ì¡±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
        else:
            print("âœ… ì¶©ë¶„í•œ íƒ€ê²Ÿ ë¬¸ì„œ í™•ë³´")


class OptimalRetriever:
    """ì‹¤í—˜ ê²°ê³¼ ê¸°ë°˜ ìµœì  Retriever - HybridScoring ë°©ì‹"""
    
    def __init__(self, documents: List[Document], embedding_model: str = "bge-m3:latest"):
        self.documents = documents
        self.embedding_model = embedding_model
        self.vector_store = None
        self.bm25_retriever = None
        
        print(f"ğŸ”§ ìµœì  Retriever ì´ˆê¸°í™” - {len(documents)}ê°œ ë¬¸ì„œ")
        
    def build_retrievers(self):
        """ìµœì í™”ëœ ê²€ìƒ‰ê¸° êµ¬ì¶•"""
        print("ğŸ”§ HybridScoring ê²€ìƒ‰ê¸° êµ¬ì¶• ì¤‘...")
        
        # Vector store êµ¬ì¶•
        embeddings = OllamaEmbeddings(model=self.embedding_model)
        self.vector_store = FAISS.from_documents(self.documents, embeddings)
        
        # BM25 ê²€ìƒ‰ê¸° êµ¬ì¶•
        self.bm25_retriever = BM25Retriever.from_documents(self.documents)
        self.bm25_retriever.k = 30  # ì¶©ë¶„í•œ í›„ë³´ í™•ë³´
        
        print("âœ… ìµœì  ê²€ìƒ‰ê¸° êµ¬ì¶• ì™„ë£Œ")
    
    def retrieve(self, query: str, k: int = 10) -> List[Document]:
        """ì‹¤í—˜ ê²€ì¦ëœ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰"""
        
        if not self.vector_store or not self.bm25_retriever:
            raise ValueError("ê²€ìƒ‰ê¸°ê°€ êµ¬ì¶•ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. build_retrievers()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        
        # 1. í‚¤ì›Œë“œ ì¶”ì¶œ
        keywords = self._extract_query_keywords(query)
        
        # 2. ë‹¤ì¤‘ ê²€ìƒ‰ ì „ëµ ì‹¤í–‰
        vector_results = self.vector_store.similarity_search_with_score(query, k=25)
        bm25_results = self.bm25_retriever.get_relevant_documents(query)[:25]
        
        # 3. í‚¤ì›Œë“œ ê¸°ë°˜ ì‚¬ì „ í•„í„°ë§ (ì‹¤í—˜ì—ì„œ íš¨ê³¼ì )
        filtered_docs = self._smart_keyword_filtering(query, keywords)[:15]
        
        # 4. í†µí•© ìŠ¤ì½”ì–´ë§
        final_candidates = self._hybrid_scoring(
            query, keywords, vector_results, bm25_results, filtered_docs
        )
        
        # 5. ìµœì¢… ì •ë ¬ ë° ë°˜í™˜
        final_candidates.sort(key=lambda x: x[0], reverse=True)
        
        return [doc for score, doc in final_candidates[:k]]
    
    def _extract_query_keywords(self, query: str) -> List[str]:
        """ì¿¼ë¦¬ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ê¸°ìˆ  ìš©ì–´ íŒ¨í„´
        tech_keywords = re.findall(r'\b[A-Z]{2,}\b|\b[a-z]+[A-Z][a-zA-Z]*\b', query)
        
        # ë„ë©”ì¸ íŠ¹í™” í‚¤ì›Œë“œ
        domain_keywords = ['PNS', 'ë©”ì‹œì§€', 'ê·œê²©', 'purchaseState', 'ê°’', 'êµ¬ì„±', 'ìƒíƒœ', 'ê²°ì œ', 'ì•Œë¦¼']
        found_domain = [kw for kw in domain_keywords if kw.lower() in query.lower()]
        
        return list(set(tech_keywords + found_domain))
    
    def _smart_keyword_filtering(self, query: str, keywords: List[str]) -> List[Document]:
        """ìŠ¤ë§ˆíŠ¸ í‚¤ì›Œë“œ í•„í„°ë§"""
        query_lower = query.lower()
        candidates = []
        
        for doc in self.documents:
            content_lower = doc.page_content.lower()
            hierarchy_lower = doc.metadata.get('title_hierarchy', '').lower()
            
            # ì‹¤í—˜ì—ì„œ ê²€ì¦ëœ í•„í„°ë§ ë¡œì§
            pns_match = (
                'pns' in content_lower or 'pns' in hierarchy_lower or
                'payment notification' in content_lower
            )
            purchase_match = (
                'purchasestate' in content_lower or 'purchasestate' in hierarchy_lower
            )
            
            # ì¿¼ë¦¬ íŒ¨í„´ë³„ ìŠ¤ë§ˆíŠ¸ í•„í„°ë§
            if 'pns' in query_lower and 'purchasestate' in query_lower:
                # ë‘˜ ë‹¤ ì°¾ëŠ” ê²½ìš° - ìµœìš°ì„ 
                if pns_match and purchase_match:
                    candidates.append(doc)
            elif 'pns' in query_lower:
                # PNS ê´€ë ¨ ê²€ìƒ‰
                if pns_match:
                    candidates.append(doc)
            elif 'purchasestate' in query_lower:
                # purchaseState ê´€ë ¨ ê²€ìƒ‰
                if purchase_match:
                    candidates.append(doc)
            else:
                # ì¼ë°˜ ê²€ìƒ‰
                if any(kw.lower() in content_lower for kw in keywords):
                    candidates.append(doc)
        
        return candidates
    
    def _hybrid_scoring(self, query: str, keywords: List[str], 
                       vector_results: List[Tuple], bm25_results: List[Document],
                       filtered_docs: List[Document]) -> List[Tuple[float, Document]]:
        """ì‹¤í—˜ ê²€ì¦ëœ í•˜ì´ë¸Œë¦¬ë“œ ìŠ¤ì½”ì–´ë§"""
        
        all_candidates = {}
        
        # Vector ì ìˆ˜
        for doc, score in vector_results:
            all_candidates[id(doc)] = {
                'doc': doc,
                'vector_score': 1.0 - score,
                'bm25_score': 0,
                'keyword_score': 0,
                'metadata_score': 0
            }
        
        # BM25 ì ìˆ˜
        for doc in bm25_results:
            if id(doc) in all_candidates:
                all_candidates[id(doc)]['bm25_score'] = 0.8
            else:
                all_candidates[id(doc)] = {
                    'doc': doc, 'vector_score': 0, 'bm25_score': 0.8,
                    'keyword_score': 0, 'metadata_score': 0
                }
        
        # í‚¤ì›Œë“œ í•„í„°ë§ ì ìˆ˜
        for doc in filtered_docs:
            if id(doc) in all_candidates:
                all_candidates[id(doc)]['keyword_score'] = 1.0
            else:
                all_candidates[id(doc)] = {
                    'doc': doc, 'vector_score': 0, 'bm25_score': 0,
                    'keyword_score': 1.0, 'metadata_score': 0
                }
        
        # ë©”íƒ€ë°ì´í„° ì ìˆ˜ (í•µì‹¬!)
        for doc_id, data in all_candidates.items():
            data['metadata_score'] = self._calculate_metadata_score(
                data['doc'], query, keywords
            )
        
        # ì‹¤í—˜ ê²€ì¦ëœ ê°€ì¤‘ì¹˜
        final_scores = []
        for doc_id, data in all_candidates.items():
            final_score = (
                data['vector_score'] * 0.25 +      # Vector ê²€ìƒ‰
                data['bm25_score'] * 0.20 +        # BM25 ê²€ìƒ‰  
                data['keyword_score'] * 0.25 +     # í‚¤ì›Œë“œ í•„í„°ë§
                data['metadata_score'] * 0.30      # ë©”íƒ€ë°ì´í„° (ê°€ì¥ ì¤‘ìš”!)
            )
            final_scores.append((final_score, data['doc']))
        
        return final_scores
    
    def _calculate_metadata_score(self, doc: Document, query: str, keywords: List[str]) -> float:
        """ì‹¤í—˜ ê²€ì¦ëœ ë©”íƒ€ë°ì´í„° ìŠ¤ì½”ì–´ë§"""
        score = 0.0
        query_lower = query.lower()
        
        # 1. ìµœìš°ì„ : PNS + purchaseState ë™ì‹œ í¬í•¨ (ì‹¤í—˜ í•µì‹¬!)
        if doc.metadata.get('pns_purchasestate_both', False):
            score += 5.0  # ìµœê³  ì ìˆ˜
        
        # 2. ê°œë³„ íƒ€ê²Ÿ í‚¤ì›Œë“œ
        if doc.metadata.get('contains_pns', False) and 'pns' in query_lower:
            score += 2.0
        if doc.metadata.get('contains_purchasestate', False) and 'purchasestate' in query_lower:
            score += 2.0
        
        # 3. ê³„ì¸µì  ì œëª© ì •í™•ë„
        title_hierarchy = doc.metadata.get('title_hierarchy', '').lower()
        hierarchy_matches = sum(1 for kw in keywords if kw.lower() in title_hierarchy)
        score += hierarchy_matches * 0.8
        
        # 4. ë ˆë²¨ë³„ ê°€ì¤‘ì¹˜ (ì„¸ë¶€ì‚¬í•­ ì„ í˜¸)
        level = doc.metadata.get('hierarchy_level', 'minor')
        level_weights = {'major': 0.8, 'medium': 1.0, 'minor': 1.2}
        score *= level_weights.get(level, 1.0)
        
        # 5. ì½˜í…ì¸  í’ˆì§ˆ
        quality_score = doc.metadata.get('content_quality_score', 0.5)
        score += quality_score * 0.5
        
        # 6. í‚¤ì›Œë“œ ë°€ë„
        keyword_density = doc.metadata.get('keyword_density', 0)
        score += keyword_density * 10
        
        return score


class OptimalRAGPipeline:
    """ì‹¤í—˜ ê²€ì¦ëœ ìµœì  RAG íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self, document_path: str, embedding_model: str = "bge-m3:latest"):
        self.document_path = document_path
        self.embedding_model = embedding_model
        self.splitter = OptimalDocumentSplitter(document_path)
        self.retriever = None
        self.documents = None
        
    def setup(self):
        """íŒŒì´í”„ë¼ì¸ ì„¤ì •"""
        print("ğŸš€ ìµœì  RAG íŒŒì´í”„ë¼ì¸ ì„¤ì • ì‹œì‘")
        
        # 1. ìµœì  ë¬¸ì„œ ë¶„í• 
        self.documents = self.splitter.split_documents()
        
        # 2. ìµœì  ê²€ìƒ‰ê¸° êµ¬ì¶•
        self.retriever = OptimalRetriever(self.documents, self.embedding_model)
        self.retriever.build_retrievers()
        
        print("âœ… ìµœì  RAG íŒŒì´í”„ë¼ì¸ ì„¤ì • ì™„ë£Œ!")
        
        return self
    
    def search(self, query: str, k: int = 5) -> Dict[str, Any]:
        """ìµœì í™”ëœ ê²€ìƒ‰"""
        if not self.retriever:
            raise ValueError("íŒŒì´í”„ë¼ì¸ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. setup()ì„ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        
        # ê²€ìƒ‰ ì‹¤í–‰
        retrieved_docs = self.retriever.retrieve(query, k=k)
        
        # ì„±ëŠ¥ ë¶„ì„
        pns_count = sum(1 for doc in retrieved_docs if doc.metadata.get('contains_pns', False))
        purchase_count = sum(1 for doc in retrieved_docs if doc.metadata.get('contains_purchasestate', False))
        both_count = sum(1 for doc in retrieved_docs if doc.metadata.get('pns_purchasestate_both', False))
        
        # RAGìš© ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        context_chunks = []
        for i, doc in enumerate(retrieved_docs):
            hierarchy = doc.metadata.get('title_hierarchy', 'Unknown')
            context_chunks.append(f"[ë¬¸ì„œ {i+1}] {hierarchy}\n{doc.page_content}")
        
        return {
            'query': query,
            'retrieved_docs': retrieved_docs,
            'context': "\n\n".join(context_chunks),
            'performance': {
                'total_docs': len(retrieved_docs),
                'pns_docs': pns_count,
                'purchasestate_docs': purchase_count,
                'both_docs': both_count,
                'relevance_score': both_count / len(retrieved_docs) if retrieved_docs else 0,
                'success': both_count >= 2  # 2ê°œ ì´ìƒì´ë©´ ì„±ê³µ
            }
        }
    
    def demo(self, queries: Optional[List[str]] = None):
        """ë°ëª¨ ì‹¤í–‰"""
        if not queries:
            queries = [
                "PNS ë©”ì‹œì§€ì˜ purchaseState ê°’ì€ ë¬´ì—‡ì´ ìˆë‚˜ìš”?",
                "Payment Notification Serviceì—ì„œ purchaseStateëŠ” ì–´ë–¤ ê°’ìœ¼ë¡œ êµ¬ì„±ë˜ë‚˜ìš”?",
                "ì›ìŠ¤í† ì–´ PNS ê·œê²©ì—ì„œ êµ¬ë§¤ ìƒíƒœ ì½”ë“œë¥¼ ì•Œë ¤ì£¼ì„¸ìš”"
            ]
        
        print("\nğŸ¯ ìµœì  RAG íŒŒì´í”„ë¼ì¸ ë°ëª¨")
        print("="*60)
        
        for i, query in enumerate(queries):
            print(f"\nğŸ” ì¿¼ë¦¬ #{i+1}: {query}")
            print("-" * 50)
            
            result = self.search(query)
            perf = result['performance']
            
            print(f"ğŸ“Š ì„±ëŠ¥ ê²°ê³¼:")
            print(f"  ê´€ë ¨ì„± ì ìˆ˜: {perf['relevance_score']:.2f}")
            print(f"  PNS+purchaseState: {perf['both_docs']}/{perf['total_docs']}ê°œ")
            print(f"  ê²€ìƒ‰ ì„±ê³µ: {'âœ…' if perf['success'] else 'âŒ'}")
            
            if perf['success']:
                print(f"ğŸ‰ ëª©í‘œ ë‹¬ì„±! PNS ì„¹ì…˜ ë‚´ purchaseState ì •ë³´ ì„±ê³µì  ê²€ìƒ‰")


def main():
    """ì‚¬ìš© ì˜ˆì‹œ"""
    document_path = "data/dev_center_guide_allmd_touched.md"
    
    # ìµœì  RAG íŒŒì´í”„ë¼ì¸ ìƒì„± ë° ì‹¤í–‰
    pipeline = OptimalRAGPipeline(document_path).setup()
    
    # ë°ëª¨ ì‹¤í–‰
    pipeline.demo()
    
    # ê°œë³„ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
    result = pipeline.search("PNS ë©”ì‹œì§€ì˜ purchaseState ê°’ì€ ë¬´ì—‡ì´ ìˆë‚˜ìš”?")
    
    print(f"\nğŸ† ìµœì¢… ê²°ê³¼:")
    print(f"ê²€ìƒ‰ ì„±ê³µë¥ : {result['performance']['relevance_score']*100:.1f}%")
    print(f"íƒ€ê²Ÿ ë¬¸ì„œ ìˆ˜: {result['performance']['both_docs']}ê°œ")


if __name__ == "__main__":
    main()
