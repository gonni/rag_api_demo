{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GitBook MD íŒŒì¼ ìŠ¤ë§ˆíŠ¸ ë¶„í• ê¸°\n",
        "\n",
        "GitBookì˜ íŠ¹ì„±ì„ ê³ ë ¤í•œ íš¨ê³¼ì ì¸ ë§ˆí¬ë‹¤ìš´ ë¶„í•  ì „ëµ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from typing import List, Dict, Any, Tuple\n",
        "from pathlib import Path\n",
        "from langchain.docstore.document import Document\n",
        "from langchain_text_splitters import (\n",
        "    MarkdownHeaderTextSplitter,\n",
        "    RecursiveCharacterTextSplitter,\n",
        "    Language\n",
        ")\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GitBookSmartSplitter:\n",
        "    \"\"\"GitBook íŠ¹í™” ìŠ¤ë§ˆíŠ¸ ë¶„í• ê¸°\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.header_splitter = MarkdownHeaderTextSplitter(\n",
        "            headers_to_split_on=[\n",
        "                (\"#\", \"title\"),\n",
        "                (\"##\", \"section\"),\n",
        "                (\"###\", \"subsection\"),\n",
        "                (\"####\", \"subsubsection\"),\n",
        "                (\"#####\", \"subsubsubsection\")\n",
        "            ]\n",
        "        )\n",
        "        \n",
        "        self.recursive_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=200,\n",
        "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
        "            length_function=len,\n",
        "            is_separator_regex=False\n",
        "        )\n",
        "    \n",
        "    def preprocess_gitbook_content(self, content: str) -> str:\n",
        "        \"\"\"GitBook ì½˜í…ì¸  ì „ì²˜ë¦¬\"\"\"\n",
        "        # GitBook íŠ¹ìˆ˜ íƒœê·¸ ì œê±°\n",
        "        content = re.sub(r'{%[^%]*%}', '', content)  # Liquid íƒœê·¸\n",
        "        content = re.sub(r'\\{\\{[^}]*\\}\\}', '', content)  # Handlebars\n",
        "        content = re.sub(r'<[^>]*>', '', content)  # HTML íƒœê·¸\n",
        "        \n",
        "        # GitBook ë§í¬ ì •ë¦¬\n",
        "        content = re.sub(r'\\[([^\\]]+)\\]\\([^)]*\\)', r'\\1', content)  # ë§í¬ í…ìŠ¤íŠ¸ë§Œ ìœ ì§€\n",
        "        \n",
        "        # ë¶ˆí•„ìš”í•œ ê³µë°± ì •ë¦¬\n",
        "        content = re.sub(r'\\n\\s*\\n\\s*\\n', '\\n\\n', content)\n",
        "        \n",
        "        return content.strip()\n",
        "    \n",
        "    def extract_gitbook_metadata(self, content: str) -> Dict[str, Any]:\n",
        "        \"\"\"GitBook ë©”íƒ€ë°ì´í„° ì¶”ì¶œ\"\"\"\n",
        "        metadata = {\n",
        "            'has_code_blocks': False,\n",
        "            'has_tables': False,\n",
        "            'has_images': False,\n",
        "            'has_links': False,\n",
        "            'header_count': 0,\n",
        "            'code_languages': [],\n",
        "            'estimated_complexity': 'low'\n",
        "        }\n",
        "        \n",
        "        # ì½”ë“œ ë¸”ë¡ í™•ì¸\n",
        "        code_blocks = re.findall(r'```(\\w+)?\\n([\\s\\S]*?)```', content)\n",
        "        if code_blocks:\n",
        "            metadata['has_code_blocks'] = True\n",
        "            metadata['code_languages'] = [lang for lang, _ in code_blocks if lang]\n",
        "        \n",
        "        # í…Œì´ë¸” í™•ì¸\n",
        "        if '|' in content and '---' in content:\n",
        "            metadata['has_tables'] = True\n",
        "        \n",
        "        # ì´ë¯¸ì§€ í™•ì¸\n",
        "        if '![' in content:\n",
        "            metadata['has_images'] = True\n",
        "        \n",
        "        # ë§í¬ í™•ì¸\n",
        "        if '[' in content and '](' in content:\n",
        "            metadata['has_links'] = True\n",
        "        \n",
        "        # í—¤ë” ê°œìˆ˜\n",
        "        metadata['header_count'] = len(re.findall(r'^#{1,6}\\s+', content, re.MULTILINE))\n",
        "        \n",
        "        # ë³µì¡ë„ ì¶”ì •\n",
        "        if metadata['has_code_blocks'] and metadata['has_tables']:\n",
        "            metadata['estimated_complexity'] = 'high'\n",
        "        elif metadata['has_code_blocks'] or metadata['has_tables']:\n",
        "            metadata['estimated_complexity'] = 'medium'\n",
        "        \n",
        "        return metadata\n",
        "    \n",
        "    def split_by_gitbook_structure(self, content: str, source: str = \"\") -> List[Document]:\n",
        "        \"\"\"GitBook êµ¬ì¡°ì— ë”°ë¥¸ ë¶„í• \"\"\"\n",
        "        # 1ë‹¨ê³„: í—¤ë” ê¸°ë°˜ ë¶„í• \n",
        "        header_docs = self.header_splitter.split_text(content)\n",
        "        \n",
        "        documents = []\n",
        "        for i, doc in enumerate(header_docs):\n",
        "            # ë©”íƒ€ë°ì´í„° ì¶”ì¶œ\n",
        "            metadata = self.extract_gitbook_metadata(doc.page_content)\n",
        "            \n",
        "            # GitBook íŠ¹ì„±ì— ë”°ë¥¸ ì¶”ê°€ ë©”íƒ€ë°ì´í„°\n",
        "            doc_metadata = {\n",
        "                **doc.metadata,\n",
        "                **metadata,\n",
        "                'source': source,\n",
        "                'split_method': 'header_based',\n",
        "                'chunk_idx': i,\n",
        "                'gitbook_processed': True\n",
        "            }\n",
        "            \n",
        "            # ìƒˆë¡œìš´ Document ìƒì„±\n",
        "            new_doc = Document(\n",
        "                page_content=doc.page_content,\n",
        "                metadata=doc_metadata\n",
        "            )\n",
        "            documents.append(new_doc)\n",
        "        \n",
        "        return documents\n",
        "    \n",
        "    def split_by_content_type(self, content: str, source: str = \"\") -> List[Document]:\n",
        "        \"\"\"ì½˜í…ì¸  íƒ€ì…ì— ë”°ë¥¸ ë¶„í• \"\"\"\n",
        "        documents = []\n",
        "        \n",
        "        # ì½”ë“œ ë¸”ë¡ ë¶„ë¦¬\n",
        "        code_pattern = r'```(\\w+)?\\n([\\s\\S]*?)```'\n",
        "        code_blocks = re.findall(code_pattern, content)\n",
        "        \n",
        "        # ì½”ë“œ ë¸”ë¡ì„ ì„ì‹œë¡œ ë§ˆìŠ¤í‚¹\n",
        "        masked_content = re.sub(code_pattern, '___CODE_BLOCK___', content)\n",
        "        \n",
        "        # ì¼ë°˜ í…ìŠ¤íŠ¸ ë¶„í• \n",
        "        text_chunks = self.recursive_splitter.split_text(masked_content)\n",
        "        \n",
        "        # ì½”ë“œ ë¸”ë¡ ë³µì› ë° Document ìƒì„±\n",
        "        code_idx = 0\n",
        "        for i, chunk in enumerate(text_chunks):\n",
        "            # ì½”ë“œ ë¸”ë¡ ë³µì›\n",
        "            restored_chunk = chunk\n",
        "            while '___CODE_BLOCK___' in restored_chunk and code_idx < len(code_blocks):\n",
        "                lang, code = code_blocks[code_idx]\n",
        "                code_block = f\"```{lang}\\n{code}\\n```\"\n",
        "                restored_chunk = restored_chunk.replace('___CODE_BLOCK___', code_block, 1)\n",
        "                code_idx += 1\n",
        "            \n",
        "            # ë©”íƒ€ë°ì´í„° ìƒì„±\n",
        "            metadata = self.extract_gitbook_metadata(restored_chunk)\n",
        "            \n",
        "            doc = Document(\n",
        "                page_content=restored_chunk,\n",
        "                metadata={\n",
        "                    **metadata,\n",
        "                    'source': source,\n",
        "                    'split_method': 'content_type_based',\n",
        "                    'chunk_idx': i,\n",
        "                    'gitbook_processed': True\n",
        "                }\n",
        "            )\n",
        "            documents.append(doc)\n",
        "        \n",
        "        return documents\n",
        "    \n",
        "    def split_by_semantic_units(self, content: str, source: str = \"\") -> List[Document]:\n",
        "        \"\"\"ì˜ë¯¸ì  ë‹¨ìœ„ë¡œ ë¶„í• \"\"\"\n",
        "        documents = []\n",
        "        \n",
        "        # ì„¹ì…˜ë³„ë¡œ ë¶„í• \n",
        "        sections = re.split(r'(?=^#{1,6}\\s+)', content, flags=re.MULTILINE)\n",
        "        \n",
        "        for i, section in enumerate(sections):\n",
        "            if not section.strip():\n",
        "                continue\n",
        "            \n",
        "            # ì„¹ì…˜ ë‚´ì—ì„œ ë” ì„¸ë¶„í™”\n",
        "            if len(section) > 2000:  # í° ì„¹ì…˜ì€ ë‹¤ì‹œ ë¶„í• \n",
        "                subsections = self.recursive_splitter.split_text(section)\n",
        "                for j, subsection in enumerate(subsections):\n",
        "                    metadata = self.extract_gitbook_metadata(subsection)\n",
        "                    \n",
        "                    doc = Document(\n",
        "                        page_content=subsection,\n",
        "                        metadata={\n",
        "                            **metadata,\n",
        "                            'source': source,\n",
        "                            'split_method': 'semantic_units',\n",
        "                            'section_idx': i,\n",
        "                            'subsection_idx': j,\n",
        "                            'gitbook_processed': True\n",
        "                        }\n",
        "                    )\n",
        "                    documents.append(doc)\n",
        "            else:\n",
        "                metadata = self.extract_gitbook_metadata(section)\n",
        "                \n",
        "                doc = Document(\n",
        "                    page_content=section,\n",
        "                    metadata={\n",
        "                        **metadata,\n",
        "                        'source': source,\n",
        "                        'split_method': 'semantic_units',\n",
        "                        'section_idx': i,\n",
        "                        'gitbook_processed': True\n",
        "                    }\n",
        "                )\n",
        "                documents.append(doc)\n",
        "        \n",
        "        return documents\n",
        "    \n",
        "    def smart_split(self, content: str, source: str = \"\", strategy: str = \"auto\") -> List[Document]:\n",
        "        \"\"\"ìŠ¤ë§ˆíŠ¸ ë¶„í•  ë©”ì¸ í•¨ìˆ˜\"\"\"\n",
        "        # ì „ì²˜ë¦¬\n",
        "        processed_content = self.preprocess_gitbook_content(content)\n",
        "        \n",
        "        # ë©”íƒ€ë°ì´í„° ë¶„ì„\n",
        "        metadata = self.extract_gitbook_metadata(processed_content)\n",
        "        \n",
        "        # ì „ëµ ì„ íƒ\n",
        "        if strategy == \"auto\":\n",
        "            if metadata['header_count'] > 5:\n",
        "                strategy = \"structure\"\n",
        "            elif metadata['has_code_blocks'] and metadata['estimated_complexity'] == 'high':\n",
        "                strategy = \"content_type\"\n",
        "            else:\n",
        "                strategy = \"semantic\"\n",
        "        \n",
        "        # ë¶„í•  ì‹¤í–‰\n",
        "        if strategy == \"structure\":\n",
        "            documents = self.split_by_gitbook_structure(processed_content, source)\n",
        "        elif strategy == \"content_type\":\n",
        "            documents = self.split_by_content_type(processed_content, source)\n",
        "        elif strategy == \"semantic\":\n",
        "            documents = self.split_by_semantic_units(processed_content, source)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown strategy: {strategy}\")\n",
        "        \n",
        "        # ê²°ê³¼ í†µê³„\n",
        "        print(f\"ğŸ“Š ë¶„í•  ê²°ê³¼:\")\n",
        "        print(f\"  - ì „ëµ: {strategy}\")\n",
        "        print(f\"  - ë¬¸ì„œ ìˆ˜: {len(documents)}\")\n",
        "        print(f\"  - í‰ê·  ê¸¸ì´: {sum(len(doc.page_content) for doc in documents) // len(documents)} ë¬¸ì\")\n",
        "        print(f\"  - ë³µì¡ë„: {metadata['estimated_complexity']}\")\n",
        "        \n",
        "        return documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GitBook ìŠ¤ë§ˆíŠ¸ ë¶„í• ê¸° í…ŒìŠ¤íŠ¸\n",
        "def load_test_gitbook_content() -> str:\n",
        "    \"\"\"í…ŒìŠ¤íŠ¸ìš© GitBook ì½˜í…ì¸  ë¡œë“œ\"\"\"\n",
        "    # ì‹¤ì œ GitBook ì½˜í…ì¸  ì˜ˆì‹œ\n",
        "    test_content = \"\"\"\n",
        "# ì›ìŠ¤í† ì–´ ì¸ì•±ê²°ì œ API V7(SDK V21)\n",
        "\n",
        "ì›ìŠ¤í† ì–´ì˜ ìµœì‹  ì¸ì•±ê²°ì œ API V7(SDK V21)ì´ ì¶œì‹œë˜ì—ˆìŠµë‹ˆë‹¤.\n",
        "\n",
        "## ê°œìš”\n",
        "\n",
        "ì´ ë¬¸ì„œëŠ” ì›ìŠ¤í† ì–´ ì¸ì•±ê²°ì œ API V7ì˜ ì‚¬ìš©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.\n",
        "\n",
        "### ì£¼ìš” ê¸°ëŠ¥\n",
        "\n",
        "- API V7 ì§€ì›\n",
        "- SDK V21 í†µí•©\n",
        "- í–¥ìƒëœ ë³´ì•ˆ\n",
        "\n",
        "## ì„¤ì¹˜ ë°©ë²•\n",
        "\n",
        "### Gradle ì„¤ì •\n",
        "\n",
        "```gradle\n",
        "dependencies {\n",
        "    implementation 'com.onestore:iap:21.0.0'\n",
        "}\n",
        "```\n",
        "\n",
        "### ì´ˆê¸°í™” ì½”ë“œ\n",
        "\n",
        "```kotlin\n",
        "val purchaseClient = PurchaseClient.newBuilder()\n",
        "    .setListener(purchasesUpdatedListener)\n",
        "    .build()\n",
        "```\n",
        "\n",
        "## êµ¬ë§¤ í”„ë¡œì„¸ìŠ¤\n",
        "\n",
        "### 1. ìƒí’ˆ ì¡°íšŒ\n",
        "\n",
        "```kotlin\n",
        "purchaseClient.queryProductDetailsAsync(\n",
        "    QueryProductDetailsParams.newBuilder()\n",
        "        .setProductIds(listOf(\"product_id\"))\n",
        "        .build()\n",
        ")\n",
        "```\n",
        "\n",
        "### 2. êµ¬ë§¤ ì‹¤í–‰\n",
        "\n",
        "```kotlin\n",
        "purchaseClient.launchPurchaseFlow(\n",
        "    activity,\n",
        "    PurchaseFlowParams.newBuilder()\n",
        "        .setProductId(\"product_id\")\n",
        "        .build()\n",
        ")\n",
        "```\n",
        "\n",
        "## í…Œì´ë¸” ì˜ˆì‹œ\n",
        "\n",
        "| ê¸°ëŠ¥ | ì„¤ëª… | ë²„ì „ |\n",
        "|------|------|------|\n",
        "| API V7 | ìµœì‹  API | 21.0.0 |\n",
        "| SDK V21 | í†µí•© SDK | 21.0.0 |\n",
        "\n",
        "## ì´ë¯¸ì§€ ì˜ˆì‹œ\n",
        "\n",
        "![ê²°ì œ í™”ë©´](payment_screen.png)\n",
        "\n",
        "## ë§í¬ ì˜ˆì‹œ\n",
        "\n",
        "[ê³µì‹ ë¬¸ì„œ](https://onestore-dev.gitbook.io)\n",
        "\n",
        "{% hint style=\"info\" %}\n",
        "ì´ê²ƒì€ GitBook íŒíŠ¸ì…ë‹ˆë‹¤.\n",
        "{% endhint %}\n",
        "\n",
        "{% hint style=\"warning\" %}\n",
        "ì£¼ì˜ì‚¬í•­ì…ë‹ˆë‹¤.\n",
        "{% endhint %}\n",
        "\"\"\"\n",
        "    return test_content\n",
        "\n",
        "# ìŠ¤ë§ˆíŠ¸ ë¶„í• ê¸° ì´ˆê¸°í™”\n",
        "splitter = GitBookSmartSplitter()\n",
        "\n",
        "# í…ŒìŠ¤íŠ¸ ì½˜í…ì¸  ë¡œë“œ\n",
        "test_content = load_test_gitbook_content()\n",
        "print(f\"ğŸ“„ í…ŒìŠ¤íŠ¸ ì½˜í…ì¸  í¬ê¸°: {len(test_content)} ë¬¸ì\")\n",
        "print(f\"ğŸ“„ í…ŒìŠ¤íŠ¸ ì½˜í…ì¸  ë¯¸ë¦¬ë³´ê¸°:\")\n",
        "print(\"=\" * 60)\n",
        "print(test_content[:500] + \"...\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ë‹¤ì–‘í•œ ì „ëµìœ¼ë¡œ ë¶„í•  í…ŒìŠ¤íŠ¸\n",
        "strategies = [\"auto\", \"structure\", \"content_type\", \"semantic\"]\n",
        "\n",
        "for strategy in strategies:\n",
        "    print(f\"\\nğŸ”§ ì „ëµ: {strategy}\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    documents = splitter.smart_split(test_content, source=\"test_gitbook.md\", strategy=strategy)\n",
        "    \n",
        "    # ê²°ê³¼ ë¶„ì„\n",
        "    for i, doc in enumerate(documents[:3], 1):  # ì²˜ìŒ 3ê°œë§Œ ì¶œë ¥\n",
        "        print(f\"\\nğŸ“„ ë¬¸ì„œ {i}:\")\n",
        "        print(f\"  ê¸¸ì´: {len(doc.page_content)} ë¬¸ì\")\n",
        "        print(f\"  ë¶„í•  ë°©ë²•: {doc.metadata.get('split_method', 'unknown')}\")\n",
        "        print(f\"  ì½”ë“œ ë¸”ë¡: {doc.metadata.get('has_code_blocks', False)}\")\n",
        "        print(f\"  ë³µì¡ë„: {doc.metadata.get('estimated_complexity', 'unknown')}\")\n",
        "        print(f\"  ë‚´ìš©: {doc.page_content[:100]}...\")\n",
        "    \n",
        "    if len(documents) > 3:\n",
        "        print(f\"\\n... ë° {len(documents) - 3}ê°œ ë”\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ì‹¤ì œ GitBook íŒŒì¼ ì²˜ë¦¬\n",
        "def process_gitbook_file(file_path: str, output_dir: str = \"data/gitbook_splits\"):\n",
        "    \"\"\"ì‹¤ì œ GitBook íŒŒì¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.\"\"\"\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"âŒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {file_path}\")\n",
        "        return\n",
        "    \n",
        "    # íŒŒì¼ ë¡œë“œ\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        content = f.read()\n",
        "    \n",
        "    print(f\"ğŸ“„ íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {len(content)} ë¬¸ì\")\n",
        "    \n",
        "    # ìŠ¤ë§ˆíŠ¸ ë¶„í• \n",
        "    documents = splitter.smart_split(content, source=file_path, strategy=\"auto\")\n",
        "    \n",
        "    # ê²°ê³¼ ì €ì¥\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    # JSON í˜•íƒœë¡œ ì €ì¥\n",
        "    docs_data = []\n",
        "    for i, doc in enumerate(documents):\n",
        "        doc_data = {\n",
        "            'id': i,\n",
        "            'content': doc.page_content,\n",
        "            'metadata': doc.metadata\n",
        "        }\n",
        "        docs_data.append(doc_data)\n",
        "    \n",
        "    output_file = os.path.join(output_dir, f\"{Path(file_path).stem}_split.json\")\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(docs_data, f, ensure_ascii=False, indent=2)\n",
        "    \n",
        "    print(f\"âœ… ë¶„í•  ê²°ê³¼ ì €ì¥ ì™„ë£Œ: {output_file}\")\n",
        "    \n",
        "    # í†µê³„ ì¶œë ¥\n",
        "    print(f\"\\nğŸ“Š ë¶„í•  í†µê³„:\")\n",
        "    print(f\"  - ì´ ë¬¸ì„œ ìˆ˜: {len(documents)}\")\n",
        "    print(f\"  - í‰ê·  ê¸¸ì´: {sum(len(doc.page_content) for doc in documents) // len(documents)} ë¬¸ì\")\n",
        "    print(f\"  - ìµœì†Œ ê¸¸ì´: {min(len(doc.page_content) for doc in documents)} ë¬¸ì\")\n",
        "    print(f\"  - ìµœëŒ€ ê¸¸ì´: {max(len(doc.page_content) for doc in documents)} ë¬¸ì\")\n",
        "    \n",
        "    # ë©”íƒ€ë°ì´í„° í†µê³„\n",
        "    code_blocks_count = sum(1 for doc in documents if doc.metadata.get('has_code_blocks', False))\n",
        "    tables_count = sum(1 for doc in documents if doc.metadata.get('has_tables', False))\n",
        "    \n",
        "    print(f\"  - ì½”ë“œ ë¸”ë¡ í¬í•¨: {code_blocks_count}ê°œ\")\n",
        "    print(f\"  - í…Œì´ë¸” í¬í•¨: {tables_count}ê°œ\")\n",
        "    \n",
        "    return documents\n",
        "\n",
        "# ì‹¤ì œ íŒŒì¼ ì²˜ë¦¬ (ì˜ˆì‹œ)\n",
        "test_files = [\n",
        "    \"data/iap_gitbook.md\",  # ì´ì „ì— ìƒì„±í•œ í†µí•© íŒŒì¼\n",
        "    \"data/dev_center_guide_touched.md\"  # ê¸°ì¡´ íŒŒì¼\n",
        "]\n",
        "\n",
        "for file_path in test_files:\n",
        "    if os.path.exists(file_path):\n",
        "        print(f\"\\nğŸ”§ ì²˜ë¦¬ ì¤‘: {file_path}\")\n",
        "        print(\"=\" * 60)\n",
        "        documents = process_gitbook_file(file_path)\n",
        "    else:\n",
        "        print(f\"\\nâš ï¸ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ë¶„í•  í’ˆì§ˆ í‰ê°€\n",
        "def evaluate_split_quality(documents: List[Document]) -> Dict[str, Any]:\n",
        "    \"\"\"ë¶„í•  í’ˆì§ˆì„ í‰ê°€í•©ë‹ˆë‹¤.\"\"\"\n",
        "    \n",
        "    # ê¸¸ì´ ë¶„í¬ ë¶„ì„\n",
        "    lengths = [len(doc.page_content) for doc in documents]\n",
        "    avg_length = sum(lengths) / len(lengths)\n",
        "    \n",
        "    # í‚¤ì›Œë“œ ë³´ì¡´ë¥  í™•ì¸\n",
        "    important_keywords = [\n",
        "        \"launchPurchaseFlow\", \"PurchaseClient\", \"IapResult\",\n",
        "        \"API\", \"SDK\", \"ê²°ì œ\", \"êµ¬ë§¤\"\n",
        "    ]\n",
        "    \n",
        "    keyword_preservation = {}\n",
        "    for keyword in important_keywords:\n",
        "        docs_with_keyword = [\n",
        "            doc for doc in documents \n",
        "            if keyword.lower() in doc.page_content.lower()\n",
        "        ]\n",
        "        keyword_preservation[keyword] = len(docs_with_keyword)\n",
        "    \n",
        "    # ë©”íƒ€ë°ì´í„° í’ˆì§ˆ\n",
        "    metadata_completeness = sum(\n",
        "        1 for doc in documents \n",
        "        if doc.metadata.get('gitbook_processed', False)\n",
        "    ) / len(documents)\n",
        "    \n",
        "    # ì¤‘ë³µ ì½˜í…ì¸  í™•ì¸\n",
        "    content_hashes = set()\n",
        "    duplicates = 0\n",
        "    for doc in documents:\n",
        "        content_hash = hash(doc.page_content[:100])  # ì²˜ìŒ 100ìë¡œ í•´ì‹œ\n",
        "        if content_hash in content_hashes:\n",
        "            duplicates += 1\n",
        "        content_hashes.add(content_hash)\n",
        "    \n",
        "    duplicate_rate = duplicates / len(documents) if documents else 0\n",
        "    \n",
        "    return {\n",
        "        'total_documents': len(documents),\n",
        "        'average_length': avg_length,\n",
        "        'length_variance': sum((l - avg_length) ** 2 for l in lengths) / len(lengths),\n",
        "        'keyword_preservation': keyword_preservation,\n",
        "        'metadata_completeness': metadata_completeness,\n",
        "        'duplicate_rate': duplicate_rate,\n",
        "        'quality_score': (1 - duplicate_rate) * metadata_completeness * 100\n",
        "    }\n",
        "\n",
        "# í’ˆì§ˆ í‰ê°€ ì‹¤í–‰\n",
        "if os.path.exists(\"data/iap_gitbook.md\"):\n",
        "    with open(\"data/iap_gitbook.md\", 'r', encoding='utf-8') as f:\n",
        "        content = f.read()\n",
        "    \n",
        "    documents = splitter.smart_split(content, source=\"iap_gitbook.md\", strategy=\"auto\")\n",
        "    \n",
        "    quality_metrics = evaluate_split_quality(documents)\n",
        "    \n",
        "    print(f\"ğŸ“Š ë¶„í•  í’ˆì§ˆ í‰ê°€ ê²°ê³¼:\")\n",
        "    print(f\"  - ì´ ë¬¸ì„œ ìˆ˜: {quality_metrics['total_documents']}\")\n",
        "    print(f\"  - í‰ê·  ê¸¸ì´: {quality_metrics['average_length']:.1f} ë¬¸ì\")\n",
        "    print(f\"  - ê¸¸ì´ ë¶„ì‚°: {quality_metrics['length_variance']:.1f}\")\n",
        "    print(f\"  - ë©”íƒ€ë°ì´í„° ì™„ì„±ë„: {quality_metrics['metadata_completeness']:.2f}\")\n",
        "    print(f\"  - ì¤‘ë³µë¥ : {quality_metrics['duplicate_rate']:.2f}\")\n",
        "    print(f\"  - í’ˆì§ˆ ì ìˆ˜: {quality_metrics['quality_score']:.1f}/100\")\n",
        "    \n",
        "    print(f\"\\nğŸ” í‚¤ì›Œë“œ ë³´ì¡´ë¥ :\")\n",
        "    for keyword, count in quality_metrics['keyword_preservation'].items():\n",
        "        print(f\"  - {keyword}: {count}ê°œ ë¬¸ì„œì—ì„œ ë°œê²¬\")\n",
        "else:\n",
        "    print(\"âŒ í‰ê°€í•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RAG ìµœì í™” ë¶„í• \n",
        "def create_rag_optimized_splits(content: str, source: str = \"\") -> List[Document]:\n",
        "    \"\"\"RAG ì‹œìŠ¤í…œì— ìµœì í™”ëœ ë¶„í• \"\"\"\n",
        "    \n",
        "    # 1ë‹¨ê³„: ê¸°ë³¸ ìŠ¤ë§ˆíŠ¸ ë¶„í• \n",
        "    base_docs = splitter.smart_split(content, source, strategy=\"auto\")\n",
        "    \n",
        "    # 2ë‹¨ê³„: RAG ìµœì í™”\n",
        "    optimized_docs = []\n",
        "    \n",
        "    for doc in base_docs:\n",
        "        # ë„ˆë¬´ ì‘ì€ ì²­í¬ëŠ” ê±´ë„ˆë›°ê¸°\n",
        "        if len(doc.page_content) < 100:\n",
        "            continue\n",
        "        \n",
        "        # ë„ˆë¬´ í° ì²­í¬ëŠ” ì¬ë¶„í• \n",
        "        if len(doc.page_content) > 2000:\n",
        "            sub_docs = splitter.recursive_splitter.split_text(doc.page_content)\n",
        "            for i, sub_content in enumerate(sub_docs):\n",
        "                if len(sub_content) >= 100:  # ìµœì†Œ ê¸¸ì´ í™•ì¸\n",
        "                    sub_metadata = {\n",
        "                        **doc.metadata,\n",
        "                        'split_method': 'rag_optimized',\n",
        "                        'sub_chunk_idx': i,\n",
        "                        'original_length': len(doc.page_content)\n",
        "                    }\n",
        "                    \n",
        "                    optimized_docs.append(Document(\n",
        "                        page_content=sub_content,\n",
        "                        metadata=sub_metadata\n",
        "                    ))\n",
        "        else:\n",
        "            # ì ì ˆí•œ í¬ê¸°ëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš©\n",
        "            doc.metadata['split_method'] = 'rag_optimized'\n",
        "            optimized_docs.append(doc)\n",
        "    \n",
        "    # 3ë‹¨ê³„: í‚¤ì›Œë“œ ê°•í™”\n",
        "    for doc in optimized_docs:\n",
        "        # ì¤‘ìš” í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì„œëŠ” ìš°ì„ ìˆœìœ„ ë¶€ì—¬\n",
        "        important_keywords = [\n",
        "            \"launchPurchaseFlow\", \"PurchaseClient\", \"IapResult\",\n",
        "            \"API\", \"SDK\", \"ê²°ì œ\", \"êµ¬ë§¤\"\n",
        "        ]\n",
        "        \n",
        "        keyword_count = sum(\n",
        "            1 for keyword in important_keywords\n",
        "            if keyword.lower() in doc.page_content.lower()\n",
        "        )\n",
        "        \n",
        "        doc.metadata['keyword_score'] = keyword_count\n",
        "        doc.metadata['is_important'] = keyword_count >= 2\n",
        "    \n",
        "    print(f\"âœ… RAG ìµœì í™” ì™„ë£Œ: {len(optimized_docs)}ê°œ ë¬¸ì„œ\")\n",
        "    print(f\"  - ì¤‘ìš” ë¬¸ì„œ: {sum(1 for doc in optimized_docs if doc.metadata.get('is_important', False))}ê°œ\")\n",
        "    \n",
        "    return optimized_docs\n",
        "\n",
        "# RAG ìµœì í™” ë¶„í•  í…ŒìŠ¤íŠ¸\n",
        "if os.path.exists(\"data/iap_gitbook.md\"):\n",
        "    with open(\"data/iap_gitbook.md\", 'r', encoding='utf-8') as f:\n",
        "        content = f.read()\n",
        "    \n",
        "    rag_docs = create_rag_optimized_splits(content, \"iap_gitbook.md\")\n",
        "    \n",
        "    # ê²°ê³¼ ì €ì¥\n",
        "    output_file = \"data/rag_optimized_splits.json\"\n",
        "    docs_data = [\n",
        "        {\n",
        "            'id': i,\n",
        "            'content': doc.page_content,\n",
        "            'metadata': doc.metadata\n",
        "        }\n",
        "        for i, doc in enumerate(rag_docs)\n",
        "    ]\n",
        "    \n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(docs_data, f, ensure_ascii=False, indent=2)\n",
        "    \n",
        "    print(f\"âœ… RAG ìµœì í™” ê²°ê³¼ ì €ì¥: {output_file}\")\n",
        "else:\n",
        "    print(\"âŒ RAG ìµœì í™”ë¥¼ ìœ„í•œ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}