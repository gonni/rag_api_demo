{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GitBook MD 파일 스마트 분할기\n",
        "\n",
        "GitBook의 특성을 고려한 효과적인 마크다운 분할 전략"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "from typing import List, Dict, Any, Tuple\n",
        "from pathlib import Path\n",
        "from langchain.docstore.document import Document\n",
        "from langchain_text_splitters import (\n",
        "    MarkdownHeaderTextSplitter,\n",
        "    RecursiveCharacterTextSplitter,\n",
        "    Language\n",
        ")\n",
        "import json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GitBookSmartSplitter:\n",
        "    \"\"\"GitBook 특화 스마트 분할기\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.header_splitter = MarkdownHeaderTextSplitter(\n",
        "            headers_to_split_on=[\n",
        "                (\"#\", \"title\"),\n",
        "                (\"##\", \"section\"),\n",
        "                (\"###\", \"subsection\"),\n",
        "                (\"####\", \"subsubsection\"),\n",
        "                (\"#####\", \"subsubsubsection\")\n",
        "            ]\n",
        "        )\n",
        "        \n",
        "        self.recursive_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=1000,\n",
        "            chunk_overlap=200,\n",
        "            separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"],\n",
        "            length_function=len,\n",
        "            is_separator_regex=False\n",
        "        )\n",
        "    \n",
        "    def preprocess_gitbook_content(self, content: str) -> str:\n",
        "        \"\"\"GitBook 콘텐츠 전처리\"\"\"\n",
        "        # GitBook 특수 태그 제거\n",
        "        content = re.sub(r'{%[^%]*%}', '', content)  # Liquid 태그\n",
        "        content = re.sub(r'\\{\\{[^}]*\\}\\}', '', content)  # Handlebars\n",
        "        content = re.sub(r'<[^>]*>', '', content)  # HTML 태그\n",
        "        \n",
        "        # GitBook 링크 정리\n",
        "        content = re.sub(r'\\[([^\\]]+)\\]\\([^)]*\\)', r'\\1', content)  # 링크 텍스트만 유지\n",
        "        \n",
        "        # 불필요한 공백 정리\n",
        "        content = re.sub(r'\\n\\s*\\n\\s*\\n', '\\n\\n', content)\n",
        "        \n",
        "        return content.strip()\n",
        "    \n",
        "    def extract_gitbook_metadata(self, content: str) -> Dict[str, Any]:\n",
        "        \"\"\"GitBook 메타데이터 추출\"\"\"\n",
        "        metadata = {\n",
        "            'has_code_blocks': False,\n",
        "            'has_tables': False,\n",
        "            'has_images': False,\n",
        "            'has_links': False,\n",
        "            'header_count': 0,\n",
        "            'code_languages': [],\n",
        "            'estimated_complexity': 'low'\n",
        "        }\n",
        "        \n",
        "        # 코드 블록 확인\n",
        "        code_blocks = re.findall(r'```(\\w+)?\\n([\\s\\S]*?)```', content)\n",
        "        if code_blocks:\n",
        "            metadata['has_code_blocks'] = True\n",
        "            metadata['code_languages'] = [lang for lang, _ in code_blocks if lang]\n",
        "        \n",
        "        # 테이블 확인\n",
        "        if '|' in content and '---' in content:\n",
        "            metadata['has_tables'] = True\n",
        "        \n",
        "        # 이미지 확인\n",
        "        if '![' in content:\n",
        "            metadata['has_images'] = True\n",
        "        \n",
        "        # 링크 확인\n",
        "        if '[' in content and '](' in content:\n",
        "            metadata['has_links'] = True\n",
        "        \n",
        "        # 헤더 개수\n",
        "        metadata['header_count'] = len(re.findall(r'^#{1,6}\\s+', content, re.MULTILINE))\n",
        "        \n",
        "        # 복잡도 추정\n",
        "        if metadata['has_code_blocks'] and metadata['has_tables']:\n",
        "            metadata['estimated_complexity'] = 'high'\n",
        "        elif metadata['has_code_blocks'] or metadata['has_tables']:\n",
        "            metadata['estimated_complexity'] = 'medium'\n",
        "        \n",
        "        return metadata\n",
        "    \n",
        "    def split_by_gitbook_structure(self, content: str, source: str = \"\") -> List[Document]:\n",
        "        \"\"\"GitBook 구조에 따른 분할\"\"\"\n",
        "        # 1단계: 헤더 기반 분할\n",
        "        header_docs = self.header_splitter.split_text(content)\n",
        "        \n",
        "        documents = []\n",
        "        for i, doc in enumerate(header_docs):\n",
        "            # 메타데이터 추출\n",
        "            metadata = self.extract_gitbook_metadata(doc.page_content)\n",
        "            \n",
        "            # GitBook 특성에 따른 추가 메타데이터\n",
        "            doc_metadata = {\n",
        "                **doc.metadata,\n",
        "                **metadata,\n",
        "                'source': source,\n",
        "                'split_method': 'header_based',\n",
        "                'chunk_idx': i,\n",
        "                'gitbook_processed': True\n",
        "            }\n",
        "            \n",
        "            # 새로운 Document 생성\n",
        "            new_doc = Document(\n",
        "                page_content=doc.page_content,\n",
        "                metadata=doc_metadata\n",
        "            )\n",
        "            documents.append(new_doc)\n",
        "        \n",
        "        return documents\n",
        "    \n",
        "    def split_by_content_type(self, content: str, source: str = \"\") -> List[Document]:\n",
        "        \"\"\"콘텐츠 타입에 따른 분할\"\"\"\n",
        "        documents = []\n",
        "        \n",
        "        # 코드 블록 분리\n",
        "        code_pattern = r'```(\\w+)?\\n([\\s\\S]*?)```'\n",
        "        code_blocks = re.findall(code_pattern, content)\n",
        "        \n",
        "        # 코드 블록을 임시로 마스킹\n",
        "        masked_content = re.sub(code_pattern, '___CODE_BLOCK___', content)\n",
        "        \n",
        "        # 일반 텍스트 분할\n",
        "        text_chunks = self.recursive_splitter.split_text(masked_content)\n",
        "        \n",
        "        # 코드 블록 복원 및 Document 생성\n",
        "        code_idx = 0\n",
        "        for i, chunk in enumerate(text_chunks):\n",
        "            # 코드 블록 복원\n",
        "            restored_chunk = chunk\n",
        "            while '___CODE_BLOCK___' in restored_chunk and code_idx < len(code_blocks):\n",
        "                lang, code = code_blocks[code_idx]\n",
        "                code_block = f\"```{lang}\\n{code}\\n```\"\n",
        "                restored_chunk = restored_chunk.replace('___CODE_BLOCK___', code_block, 1)\n",
        "                code_idx += 1\n",
        "            \n",
        "            # 메타데이터 생성\n",
        "            metadata = self.extract_gitbook_metadata(restored_chunk)\n",
        "            \n",
        "            doc = Document(\n",
        "                page_content=restored_chunk,\n",
        "                metadata={\n",
        "                    **metadata,\n",
        "                    'source': source,\n",
        "                    'split_method': 'content_type_based',\n",
        "                    'chunk_idx': i,\n",
        "                    'gitbook_processed': True\n",
        "                }\n",
        "            )\n",
        "            documents.append(doc)\n",
        "        \n",
        "        return documents\n",
        "    \n",
        "    def split_by_semantic_units(self, content: str, source: str = \"\") -> List[Document]:\n",
        "        \"\"\"의미적 단위로 분할\"\"\"\n",
        "        documents = []\n",
        "        \n",
        "        # 섹션별로 분할\n",
        "        sections = re.split(r'(?=^#{1,6}\\s+)', content, flags=re.MULTILINE)\n",
        "        \n",
        "        for i, section in enumerate(sections):\n",
        "            if not section.strip():\n",
        "                continue\n",
        "            \n",
        "            # 섹션 내에서 더 세분화\n",
        "            if len(section) > 2000:  # 큰 섹션은 다시 분할\n",
        "                subsections = self.recursive_splitter.split_text(section)\n",
        "                for j, subsection in enumerate(subsections):\n",
        "                    metadata = self.extract_gitbook_metadata(subsection)\n",
        "                    \n",
        "                    doc = Document(\n",
        "                        page_content=subsection,\n",
        "                        metadata={\n",
        "                            **metadata,\n",
        "                            'source': source,\n",
        "                            'split_method': 'semantic_units',\n",
        "                            'section_idx': i,\n",
        "                            'subsection_idx': j,\n",
        "                            'gitbook_processed': True\n",
        "                        }\n",
        "                    )\n",
        "                    documents.append(doc)\n",
        "            else:\n",
        "                metadata = self.extract_gitbook_metadata(section)\n",
        "                \n",
        "                doc = Document(\n",
        "                    page_content=section,\n",
        "                    metadata={\n",
        "                        **metadata,\n",
        "                        'source': source,\n",
        "                        'split_method': 'semantic_units',\n",
        "                        'section_idx': i,\n",
        "                        'gitbook_processed': True\n",
        "                    }\n",
        "                )\n",
        "                documents.append(doc)\n",
        "        \n",
        "        return documents\n",
        "    \n",
        "    def smart_split(self, content: str, source: str = \"\", strategy: str = \"auto\") -> List[Document]:\n",
        "        \"\"\"스마트 분할 메인 함수\"\"\"\n",
        "        # 전처리\n",
        "        processed_content = self.preprocess_gitbook_content(content)\n",
        "        \n",
        "        # 메타데이터 분석\n",
        "        metadata = self.extract_gitbook_metadata(processed_content)\n",
        "        \n",
        "        # 전략 선택\n",
        "        if strategy == \"auto\":\n",
        "            if metadata['header_count'] > 5:\n",
        "                strategy = \"structure\"\n",
        "            elif metadata['has_code_blocks'] and metadata['estimated_complexity'] == 'high':\n",
        "                strategy = \"content_type\"\n",
        "            else:\n",
        "                strategy = \"semantic\"\n",
        "        \n",
        "        # 분할 실행\n",
        "        if strategy == \"structure\":\n",
        "            documents = self.split_by_gitbook_structure(processed_content, source)\n",
        "        elif strategy == \"content_type\":\n",
        "            documents = self.split_by_content_type(processed_content, source)\n",
        "        elif strategy == \"semantic\":\n",
        "            documents = self.split_by_semantic_units(processed_content, source)\n",
        "        else:\n",
        "            raise ValueError(f\"Unknown strategy: {strategy}\")\n",
        "        \n",
        "        # 결과 통계\n",
        "        print(f\"📊 분할 결과:\")\n",
        "        print(f\"  - 전략: {strategy}\")\n",
        "        print(f\"  - 문서 수: {len(documents)}\")\n",
        "        print(f\"  - 평균 길이: {sum(len(doc.page_content) for doc in documents) // len(documents)} 문자\")\n",
        "        print(f\"  - 복잡도: {metadata['estimated_complexity']}\")\n",
        "        \n",
        "        return documents"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# GitBook 스마트 분할기 테스트\n",
        "def load_test_gitbook_content() -> str:\n",
        "    \"\"\"테스트용 GitBook 콘텐츠 로드\"\"\"\n",
        "    # 실제 GitBook 콘텐츠 예시\n",
        "    test_content = \"\"\"\n",
        "# 원스토어 인앱결제 API V7(SDK V21)\n",
        "\n",
        "원스토어의 최신 인앱결제 API V7(SDK V21)이 출시되었습니다.\n",
        "\n",
        "## 개요\n",
        "\n",
        "이 문서는 원스토어 인앱결제 API V7의 사용법을 설명합니다.\n",
        "\n",
        "### 주요 기능\n",
        "\n",
        "- API V7 지원\n",
        "- SDK V21 통합\n",
        "- 향상된 보안\n",
        "\n",
        "## 설치 방법\n",
        "\n",
        "### Gradle 설정\n",
        "\n",
        "```gradle\n",
        "dependencies {\n",
        "    implementation 'com.onestore:iap:21.0.0'\n",
        "}\n",
        "```\n",
        "\n",
        "### 초기화 코드\n",
        "\n",
        "```kotlin\n",
        "val purchaseClient = PurchaseClient.newBuilder()\n",
        "    .setListener(purchasesUpdatedListener)\n",
        "    .build()\n",
        "```\n",
        "\n",
        "## 구매 프로세스\n",
        "\n",
        "### 1. 상품 조회\n",
        "\n",
        "```kotlin\n",
        "purchaseClient.queryProductDetailsAsync(\n",
        "    QueryProductDetailsParams.newBuilder()\n",
        "        .setProductIds(listOf(\"product_id\"))\n",
        "        .build()\n",
        ")\n",
        "```\n",
        "\n",
        "### 2. 구매 실행\n",
        "\n",
        "```kotlin\n",
        "purchaseClient.launchPurchaseFlow(\n",
        "    activity,\n",
        "    PurchaseFlowParams.newBuilder()\n",
        "        .setProductId(\"product_id\")\n",
        "        .build()\n",
        ")\n",
        "```\n",
        "\n",
        "## 테이블 예시\n",
        "\n",
        "| 기능 | 설명 | 버전 |\n",
        "|------|------|------|\n",
        "| API V7 | 최신 API | 21.0.0 |\n",
        "| SDK V21 | 통합 SDK | 21.0.0 |\n",
        "\n",
        "## 이미지 예시\n",
        "\n",
        "![결제 화면](payment_screen.png)\n",
        "\n",
        "## 링크 예시\n",
        "\n",
        "[공식 문서](https://onestore-dev.gitbook.io)\n",
        "\n",
        "{% hint style=\"info\" %}\n",
        "이것은 GitBook 힌트입니다.\n",
        "{% endhint %}\n",
        "\n",
        "{% hint style=\"warning\" %}\n",
        "주의사항입니다.\n",
        "{% endhint %}\n",
        "\"\"\"\n",
        "    return test_content\n",
        "\n",
        "# 스마트 분할기 초기화\n",
        "splitter = GitBookSmartSplitter()\n",
        "\n",
        "# 테스트 콘텐츠 로드\n",
        "test_content = load_test_gitbook_content()\n",
        "print(f\"📄 테스트 콘텐츠 크기: {len(test_content)} 문자\")\n",
        "print(f\"📄 테스트 콘텐츠 미리보기:\")\n",
        "print(\"=\" * 60)\n",
        "print(test_content[:500] + \"...\")\n",
        "print(\"=\" * 60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 다양한 전략으로 분할 테스트\n",
        "strategies = [\"auto\", \"structure\", \"content_type\", \"semantic\"]\n",
        "\n",
        "for strategy in strategies:\n",
        "    print(f\"\\n🔧 전략: {strategy}\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    documents = splitter.smart_split(test_content, source=\"test_gitbook.md\", strategy=strategy)\n",
        "    \n",
        "    # 결과 분석\n",
        "    for i, doc in enumerate(documents[:3], 1):  # 처음 3개만 출력\n",
        "        print(f\"\\n📄 문서 {i}:\")\n",
        "        print(f\"  길이: {len(doc.page_content)} 문자\")\n",
        "        print(f\"  분할 방법: {doc.metadata.get('split_method', 'unknown')}\")\n",
        "        print(f\"  코드 블록: {doc.metadata.get('has_code_blocks', False)}\")\n",
        "        print(f\"  복잡도: {doc.metadata.get('estimated_complexity', 'unknown')}\")\n",
        "        print(f\"  내용: {doc.page_content[:100]}...\")\n",
        "    \n",
        "    if len(documents) > 3:\n",
        "        print(f\"\\n... 및 {len(documents) - 3}개 더\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 실제 GitBook 파일 처리\n",
        "def process_gitbook_file(file_path: str, output_dir: str = \"data/gitbook_splits\"):\n",
        "    \"\"\"실제 GitBook 파일을 처리합니다.\"\"\"\n",
        "    if not os.path.exists(file_path):\n",
        "        print(f\"❌ 파일을 찾을 수 없습니다: {file_path}\")\n",
        "        return\n",
        "    \n",
        "    # 파일 로드\n",
        "    with open(file_path, 'r', encoding='utf-8') as f:\n",
        "        content = f.read()\n",
        "    \n",
        "    print(f\"📄 파일 로드 완료: {len(content)} 문자\")\n",
        "    \n",
        "    # 스마트 분할\n",
        "    documents = splitter.smart_split(content, source=file_path, strategy=\"auto\")\n",
        "    \n",
        "    # 결과 저장\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    \n",
        "    # JSON 형태로 저장\n",
        "    docs_data = []\n",
        "    for i, doc in enumerate(documents):\n",
        "        doc_data = {\n",
        "            'id': i,\n",
        "            'content': doc.page_content,\n",
        "            'metadata': doc.metadata\n",
        "        }\n",
        "        docs_data.append(doc_data)\n",
        "    \n",
        "    output_file = os.path.join(output_dir, f\"{Path(file_path).stem}_split.json\")\n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(docs_data, f, ensure_ascii=False, indent=2)\n",
        "    \n",
        "    print(f\"✅ 분할 결과 저장 완료: {output_file}\")\n",
        "    \n",
        "    # 통계 출력\n",
        "    print(f\"\\n📊 분할 통계:\")\n",
        "    print(f\"  - 총 문서 수: {len(documents)}\")\n",
        "    print(f\"  - 평균 길이: {sum(len(doc.page_content) for doc in documents) // len(documents)} 문자\")\n",
        "    print(f\"  - 최소 길이: {min(len(doc.page_content) for doc in documents)} 문자\")\n",
        "    print(f\"  - 최대 길이: {max(len(doc.page_content) for doc in documents)} 문자\")\n",
        "    \n",
        "    # 메타데이터 통계\n",
        "    code_blocks_count = sum(1 for doc in documents if doc.metadata.get('has_code_blocks', False))\n",
        "    tables_count = sum(1 for doc in documents if doc.metadata.get('has_tables', False))\n",
        "    \n",
        "    print(f\"  - 코드 블록 포함: {code_blocks_count}개\")\n",
        "    print(f\"  - 테이블 포함: {tables_count}개\")\n",
        "    \n",
        "    return documents\n",
        "\n",
        "# 실제 파일 처리 (예시)\n",
        "test_files = [\n",
        "    \"data/iap_gitbook.md\",  # 이전에 생성한 통합 파일\n",
        "    \"data/dev_center_guide_touched.md\"  # 기존 파일\n",
        "]\n",
        "\n",
        "for file_path in test_files:\n",
        "    if os.path.exists(file_path):\n",
        "        print(f\"\\n🔧 처리 중: {file_path}\")\n",
        "        print(\"=\" * 60)\n",
        "        documents = process_gitbook_file(file_path)\n",
        "    else:\n",
        "        print(f\"\\n⚠️ 파일이 없습니다: {file_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 분할 품질 평가\n",
        "def evaluate_split_quality(documents: List[Document]) -> Dict[str, Any]:\n",
        "    \"\"\"분할 품질을 평가합니다.\"\"\"\n",
        "    \n",
        "    # 길이 분포 분석\n",
        "    lengths = [len(doc.page_content) for doc in documents]\n",
        "    avg_length = sum(lengths) / len(lengths)\n",
        "    \n",
        "    # 키워드 보존률 확인\n",
        "    important_keywords = [\n",
        "        \"launchPurchaseFlow\", \"PurchaseClient\", \"IapResult\",\n",
        "        \"API\", \"SDK\", \"결제\", \"구매\"\n",
        "    ]\n",
        "    \n",
        "    keyword_preservation = {}\n",
        "    for keyword in important_keywords:\n",
        "        docs_with_keyword = [\n",
        "            doc for doc in documents \n",
        "            if keyword.lower() in doc.page_content.lower()\n",
        "        ]\n",
        "        keyword_preservation[keyword] = len(docs_with_keyword)\n",
        "    \n",
        "    # 메타데이터 품질\n",
        "    metadata_completeness = sum(\n",
        "        1 for doc in documents \n",
        "        if doc.metadata.get('gitbook_processed', False)\n",
        "    ) / len(documents)\n",
        "    \n",
        "    # 중복 콘텐츠 확인\n",
        "    content_hashes = set()\n",
        "    duplicates = 0\n",
        "    for doc in documents:\n",
        "        content_hash = hash(doc.page_content[:100])  # 처음 100자로 해시\n",
        "        if content_hash in content_hashes:\n",
        "            duplicates += 1\n",
        "        content_hashes.add(content_hash)\n",
        "    \n",
        "    duplicate_rate = duplicates / len(documents) if documents else 0\n",
        "    \n",
        "    return {\n",
        "        'total_documents': len(documents),\n",
        "        'average_length': avg_length,\n",
        "        'length_variance': sum((l - avg_length) ** 2 for l in lengths) / len(lengths),\n",
        "        'keyword_preservation': keyword_preservation,\n",
        "        'metadata_completeness': metadata_completeness,\n",
        "        'duplicate_rate': duplicate_rate,\n",
        "        'quality_score': (1 - duplicate_rate) * metadata_completeness * 100\n",
        "    }\n",
        "\n",
        "# 품질 평가 실행\n",
        "if os.path.exists(\"data/iap_gitbook.md\"):\n",
        "    with open(\"data/iap_gitbook.md\", 'r', encoding='utf-8') as f:\n",
        "        content = f.read()\n",
        "    \n",
        "    documents = splitter.smart_split(content, source=\"iap_gitbook.md\", strategy=\"auto\")\n",
        "    \n",
        "    quality_metrics = evaluate_split_quality(documents)\n",
        "    \n",
        "    print(f\"📊 분할 품질 평가 결과:\")\n",
        "    print(f\"  - 총 문서 수: {quality_metrics['total_documents']}\")\n",
        "    print(f\"  - 평균 길이: {quality_metrics['average_length']:.1f} 문자\")\n",
        "    print(f\"  - 길이 분산: {quality_metrics['length_variance']:.1f}\")\n",
        "    print(f\"  - 메타데이터 완성도: {quality_metrics['metadata_completeness']:.2f}\")\n",
        "    print(f\"  - 중복률: {quality_metrics['duplicate_rate']:.2f}\")\n",
        "    print(f\"  - 품질 점수: {quality_metrics['quality_score']:.1f}/100\")\n",
        "    \n",
        "    print(f\"\\n🔍 키워드 보존률:\")\n",
        "    for keyword, count in quality_metrics['keyword_preservation'].items():\n",
        "        print(f\"  - {keyword}: {count}개 문서에서 발견\")\n",
        "else:\n",
        "    print(\"❌ 평가할 파일이 없습니다.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# RAG 최적화 분할\n",
        "def create_rag_optimized_splits(content: str, source: str = \"\") -> List[Document]:\n",
        "    \"\"\"RAG 시스템에 최적화된 분할\"\"\"\n",
        "    \n",
        "    # 1단계: 기본 스마트 분할\n",
        "    base_docs = splitter.smart_split(content, source, strategy=\"auto\")\n",
        "    \n",
        "    # 2단계: RAG 최적화\n",
        "    optimized_docs = []\n",
        "    \n",
        "    for doc in base_docs:\n",
        "        # 너무 작은 청크는 건너뛰기\n",
        "        if len(doc.page_content) < 100:\n",
        "            continue\n",
        "        \n",
        "        # 너무 큰 청크는 재분할\n",
        "        if len(doc.page_content) > 2000:\n",
        "            sub_docs = splitter.recursive_splitter.split_text(doc.page_content)\n",
        "            for i, sub_content in enumerate(sub_docs):\n",
        "                if len(sub_content) >= 100:  # 최소 길이 확인\n",
        "                    sub_metadata = {\n",
        "                        **doc.metadata,\n",
        "                        'split_method': 'rag_optimized',\n",
        "                        'sub_chunk_idx': i,\n",
        "                        'original_length': len(doc.page_content)\n",
        "                    }\n",
        "                    \n",
        "                    optimized_docs.append(Document(\n",
        "                        page_content=sub_content,\n",
        "                        metadata=sub_metadata\n",
        "                    ))\n",
        "        else:\n",
        "            # 적절한 크기는 그대로 사용\n",
        "            doc.metadata['split_method'] = 'rag_optimized'\n",
        "            optimized_docs.append(doc)\n",
        "    \n",
        "    # 3단계: 키워드 강화\n",
        "    for doc in optimized_docs:\n",
        "        # 중요 키워드가 포함된 문서는 우선순위 부여\n",
        "        important_keywords = [\n",
        "            \"launchPurchaseFlow\", \"PurchaseClient\", \"IapResult\",\n",
        "            \"API\", \"SDK\", \"결제\", \"구매\"\n",
        "        ]\n",
        "        \n",
        "        keyword_count = sum(\n",
        "            1 for keyword in important_keywords\n",
        "            if keyword.lower() in doc.page_content.lower()\n",
        "        )\n",
        "        \n",
        "        doc.metadata['keyword_score'] = keyword_count\n",
        "        doc.metadata['is_important'] = keyword_count >= 2\n",
        "    \n",
        "    print(f\"✅ RAG 최적화 완료: {len(optimized_docs)}개 문서\")\n",
        "    print(f\"  - 중요 문서: {sum(1 for doc in optimized_docs if doc.metadata.get('is_important', False))}개\")\n",
        "    \n",
        "    return optimized_docs\n",
        "\n",
        "# RAG 최적화 분할 테스트\n",
        "if os.path.exists(\"data/iap_gitbook.md\"):\n",
        "    with open(\"data/iap_gitbook.md\", 'r', encoding='utf-8') as f:\n",
        "        content = f.read()\n",
        "    \n",
        "    rag_docs = create_rag_optimized_splits(content, \"iap_gitbook.md\")\n",
        "    \n",
        "    # 결과 저장\n",
        "    output_file = \"data/rag_optimized_splits.json\"\n",
        "    docs_data = [\n",
        "        {\n",
        "            'id': i,\n",
        "            'content': doc.page_content,\n",
        "            'metadata': doc.metadata\n",
        "        }\n",
        "        for i, doc in enumerate(rag_docs)\n",
        "    ]\n",
        "    \n",
        "    with open(output_file, 'w', encoding='utf-8') as f:\n",
        "        json.dump(docs_data, f, ensure_ascii=False, indent=2)\n",
        "    \n",
        "    print(f\"✅ RAG 최적화 결과 저장: {output_file}\")\n",
        "else:\n",
        "    print(\"❌ RAG 최적화를 위한 파일이 없습니다.\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}