import os
import re
import pickle
from typing import List
from pathlib import Path
from langchain.docstore.document import Document
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_ollama import OllamaEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import MarkdownHeaderTextSplitter
from langchain.retrievers import EnsembleRetriever
from langchain_community.retrievers import BM25Retriever
from typing import List, Dict, Any, Tuple, Optional

class CommonRAGUtil:
    def __init__(self):
        pass

    def save_documents(self, docs: List[Document], output_path: str):
        """
        List[Document]ë¥¼ pickle íŒŒì¼ë¡œ ì €ì¥í•©ë‹ˆë‹¤.
        
        Args:
            docs (List[Document]): ì €ì¥í•  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            output_path (str): ì €ì¥í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ
        """
        os.makedirs(output_path, exist_ok=True)
        docs_file_path = os.path.join(output_path, "documents.pkl")
        
        with open(docs_file_path, "wb") as f:
            pickle.dump(docs, f)
        
        print(f"âœ… ë¬¸ì„œ ì €ì¥ ì™„ë£Œ: {docs_file_path}")
        print(f"ğŸ“„ ì €ì¥ëœ ë¬¸ì„œ ìˆ˜: {len(docs)}")


    def load_documents(self, input_path: str) -> List[Document]:
        """
        ì €ì¥ëœ pickle íŒŒì¼ì—ì„œ List[Document]ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
        
        Args:
            input_path (str): ë¬¸ì„œê°€ ì €ì¥ëœ ë””ë ‰í† ë¦¬ ê²½ë¡œ ë˜ëŠ” íŒŒì¼ ê²½ë¡œ
            
        Returns:
            List[Document]: ë¡œë“œëœ ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
        """
        # ë””ë ‰í† ë¦¬ ê²½ë¡œì¸ ê²½ìš° documents.pkl íŒŒì¼ì„ ì°¾ìŒ
        if os.path.isdir(input_path):
            docs_file_path = os.path.join(input_path, "documents.pkl")
        else:
            docs_file_path = input_path
        
        if not os.path.exists(docs_file_path):
            raise FileNotFoundError(f"ë¬¸ì„œ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {docs_file_path}")
        
        with open(docs_file_path, "rb") as f:
            docs = pickle.load(f)
        
        print(f"âœ… ë¬¸ì„œ ë¡œë“œ ì™„ë£Œ: {docs_file_path}")
        print(f"ğŸ“„ ë¡œë“œëœ ë¬¸ì„œ ìˆ˜: {len(docs)}")
        
        return docs


    def embed_and_save_with_docs(self, docs: List[Document], output_path: str, model_name: str = "bge-m3:latest"):
        """
        ë¬¸ì„œë¥¼ ì„ë² ë”©í•˜ê³  FAISS ë°ì´í„°ë² ì´ìŠ¤ë¡œ ì €ì¥í•˜ë©°, ë™ì‹œì— ì›ë³¸ ë¬¸ì„œë„ ì €ì¥í•©ë‹ˆë‹¤.
        
        Args:
            docs (List[Document]): ì²˜ë¦¬í•  ë¬¸ì„œ ë¦¬ìŠ¤íŠ¸
            output_path (str): ì €ì¥í•  ë””ë ‰í† ë¦¬ ê²½ë¡œ
            model_name (str): ì„ë² ë”© ëª¨ë¸ëª…
        """
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        embedding_model = OllamaEmbeddings(model=model_name)
        
        # FAISS ë°ì´í„°ë² ì´ìŠ¤ ìƒì„± ë° ì €ì¥
        db = FAISS.from_documents(docs, embedding_model)
        db.save_local(output_path)
        print(f"âœ… ì„ë² ë”© ì €ì¥ ì™„ë£Œ: {output_path}")
        
        # ì›ë³¸ ë¬¸ì„œë„ í•¨ê»˜ ì €ì¥
        self.save_documents(docs, output_path)
        

    def load_both_faiss_and_docs(self, folder_path: str, model_name: str = "bge-m3:latest") -> tuple[FAISS, List[Document]]:
        """
        FAISS ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì™€ ì›ë³¸ ë¬¸ì„œë¥¼ ëª¨ë‘ ë¡œë“œí•©ë‹ˆë‹¤.
        
        Args:
            folder_path (str): ë°ì´í„°ê°€ ì €ì¥ëœ ë””ë ‰í† ë¦¬ ê²½ë¡œ
            model_name (str): ì„ë² ë”© ëª¨ë¸ëª…
            
        Returns:
            tuple: (FAISS ë°ì´í„°ë² ì´ìŠ¤, List[Document])
        """
        # ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™”
        embedding_model = OllamaEmbeddings(model=model_name)
        
        # FAISS ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ
        loaded_db = FAISS.load_local(
            folder_path=folder_path,
            embeddings=embedding_model,
            allow_dangerous_deserialization=True,
        )
        print(f"âœ… FAISS ë°ì´í„°ë² ì´ìŠ¤ ë¡œë“œ ì™„ë£Œ: {folder_path}")
        
        # ì›ë³¸ ë¬¸ì„œ ë¡œë“œ
        docs = self.load_documents(folder_path)
        
        return loaded_db, docs
    
class SmartRetriever:
    """ìŠ¤ë§ˆíŠ¸ ê²€ìƒ‰ê¸° - í‚¤ì›Œë“œ ìš°ì„ ìˆœìœ„ ê¸°ë°˜"""
    
    def __init__(self, documents: List[Document], embedding_model_name: str = "bge-m3:latest"):
        self.documents = documents
        self.embedding_model_name = embedding_model_name
        self.vector_store = None
        self.bm25_retriever = None
        self.ensemble_retriever = None
        
    def build_retrievers(self):
        """ê²€ìƒ‰ê¸° êµ¬ì¶•"""
        print(f"ğŸ”§ ê²€ìƒ‰ê¸° êµ¬ì¶• ì¤‘... (ë¬¸ì„œ ìˆ˜: {len(self.documents)})")
        
        # Vector store êµ¬ì¶•
        embeddings = OllamaEmbeddings(model=self.embedding_model_name)
        self.vector_store = FAISS.from_documents(self.documents, embeddings)
        
        # BM25 ê²€ìƒ‰ê¸° êµ¬ì¶•
        self.bm25_retriever = BM25Retriever.from_documents(
            self.documents,
            bm25_params={"k1": 1.5, "b": 0.75}
        )
        self.bm25_retriever.k = 20
        
        # Vector ê²€ìƒ‰ê¸°
        vector_retriever = self.vector_store.as_retriever(
            search_type="mmr",
            search_kwargs={"k": 20, "fetch_k": 50, "lambda_mult": 0.7}
        )
        
        # ì•™ìƒë¸” ê²€ìƒ‰ê¸°
        self.ensemble_retriever = EnsembleRetriever(
            retrievers=[self.bm25_retriever, vector_retriever],
            weights=[0.6, 0.4]  # BM25ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜
        )
        
        print("âœ… ê²€ìƒ‰ê¸° êµ¬ì¶• ì™„ë£Œ")
    
    def get_retriever(self):
        return self.ensemble_retriever
    
    def smart_search(self, query: str, max_results: int = 10) -> List[Document]:
        """ìŠ¤ë§ˆíŠ¸ ê²€ìƒ‰ - í‚¤ì›Œë“œ ìš°ì„ ìˆœìœ„ ì ìš©"""
        if not self.ensemble_retriever:
            raise ValueError("ê²€ìƒ‰ê¸°ê°€ êµ¬ì¶•ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. build_retrievers()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        
        # 1. ì•™ìƒë¸” ê²€ìƒ‰ìœ¼ë¡œ ë” ë§ì€ í›„ë³´ ê²€ìƒ‰
        raw_results = self.ensemble_retriever.invoke(query)
        
        # 2. í‚¤ì›Œë“œ ê¸°ë°˜ í•„í„°ë§ ë° ì ìˆ˜ ê³„ì‚°
        scored_results = self._score_documents(query, raw_results)
        
        # 3. ì ìˆ˜ìˆœ ì •ë ¬
        scored_results.sort(key=lambda x: x[0], reverse=True)
        
        # 4. ìƒìœ„ ê²°ê³¼ ë°˜í™˜
        return [doc for score, doc in scored_results[:max_results]]
    
    def _score_documents(self, query: str, documents: List[Document]) -> List[Tuple[float, Document]]:
        """ë¬¸ì„œ ì ìˆ˜ ê³„ì‚°"""
        scored_docs = []
        query_keywords = self._extract_query_keywords(query)
        
        for doc in documents:
            score = 0.0
            content_lower = doc.page_content.lower()
            
            # 1. í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ (ê°€ì¥ ì¤‘ìš”)
            keyword_matches = 0
            for keyword in query_keywords:
                if keyword.lower() in content_lower:
                    keyword_matches += 1
                    # ì •í™•í•œ ë§¤ì¹­ì— ë†’ì€ ì ìˆ˜
                    if keyword.lower() == keyword.lower():  # ì™„ì „ ì¼ì¹˜
                        score += 10
                    else:
                        score += 5
            
            # 2. í‚¤ì›Œë“œ ë°€ë„ ì ìˆ˜
            density = doc.metadata.get('keyword_density', 0)
            score += density * 20
            
            # 3. ì „ëµë³„ ë³´ë„ˆìŠ¤ ì ìˆ˜
            strategy = doc.metadata.get('source_strategy', '')
            if 'keyword' in strategy:
                score += 5
            
            # 4. ìœ„ì¹˜ ì ìˆ˜ (ë¬¸ì„œ ì•ë¶€ë¶„ì— í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ê°€ì )
            first_half = content_lower[:len(content_lower)//2]
            if any(kw.lower() in first_half for kw in query_keywords):
                score += 8
            
            # 5. ê¸¸ì´ ì ì •ì„± ì ìˆ˜
            doc_length = len(doc.page_content.split())
            if 50 <= doc_length <= 300:  # ì ì • ê¸¸ì´
                score += 3
            
            doc.metadata['search_score'] = score
            doc.metadata['keyword_matches'] = keyword_matches
            scored_docs.append((score, doc))
        
        return scored_docs
    
    def _extract_query_keywords(self, query: str) -> List[str]:
        """ì¿¼ë¦¬ì—ì„œ ì¤‘ìš” í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ê¸°ìˆ  ìš©ì–´ íŒ¨í„´
        tech_patterns = [
            r'\b[A-Z]{2,}\b',  # PNS, API ë“±
            r'\b[a-z]+[A-Z][a-zA-Z]*\b',  # purchaseState ë“±
        ]
        
        keywords = []
        for pattern in tech_patterns:
            keywords.extend(re.findall(pattern, query))
        
        # í•œê¸€ í‚¤ì›Œë“œ ì¶”ê°€
        korean_keywords = ['ë©”ì‹œì§€', 'ê·œê²©', 'ê°’', 'êµ¬ì„±', 'ìƒíƒœ', 'ê²°ì œ', 'ì„œë²„']
        for keyword in korean_keywords:
            if keyword in query:
                keywords.append(keyword)
        
        return list(set(keywords))