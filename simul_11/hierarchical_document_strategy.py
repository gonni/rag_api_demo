"""
ê³„ì¸µì  ë¬¸ì„œ ë¶„í•  ì „ëµ - PNS ëŒ€ì œëª©ê³¼ í•˜ìœ„ ì„¹ì…˜ ì—°ê²° ë¬¸ì œ í•´ê²°

ì´ ëª¨ë“ˆì€ ë‹¤ìŒê³¼ ê°™ì€ ì „ëµë“¤ì„ êµ¬í˜„í•©ë‹ˆë‹¤:
1. ê³„ì¸µì  ì œëª© í¬í•¨ ì „ëµ (Title Hierarchy Inclusion)
2. ë‹¤ì¤‘ ë ˆë²¨ ë¶„í•  ì „ëµ (Multi-Level Splitting)
3. ì»¨í…ìŠ¤íŠ¸ ìƒì† ì „ëµ (Context Inheritance)
"""

import os
import re
import pickle
from typing import List, Dict, Any, Tuple, Optional, Union
from dataclasses import dataclass
from langchain.docstore.document import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_text_splitters import MarkdownHeaderTextSplitter
from langchain_ollama import OllamaEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever


@dataclass
class HierarchicalSearchResult:
    """ê³„ì¸µì  ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° í´ë˜ìŠ¤"""
    strategy_name: str
    query: str
    documents: List[Document]
    hierarchy_scores: List[float]
    context_scores: List[float]
    total_docs: int
    relevant_docs: int
    pns_related_docs: int


class HierarchicalTitleStrategy:
    """ê³„ì¸µì  ì œëª© í¬í•¨ ì „ëµ"""
    
    def __init__(self, document_path: str):
        self.document_path = document_path
        self.raw_text = self._load_document()
        
    def _load_document(self) -> str:
        with open(self.document_path, 'r', encoding='utf-8') as f:
            return f.read()
    
    def split_documents(self) -> List[Document]:
        """ê³„ì¸µì  ì œëª©ì„ í¬í•¨í•˜ì—¬ ë¬¸ì„œ ë¶„í• """
        documents = []
        
        # í—¤ë” ê¸°ë°˜ ë¶„í• 
        header_splitter = MarkdownHeaderTextSplitter(
            headers_to_split_on=[
                ("#", "Header 1"),
                ("##", "Header 2"),
                ("###", "Header 3"),
                ("####", "Header 4")
            ]
        )
        header_docs = header_splitter.split_text(self.raw_text)
        
        for doc in header_docs:
            # ê³„ì¸µì  ì œëª© ê²½ë¡œ ìƒì„±
            title_hierarchy = self._build_title_hierarchy(doc.metadata)
            
            # ì›ë³¸ ë‚´ìš©ì— ì œëª© ê³„ì¸µ ì¶”ê°€
            enhanced_content = self._add_title_context(doc.page_content, title_hierarchy)
            
            # ì ì ˆí•œ í¬ê¸°ë¡œ ì²­í‚¹
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=200,
                separators=["\n\n", "\n", ". ", "? ", "! ", ", "]
            )
            chunks = text_splitter.split_text(enhanced_content)
            
            for i, chunk in enumerate(chunks):
                metadata = doc.metadata.copy()
                metadata.update({
                    "chunk_index": i,
                    "title_hierarchy": title_hierarchy,
                    "source_strategy": "hierarchical_title",
                    "hierarchy_level": len([h for h in title_hierarchy.split(" > ") if h.strip()]),
                    "contains_pns": "PNS" in title_hierarchy.upper() or "PAYMENT NOTIFICATION" in title_hierarchy.upper()
                })
                documents.append(Document(page_content=chunk, metadata=metadata))
        
        return documents
    
    def _build_title_hierarchy(self, metadata: Dict) -> str:
        """ë©”íƒ€ë°ì´í„°ì—ì„œ ì œëª© ê³„ì¸µ êµ¬ì¡° ìƒì„±"""
        hierarchy_parts = []
        
        # í—¤ë” ë ˆë²¨ë³„ë¡œ ì œëª© ìˆ˜ì§‘
        for level in range(1, 5):
            header_key = f"Header {level}"
            if header_key in metadata and metadata[header_key]:
                title = metadata[header_key].strip()
                if title:
                    hierarchy_parts.append(title)
        
        return " > ".join(hierarchy_parts) if hierarchy_parts else "Unknown Section"
    
    def _add_title_context(self, content: str, title_hierarchy: str) -> str:
        """ë‚´ìš©ì— ì œëª© ê³„ì¸µ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€"""
        context_header = f"[ì„¹ì…˜ ê²½ë¡œ]: {title_hierarchy}\n\n"
        return context_header + content


class MultiLevelSplittingStrategy:
    """ë‹¤ì¤‘ ë ˆë²¨ ë¶„í•  ì „ëµ - í° ë‹¨ìœ„ + ì‘ì€ ë‹¨ìœ„ ë¬¸ì„œ ìƒì„±"""
    
    def __init__(self, document_path: str):
        self.document_path = document_path
        self.raw_text = self._load_document()
        
    def _load_document(self) -> str:
        with open(self.document_path, 'r', encoding='utf-8') as f:
            return f.read()
    
    def split_documents(self) -> List[Document]:
        """ë‹¤ì¤‘ ë ˆë²¨ë¡œ ë¬¸ì„œ ë¶„í• """
        documents = []
        
        # í—¤ë” ê¸°ë°˜ ë¶„í• 
        header_splitter = MarkdownHeaderTextSplitter(
            headers_to_split_on=[
                ("#", "Header 1"),
                ("##", "Header 2"),
                ("###", "Header 3"),
                ("####", "Header 4")
            ]
        )
        header_docs = header_splitter.split_text(self.raw_text)
        
        # ê³„ì¸µë³„ë¡œ ë¬¸ì„œ ê·¸ë£¹í•‘
        hierarchy_groups = self._group_by_hierarchy(header_docs)
        
        # ê° ë ˆë²¨ë³„ ë¬¸ì„œ ìƒì„±
        for level, group_docs in hierarchy_groups.items():
            documents.extend(self._create_level_documents(group_docs, level))
        
        return documents
    
    def _group_by_hierarchy(self, header_docs: List[Document]) -> Dict[str, List[Document]]:
        """ê³„ì¸µë³„ë¡œ ë¬¸ì„œ ê·¸ë£¹í•‘"""
        groups: Dict[str, List[Document]] = {
            "major": [],     # ëŒ€ì œëª© ë ˆë²¨ (H1)
            "medium": [],    # ì¤‘ì œëª© ë ˆë²¨ (H2)
            "minor": [],     # ì†Œì œëª© ë ˆë²¨ (H3, H4)
        }
        
        for doc in header_docs:
            # ê°€ì¥ ë†’ì€ í—¤ë” ë ˆë²¨ í™•ì¸
            if "Header 1" in doc.metadata:
                groups["major"].append(doc)
            elif "Header 2" in doc.metadata:
                groups["medium"].append(doc)
            else:
                groups["minor"].append(doc)
        
        return groups
    
    def _create_level_documents(self, docs: List[Document], level: str) -> List[Document]:
        """ë ˆë²¨ë³„ ë¬¸ì„œ ìƒì„±"""
        level_documents = []
        
        # ë ˆë²¨ë³„ ì²­í¬ í¬ê¸° ì„¤ì •
        chunk_sizes = {
            "major": 2000,    # ëŒ€ì œëª©: í° ì»¨í…ìŠ¤íŠ¸ ë³´ì¡´
            "medium": 1200,   # ì¤‘ì œëª©: ì¤‘ê°„ í¬ê¸°
            "minor": 800      # ì†Œì œëª©: ì„¸ë°€í•œ ë¶„í• 
        }
        
        chunk_size = chunk_sizes.get(level, 1000)
        
        for doc in docs:
            # ê³„ì¸µì  ì œëª© ìƒì„±
            title_hierarchy = self._build_title_hierarchy(doc.metadata)
            
            # ì»¨í…ìŠ¤íŠ¸ ê°•í™”
            enhanced_content = self._enhance_with_context(doc.page_content, title_hierarchy, level)
            
            # ì²­í‚¹
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=chunk_size,
                chunk_overlap=200,
                separators=["\n\n", "\n", ". ", "? ", "! ", ", "]
            )
            chunks = text_splitter.split_text(enhanced_content)
            
            for i, chunk in enumerate(chunks):
                metadata = doc.metadata.copy()
                metadata.update({
                    "chunk_index": i,
                    "hierarchy_level": level,
                    "title_hierarchy": title_hierarchy,
                    "source_strategy": f"multi_level_{level}",
                    "chunk_size": len(chunk),
                    "contains_pns": self._check_pns_context(chunk, title_hierarchy)
                })
                level_documents.append(Document(page_content=chunk, metadata=metadata))
        
        return level_documents
    
    def _build_title_hierarchy(self, metadata: Dict) -> str:
        """ì œëª© ê³„ì¸µ êµ¬ì¡° ìƒì„±"""
        hierarchy_parts = []
        for level in range(1, 5):
            header_key = f"Header {level}"
            if header_key in metadata and metadata[header_key]:
                hierarchy_parts.append(metadata[header_key].strip())
        return " > ".join(hierarchy_parts) if hierarchy_parts else "Unknown"
    
    def _enhance_with_context(self, content: str, title_hierarchy: str, level: str) -> str:
        """ë ˆë²¨ë³„ ì»¨í…ìŠ¤íŠ¸ ê°•í™”"""
        # PNS ê´€ë ¨ ì„¹ì…˜ì¸ì§€ í™•ì¸
        is_pns_section = "PNS" in title_hierarchy.upper() or "PAYMENT NOTIFICATION" in title_hierarchy.upper()
        
        context_info = f"[ê³„ì¸µ]: {title_hierarchy}\n"
        
        if is_pns_section:
            context_info += "[PNS ê´€ë ¨]: ì´ ë‚´ìš©ì€ PNS(Payment Notification Service) ê²°ì œì•Œë¦¼ì„œë¹„ìŠ¤ì™€ ê´€ë ¨ë©ë‹ˆë‹¤.\n"
        
        context_info += f"[ë ˆë²¨]: {level}\n\n"
        
        return context_info + content
    
    def _check_pns_context(self, content: str, title_hierarchy: str) -> bool:
        """PNS ì»¨í…ìŠ¤íŠ¸ ì—¬ë¶€ í™•ì¸"""
        content_upper = content.upper()
        hierarchy_upper = title_hierarchy.upper()
        
        return ("PNS" in hierarchy_upper or 
                "PAYMENT NOTIFICATION" in hierarchy_upper or
                "PNS" in content_upper or
                "PAYMENT NOTIFICATION" in content_upper)


class ContextInheritanceStrategy:
    """ì»¨í…ìŠ¤íŠ¸ ìƒì† ì „ëµ - ìƒìœ„ ì„¹ì…˜ ì •ë³´ë¥¼ í•˜ìœ„ ì„¹ì…˜ì— ìƒì†"""
    
    def __init__(self, document_path: str):
        self.document_path = document_path
        self.raw_text = self._load_document()
        
    def _load_document(self) -> str:
        with open(self.document_path, 'r', encoding='utf-8') as f:
            return f.read()
    
    def split_documents(self) -> List[Document]:
        """ì»¨í…ìŠ¤íŠ¸ ìƒì†ì„ í†µí•œ ë¬¸ì„œ ë¶„í• """
        documents = []
        
        # í—¤ë” ê¸°ë°˜ ë¶„í• 
        header_splitter = MarkdownHeaderTextSplitter(
            headers_to_split_on=[
                ("#", "Header 1"),
                ("##", "Header 2"),
                ("###", "Header 3"),
                ("####", "Header 4")
            ]
        )
        header_docs = header_splitter.split_text(self.raw_text)
        
        # ê³„ì¸µì  ì»¨í…ìŠ¤íŠ¸ êµ¬ì¶•
        context_map = self._build_context_map(header_docs)
        
        for doc in header_docs:
            # ìƒì†ëœ ì»¨í…ìŠ¤íŠ¸ ê°€ì ¸ì˜¤ê¸°
            inherited_context = self._get_inherited_context(doc.metadata, context_map)
            
            # ì»¨í…ìŠ¤íŠ¸ í¬í•¨í•œ ë‚´ìš© ìƒì„±
            enhanced_content = self._create_context_enhanced_content(
                doc.page_content, 
                doc.metadata, 
                inherited_context
            )
            
            # ì²­í‚¹
            text_splitter = RecursiveCharacterTextSplitter(
                chunk_size=1000,
                chunk_overlap=200,
                separators=["\n\n", "\n", ". ", "? ", "! ", ", "]
            )
            chunks = text_splitter.split_text(enhanced_content)
            
            for i, chunk in enumerate(chunks):
                metadata = doc.metadata.copy()
                metadata.update({
                    "chunk_index": i,
                    "inherited_context": inherited_context,
                    "source_strategy": "context_inheritance",
                    "context_keywords": self._extract_context_keywords(inherited_context),
                    "pns_inheritance": self._check_pns_inheritance(inherited_context)
                })
                documents.append(Document(page_content=chunk, metadata=metadata))
        
        return documents
    
    def _build_context_map(self, header_docs: List[Document]) -> Dict[str, str]:
        """ê³„ì¸µì  ì»¨í…ìŠ¤íŠ¸ ë§µ êµ¬ì¶•"""
        context_map = {}
        
        for doc in header_docs:
            # ê° í—¤ë” ë ˆë²¨ë³„ ì»¨í…ìŠ¤íŠ¸ ì €ì¥
            for level in range(1, 5):
                header_key = f"Header {level}"
                if header_key in doc.metadata and doc.metadata[header_key]:
                    title = doc.metadata[header_key].strip()
                    
                    # ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ì¶”ì¶œ (ì²« ë¬¸ë‹¨)
                    first_paragraph = doc.page_content.split('\n\n')[0][:200]
                    context_map[title] = first_paragraph
        
        return context_map
    
    def _get_inherited_context(self, metadata: Dict, context_map: Dict[str, str]) -> str:
        """ìƒì†ë  ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘"""
        inherited_contexts = []
        
        # ìƒìœ„ ë ˆë²¨ë¶€í„° ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘
        for level in range(1, 5):
            header_key = f"Header {level}"
            if header_key in metadata and metadata[header_key]:
                title = metadata[header_key].strip()
                if title in context_map:
                    context_info = f"[{title}]: {context_map[title]}"
                    inherited_contexts.append(context_info)
        
        return "\n".join(inherited_contexts) if inherited_contexts else ""
    
    def _create_context_enhanced_content(self, content: str, metadata: Dict, inherited_context: str) -> str:
        """ì»¨í…ìŠ¤íŠ¸ê°€ ê°•í™”ëœ ë‚´ìš© ìƒì„±"""
        # í˜„ì¬ ì„¹ì…˜ì˜ ì œëª© ê²½ë¡œ
        title_path = self._build_title_path(metadata)
        
        enhanced_content = f"[ìƒìœ„ ì»¨í…ìŠ¤íŠ¸]:\n{inherited_context}\n\n"
        enhanced_content += f"[í˜„ì¬ ì„¹ì…˜]: {title_path}\n\n"
        enhanced_content += f"[ë‚´ìš©]:\n{content}"
        
        return enhanced_content
    
    def _build_title_path(self, metadata: Dict) -> str:
        """ì œëª© ê²½ë¡œ ìƒì„±"""
        path_parts = []
        for level in range(1, 5):
            header_key = f"Header {level}"
            if header_key in metadata and metadata[header_key]:
                path_parts.append(metadata[header_key].strip())
        return " > ".join(path_parts) if path_parts else "Unknown"
    
    def _extract_context_keywords(self, context: str) -> List[str]:
        """ì»¨í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ê¸°ìˆ  ìš©ì–´ íŒ¨í„´
        tech_patterns = [
            r'\b[A-Z]{2,}\b',  # PNS, API ë“±
            r'\b[a-z]+[A-Z][a-zA-Z]*\b',  # purchaseState ë“±
        ]
        
        keywords = []
        for pattern in tech_patterns:
            keywords.extend(re.findall(pattern, context))
        
        # í•œê¸€ í‚¤ì›Œë“œ
        korean_keywords = ['ê²°ì œ', 'ì•Œë¦¼', 'ì„œë¹„ìŠ¤', 'ë©”ì‹œì§€', 'ê·œê²©', 'ìƒíƒœ', 'êµ¬ë§¤']
        for keyword in korean_keywords:
            if keyword in context:
                keywords.append(keyword)
        
        return list(set(keywords))
    
    def _check_pns_inheritance(self, context: str) -> bool:
        """PNS ìƒì† ì—¬ë¶€ í™•ì¸"""
        context_upper = context.upper()
        return ("PNS" in context_upper or 
                "PAYMENT NOTIFICATION" in context_upper or
                "ê²°ì œ ì•Œë¦¼" in context or
                "ê²°ì œì•Œë¦¼" in context)


class HierarchicalSmartRetriever:
    """ê³„ì¸µì  ìŠ¤ë§ˆíŠ¸ ê²€ìƒ‰ê¸°"""
    
    def __init__(self, documents: List[Document], embedding_model_name: str = "bge-m3:latest"):
        self.documents = documents
        self.embedding_model_name = embedding_model_name
        self.vector_store = None
        self.bm25_retriever = None
        self.ensemble_retriever = None
        
    def build_retrievers(self):
        """ê²€ìƒ‰ê¸° êµ¬ì¶•"""
        print(f"ğŸ”§ ê³„ì¸µì  ê²€ìƒ‰ê¸° êµ¬ì¶• ì¤‘... (ë¬¸ì„œ ìˆ˜: {len(self.documents)})")
        
        # Vector store êµ¬ì¶•
        embeddings = OllamaEmbeddings(model=self.embedding_model_name)
        self.vector_store = FAISS.from_documents(self.documents, embeddings)
        
        # BM25 ê²€ìƒ‰ê¸° êµ¬ì¶•
        self.bm25_retriever = BM25Retriever.from_documents(
            self.documents,
            bm25_params={"k1": 1.5, "b": 0.75}
        )
        self.bm25_retriever.k = 20
        
        # Vector ê²€ìƒ‰ê¸°
        vector_retriever = self.vector_store.as_retriever(
            search_type="mmr",
            search_kwargs={"k": 20, "fetch_k": 50, "lambda_mult": 0.7}
        )
        
        # ì•™ìƒë¸” ê²€ìƒ‰ê¸°
        self.ensemble_retriever = EnsembleRetriever(
            retrievers=[self.bm25_retriever, vector_retriever],
            weights=[0.5, 0.5]
        )
        
        print("âœ… ê³„ì¸µì  ê²€ìƒ‰ê¸° êµ¬ì¶• ì™„ë£Œ")
    
    def hierarchical_search(self, query: str, max_results: int = 10) -> List[Document]:
        """ê³„ì¸µì  ê²€ìƒ‰"""
        if not self.ensemble_retriever:
            raise ValueError("ê²€ìƒ‰ê¸°ê°€ êµ¬ì¶•ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        # 1. ê¸°ë³¸ ê²€ìƒ‰
        raw_results = self.ensemble_retriever.invoke(query)
        
        # 2. ê³„ì¸µì  ì ìˆ˜ ê³„ì‚°
        scored_results = self._calculate_hierarchical_scores(query, raw_results)
        
        # 3. ì ìˆ˜ìˆœ ì •ë ¬
        scored_results.sort(key=lambda x: x[0], reverse=True)
        
        return [doc for score, doc in scored_results[:max_results]]
    
    def _calculate_hierarchical_scores(self, query: str, documents: List[Document]) -> List[Tuple[float, Document]]:
        """ê³„ì¸µì  ì ìˆ˜ ê³„ì‚°"""
        scored_docs = []
        query_keywords = self._extract_query_keywords(query)
        
        for doc in documents:
            score = 0.0
            content_lower = doc.page_content.lower()
            
            # 1. ê¸°ë³¸ í‚¤ì›Œë“œ ë§¤ì¹­
            for keyword in query_keywords:
                if keyword.lower() in content_lower:
                    score += 10
            
            # 2. ê³„ì¸µì  ì œëª© ë§¤ì¹­ ë³´ë„ˆìŠ¤
            title_hierarchy = doc.metadata.get('title_hierarchy', '')
            for keyword in query_keywords:
                if keyword.lower() in title_hierarchy.lower():
                    score += 15  # ì œëª©ì— ìˆìœ¼ë©´ ë” ë†’ì€ ì ìˆ˜
            
            # 3. PNS ìƒì†/í¬í•¨ ë³´ë„ˆìŠ¤
            if doc.metadata.get('contains_pns', False) or doc.metadata.get('pns_inheritance', False):
                if any(kw.lower() in ['pns', 'payment', 'notification'] for kw in query_keywords):
                    score += 20
            
            # 4. ì»¨í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ ë§¤ì¹­
            context_keywords = doc.metadata.get('context_keywords', [])
            for keyword in query_keywords:
                if keyword in context_keywords:
                    score += 8
            
            # 5. ë ˆë²¨ë³„ ê°€ì¤‘ì¹˜
            hierarchy_level = doc.metadata.get('hierarchy_level', 'minor')
            level_weights = {'major': 1.2, 'medium': 1.1, 'minor': 1.0}
            score *= level_weights.get(hierarchy_level, 1.0)
            
            doc.metadata['hierarchical_score'] = score
            scored_docs.append((score, doc))
        
        return scored_docs
    
    def _extract_query_keywords(self, query: str) -> List[str]:
        """ì¿¼ë¦¬ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        tech_patterns = [
            r'\b[A-Z]{2,}\b',
            r'\b[a-z]+[A-Z][a-zA-Z]*\b',
        ]
        
        keywords = []
        for pattern in tech_patterns:
            keywords.extend(re.findall(pattern, query))
        
        korean_keywords = ['ë©”ì‹œì§€', 'ê·œê²©', 'ê°’', 'êµ¬ì„±', 'ìƒíƒœ', 'ê²°ì œ', 'ì„œë²„']
        for keyword in korean_keywords:
            if keyword in query:
                keywords.append(keyword)
        
        return list(set(keywords))


class HierarchicalExperimentRunner:
    """ê³„ì¸µì  ì‹¤í—˜ ì‹¤í–‰ê¸°"""
    
    def __init__(self, document_path: str):
        self.document_path = document_path
        self.strategies: Dict[str, Union[HierarchicalTitleStrategy, MultiLevelSplittingStrategy, ContextInheritanceStrategy]] = {
            'hierarchical_title': HierarchicalTitleStrategy(document_path),
            'multi_level': MultiLevelSplittingStrategy(document_path),
            'context_inheritance': ContextInheritanceStrategy(document_path)
        }
        self.test_queries = [
            "PNS ë©”ì‹œì§€ ì„œë²„ ê·œê²©ì˜ purchaseStateëŠ” ì–´ë–¤ ê°’ìœ¼ë¡œ êµ¬ì„±ë˜ë‚˜ìš”?",
            "Payment Notification Serviceì—ì„œ purchaseState ê°’ì€ ë¬´ì—‡ì¸ê°€ìš”?",
            "PNSì—ì„œ ì‚¬ìš©ë˜ëŠ” purchaseStateì˜ COMPLETEDì™€ CANCELED ì˜ë¯¸ëŠ”?",
            "ê²°ì œ ì•Œë¦¼ ì„œë¹„ìŠ¤ì˜ purchaseState í•„ë“œ ì„¤ëª…í•´ì£¼ì„¸ìš”",
            "ì›ìŠ¤í† ì–´ PNS ë©”ì‹œì§€ ê·œê²©ì—ì„œ êµ¬ë§¤ ìƒíƒœ ì •ë³´ëŠ”?"
        ]
    
    def run_experiments(self) -> Dict[str, List[HierarchicalSearchResult]]:
        """ê³„ì¸µì  ì‹¤í—˜ ì‹¤í–‰"""
        results = {}
        
        for strategy_name, splitter in self.strategies.items():
            print(f"\nğŸ§ª ê³„ì¸µì  ì‹¤í—˜ ì‹œì‘: {strategy_name}")
            strategy_results = []
            
            # ë¬¸ì„œ ë¶„í• 
            documents = splitter.split_documents()
            print(f"ğŸ“„ ë¶„í• ëœ ë¬¸ì„œ ìˆ˜: {len(documents)}")
            
            # PNS ê´€ë ¨ ë¬¸ì„œ ìˆ˜ í™•ì¸
            pns_docs = [doc for doc in documents 
                       if doc.metadata.get('contains_pns', False) or 
                          doc.metadata.get('pns_inheritance', False) or
                          'PNS' in doc.page_content.upper()]
            print(f"ğŸ¯ PNS ê´€ë ¨ ë¬¸ì„œ ìˆ˜: {len(pns_docs)}")
            
            # ê²€ìƒ‰ê¸° êµ¬ì¶•
            retriever = HierarchicalSmartRetriever(documents)
            retriever.build_retrievers()
            
            # ê° ì¿¼ë¦¬ì— ëŒ€í•´ í…ŒìŠ¤íŠ¸
            for query in self.test_queries:
                print(f"ğŸ” ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸: {query[:40]}...")
                
                search_results = retriever.hierarchical_search(query, max_results=10)
                analysis = self._analyze_hierarchical_results(query, search_results, strategy_name)
                strategy_results.append(analysis)
            
            results[strategy_name] = strategy_results
            print(f"âœ… {strategy_name} ì „ëµ ì‹¤í—˜ ì™„ë£Œ")
        
        return results
    
    def _analyze_hierarchical_results(self, query: str, documents: List[Document], strategy_name: str) -> HierarchicalSearchResult:
        """ê³„ì¸µì  ê²°ê³¼ ë¶„ì„"""
        query_keywords = self._extract_keywords(query)
        hierarchy_scores = []
        context_scores = []
        relevant_count = 0
        pns_related_count = 0
        
        for doc in documents:
            # ê³„ì¸µì  ì ìˆ˜
            hierarchy_score = doc.metadata.get('hierarchical_score', 0)
            hierarchy_scores.append(hierarchy_score)
            
            # ì»¨í…ìŠ¤íŠ¸ ì ìˆ˜
            title_hierarchy = doc.metadata.get('title_hierarchy', '')
            context_matches = sum(1 for kw in query_keywords 
                                if kw.lower() in title_hierarchy.lower())
            context_score = context_matches / len(query_keywords) if query_keywords else 0
            context_scores.append(context_score)
            
            # ê´€ë ¨ì„± ì²´í¬
            content_lower = doc.page_content.lower()
            content_matches = sum(1 for kw in query_keywords if kw.lower() in content_lower)
            total_matches = content_matches + context_matches
            
            if total_matches >= len(query_keywords) * 0.5:  # 50% ì´ìƒ ë§¤ì¹­
                relevant_count += 1
            
            # PNS ê´€ë ¨ì„± ì²´í¬
            if (doc.metadata.get('contains_pns', False) or 
                doc.metadata.get('pns_inheritance', False) or
                'PNS' in content_lower.upper()):
                pns_related_count += 1
        
        return HierarchicalSearchResult(
            strategy_name=strategy_name,
            query=query,
            documents=documents,
            hierarchy_scores=hierarchy_scores,
            context_scores=context_scores,
            total_docs=len(documents),
            relevant_docs=relevant_count,
            pns_related_docs=pns_related_count
        )
    
    def _extract_keywords(self, query: str) -> List[str]:
        """í‚¤ì›Œë“œ ì¶”ì¶œ"""
        tech_patterns = [r'\b[A-Z]{2,}\b', r'\b[a-z]+[A-Z][a-zA-Z]*\b']
        keywords = []
        
        for pattern in tech_patterns:
            keywords.extend(re.findall(pattern, query))
        
        korean_words = ['ë©”ì‹œì§€', 'ê·œê²©', 'ê°’', 'êµ¬ì„±', 'ìƒíƒœ', 'ê²°ì œ', 'ì„œë²„', 'ì•Œë¦¼']
        for word in korean_words:
            if word in query:
                keywords.append(word)
        
        return list(set(keywords))
    
    def print_hierarchical_results(self, results: Dict[str, List[HierarchicalSearchResult]]):
        """ê³„ì¸µì  ê²°ê³¼ ì¶œë ¥"""
        print("\n" + "="*80)
        print("ğŸ† ê³„ì¸µì  RAG ê²€ìƒ‰ ìµœì í™” ì‹¤í—˜ ê²°ê³¼")
        print("="*80)
        
        for strategy_name, strategy_results in results.items():
            print(f"\nğŸ“Š ì „ëµ: {strategy_name.upper()}")
            print("-" * 60)
            
            total_hierarchy_score = sum(sum(r.hierarchy_scores[:3]) for r in strategy_results)
            total_context_score = sum(sum(r.context_scores[:3]) for r in strategy_results)
            total_relevant = sum(r.relevant_docs for r in strategy_results)
            total_pns_related = sum(r.pns_related_docs for r in strategy_results)
            
            avg_hierarchy = total_hierarchy_score / (len(strategy_results) * 3)
            avg_context = total_context_score / (len(strategy_results) * 3)
            
            print(f"í‰ê·  ê³„ì¸µ ì ìˆ˜ (ìƒìœ„3): {avg_hierarchy:.2f}")
            print(f"í‰ê·  ì»¨í…ìŠ¤íŠ¸ ì ìˆ˜ (ìƒìœ„3): {avg_context:.3f}")
            print(f"ì „ì²´ ê´€ë ¨ ë¬¸ì„œ ìˆ˜: {total_relevant}")
            print(f"PNS ê´€ë ¨ ë¬¸ì„œ ìˆ˜: {total_pns_related}")
            
            # í•µì‹¬ ì¿¼ë¦¬ ê²°ê³¼
            pns_query_result = strategy_results[0]  # ì²« ë²ˆì§¸ ì¿¼ë¦¬ê°€ í•µì‹¬ ì¿¼ë¦¬
            print(f"\nğŸ¯ í•µì‹¬ ì¿¼ë¦¬ ê²°ê³¼ (PNS + purchaseState):")
            print(f"  ê´€ë ¨ ë¬¸ì„œ: {pns_query_result.relevant_docs}/10")
            print(f"  PNS ì—°ê²°: {pns_query_result.pns_related_docs}/10")
            print(f"  í‰ê·  ê³„ì¸µ ì ìˆ˜: {sum(pns_query_result.hierarchy_scores[:3])/3:.2f}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ ê³„ì¸µì  RAG ê²€ìƒ‰ ìµœì í™” ì‹¤í—˜ ì‹œì‘")
    print("ğŸ’¡ ëª©í‘œ: PNS ëŒ€ì œëª©ê³¼ í•˜ìœ„ ì„¹ì…˜ purchaseState ì—°ê²° ë¬¸ì œ í•´ê²°")
    
    document_path = "../data/dev_center_guide_allmd_touched.md"
    
    if not os.path.exists(document_path):
        print(f"âŒ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {document_path}")
        return
    
    # ì‹¤í—˜ ì‹¤í–‰
    runner = HierarchicalExperimentRunner(document_path)
    results = runner.run_experiments()
    
    # ê²°ê³¼ ì¶œë ¥
    runner.print_hierarchical_results(results)
    
    # ê²°ê³¼ ì €ì¥
    os.makedirs("results", exist_ok=True)
    output_path = "results/hierarchical_experiment_results.pkl"
    with open(output_path, "wb") as f:
        pickle.dump(results, f)
    print(f"\nğŸ’¾ ê³„ì¸µì  ì‹¤í—˜ ê²°ê³¼ ì €ì¥ë¨: {output_path}")
    
    # ìµœì  ì „ëµ ì¶”ì²œ
    print(f"\nğŸ¯ ê¶Œì¥ì‚¬í•­:")
    print(f"1. context_inheritance ì „ëµì´ PNS-purchaseState ì—°ê²°ì— ê°€ì¥ íš¨ê³¼ì ì¼ ê²ƒìœ¼ë¡œ ì˜ˆìƒ")
    print(f"2. multi_level ì „ëµìœ¼ë¡œ ë‹¤ì–‘í•œ í¬ê¸°ì˜ ì»¨í…ìŠ¤íŠ¸ ì œê³µ")
    print(f"3. hierarchical_title ì „ëµìœ¼ë¡œ ëª…ì‹œì  ì œëª© ê²½ë¡œ í¬í•¨")


if __name__ == "__main__":
    main()
