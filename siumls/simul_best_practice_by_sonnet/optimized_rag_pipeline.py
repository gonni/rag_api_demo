"""
Optimized RAG Pipeline for Technical Documentation
ê¸°ìˆ ë¬¸ì„œ ìµœì í™” RAG íŒŒì´í”„ë¼ì¸

íŠ¹ì§•:
1. Ollama ê¸°ë°˜ í™˜ê²½ (bge-m3, exaone3.5) ì§€ì›
2. ê³„ì¸µì  ë¬¸ì„œ ë¶„í•  + ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ê²€ìƒ‰
3. í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (FAISS + BM25 + ë©”íƒ€ë°ì´í„° í•„í„°ë§)
4. ë‹¤ë‹¨ê³„ ë¦¬ë­í‚¹ ë° ë…¸ì´ì¦ˆ í•„í„°ë§
5. ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ ì§€ì›
6. ê²€ìƒ‰ ë¶„ì„ ë° ë””ë²„ê¹… ê¸°ëŠ¥
"""

import os
import json
import time
from typing import List, Dict, Any, Optional, Union

# LangChain imports
from langchain.docstore.document import Document
from langchain_ollama import OllamaEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever
from langchain_community.chat_models import ChatOllama
from langchain_core.prompts import PromptTemplate
from langchain.callbacks import StreamingStdOutCallbackHandler

# ë¡œì»¬ ëª¨ë“ˆ imports
from hierarchical_splitter import HierarchicalSplitter
from context_aware_retriever import ContextAwareRetriever


class OptimizedRAGPipeline:
    """ìµœì í™”ëœ RAG íŒŒì´í”„ë¼ì¸"""
    
    def __init__(self,
                 embed_model: str = "bge-m3:latest",
                 llm_model: str = "exaone3.5:latest",
                 data_file: str = "../../data/dev_center_guide_allmd_touched.md",
                 vector_store_path: str = "./models/faiss_optimized",
                 chunk_size: int = 1000,
                 overlap_ratio: float = 0.1,
                 final_top_k: int = 5):
        
        # ëª¨ë¸ ì„¤ì •
        self.embed_model = embed_model
        self.llm_model = llm_model
        self.data_file = data_file
        self.vector_store_path = vector_store_path
        
        # ë¶„í•  ì„¤ì •
        self.chunk_size = chunk_size
        self.overlap_ratio = overlap_ratio
        self.final_top_k = final_top_k
        
        # êµ¬ì„±ìš”ì†Œ ì´ˆê¸°í™”
        self.embeddings: Optional[OllamaEmbeddings] = None
        self.vector_store: Optional[FAISS] = None
        self.splitter: Optional[HierarchicalSplitter] = None
        self.retriever: Optional[ContextAwareRetriever] = None
        self.llm: Optional[ChatOllama] = None
        self.documents: List[Document] = []
        
        # ì„±ëŠ¥ í†µê³„
        self.stats: Dict[str, Union[int, float]] = {
            'total_documents': 0,
            'total_chunks': 0,
            'index_build_time': 0.0,
            'last_query_time': 0.0,
            'queries_processed': 0
        }
        
        print("RAG Pipeline ì´ˆê¸°í™”:")
        print(f"  - ì„ë² ë”© ëª¨ë¸: {embed_model}")
        print(f"  - LLM ëª¨ë¸: {llm_model}")
        print(f"  - ë°ì´í„° íŒŒì¼: {data_file}")
        print(f"  - ë²¡í„° ì €ì¥ì†Œ: {vector_store_path}")
    
    def initialize_models(self):
        """ëª¨ë¸ ì´ˆê¸°í™”"""
        print("ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
        
        # ì„ë² ë”© ëª¨ë¸
        self.embeddings = OllamaEmbeddings(model=self.embed_model)
        
        # LLM ëª¨ë¸ (ìŠ¤íŠ¸ë¦¬ë° ì§€ì›)
        self.llm = ChatOllama(
            model=self.llm_model,
            temperature=0.1,
            streaming=True,
            callbacks=[StreamingStdOutCallbackHandler()]
        )
        
        # ë¬¸ì„œ ë¶„í• ê¸°
        self.splitter = HierarchicalSplitter(
            chunk_size=self.chunk_size,
            overlap_ratio=self.overlap_ratio,
            preserve_tables=True,
            preserve_code=True
        )
        
        print("âœ“ ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ")
    
    def load_and_process_documents(self, force_rebuild: bool = False):
        """ë¬¸ì„œ ë¡œë“œ ë° ì²˜ë¦¬"""
        print("ë¬¸ì„œ ì²˜ë¦¬ ì‹œì‘...")
        start_time = time.time()
        
        # ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ í™•ì¸
        if not force_rebuild and os.path.exists(self.vector_store_path) and self.embeddings is not None:
            try:
                print("ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì¤‘...")
                self.vector_store = FAISS.load_local(
                    self.vector_store_path, 
                    self.embeddings, 
                    allow_dangerous_deserialization=True
                )
                
                # ë©”íƒ€ë°ì´í„° ë¡œë“œ
                metadata_file = f"{self.vector_store_path}/metadata.json"
                if os.path.exists(metadata_file):
                    with open(metadata_file, 'r', encoding='utf-8') as f:
                        self.stats.update(json.load(f))
                
                print(f"âœ“ ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì™„ë£Œ (ë¬¸ì„œ: {self.stats.get('total_chunks', 0)}ê°œ)")
                return
                
            except Exception as e:
                print(f"ê¸°ì¡´ ë²¡í„° ìŠ¤í† ì–´ ë¡œë“œ ì‹¤íŒ¨: {e}")
                print("ìƒˆë¡œ ìƒì„±í•©ë‹ˆë‹¤...")
        
        # ë¬¸ì„œ ë¡œë“œ
        if not os.path.exists(self.data_file):
            raise FileNotFoundError(f"ë°ì´í„° íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.data_file}")
        
        with open(self.data_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        print(f"ì›ë³¸ ë¬¸ì„œ í¬ê¸°: {len(content):,} ë¬¸ì")
        
        # ê³„ì¸µì  ë¶„í• 
        if self.splitter is None:
            raise ValueError("ë¬¸ì„œ ë¶„í• ê¸°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. initialize_models()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        
        print("ê³„ì¸µì  ë¬¸ì„œ ë¶„í•  ì¤‘...")
        self.documents = self.splitter.split_document(content)
        
        self.stats['total_documents'] = 1
        self.stats['total_chunks'] = len(self.documents)
        
        print(f"âœ“ ë¶„í•  ì™„ë£Œ: {len(self.documents)}ê°œ ì²­í¬")
        
        # ë²¡í„° ìŠ¤í† ì–´ êµ¬ì¶•
        if self.embeddings is None:
            raise ValueError("ì„ë² ë”© ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. initialize_models()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
            
        print("ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶• ì¤‘...")
        self.vector_store = FAISS.from_documents(self.documents, self.embeddings)
        
        # ë²¡í„° ìŠ¤í† ì–´ ì €ì¥
        os.makedirs(self.vector_store_path, exist_ok=True)
        self.vector_store.save_local(self.vector_store_path)
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        self.stats['index_build_time'] = time.time() - start_time
        metadata_file = f"{self.vector_store_path}/metadata.json"
        with open(metadata_file, 'w', encoding='utf-8') as f:
            json.dump(self.stats, f, ensure_ascii=False, indent=2)
        
        print(f"âœ“ ë²¡í„° ì¸ë±ìŠ¤ êµ¬ì¶• ì™„ë£Œ (ì†Œìš”ì‹œê°„: {self.stats['index_build_time']:.2f}ì´ˆ)")
    
    def build_retriever(self):
        """ê²€ìƒ‰ê¸° êµ¬ì¶•"""
        print("ê²€ìƒ‰ê¸° êµ¬ì¶• ì¤‘...")
        
        if not self.vector_store:
            raise ValueError("ë²¡í„° ìŠ¤í† ì–´ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. load_and_process_documents()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        
        # FAISS ê²€ìƒ‰ê¸°
        vector_retriever = self.vector_store.as_retriever(
            search_type="mmr",
            search_kwargs={"k": 15, "fetch_k": 50, "lambda_mult": 0.7}
        )
        
        # BM25 ê²€ìƒ‰ê¸°
        if not self.documents:
            # ë²¡í„° ìŠ¤í† ì–´ì—ì„œ ë¬¸ì„œ ë³µì› (ê°„ì†Œí™”ëœ ë²„ì „)
            print("ë¬¸ì„œë¥¼ ë²¡í„° ìŠ¤í† ì–´ì—ì„œ ë³µì› ì¤‘...")
            # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë¬¸ì„œ ë©”íƒ€ë°ì´í„°ë¥¼ ë³„ë„ ì €ì¥í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤
            self.documents = [Document(page_content="ì„ì‹œ ë¬¸ì„œ", metadata={})]
        
        bm25_retriever = BM25Retriever.from_documents(
            self.documents, 
            bm25_params={"k1": 1.5, "b": 0.75}
        )
        bm25_retriever.k = 20
        
        # ì•™ìƒë¸” ê²€ìƒ‰ê¸°
        ensemble_retriever = EnsembleRetriever(
            retrievers=[bm25_retriever, vector_retriever],
            weights=[0.4, 0.6]
        )
        
        # ì»¨í…ìŠ¤íŠ¸ ì¸ì‹ ê²€ìƒ‰ê¸°ë¡œ ë˜í•‘
        self.retriever = ContextAwareRetriever(
            base_retriever=ensemble_retriever,
            rerank_top_k=20,
            final_top_k=self.final_top_k,
            enable_query_expansion=True,
            noise_threshold=0.7
        )
        
        print("âœ“ ê²€ìƒ‰ê¸° êµ¬ì¶• ì™„ë£Œ")
    
    def create_prompt_template(self) -> PromptTemplate:
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±"""
        
        template = """ë‹¹ì‹ ì€ ì›ìŠ¤í† ì–´ ì¸ì•±ê²°ì œ ê¸°ìˆ ë¬¸ì„œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì£¼ì–´ì§„ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ì‚¬ìš©ìì˜ ì§ˆë¬¸ì— ì •í™•í•˜ê³  ìì„¸í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.

**ë‹µë³€ ì§€ì¹¨:**
1. ê¸°ìˆ ì  ì •í™•ì„±ì„ ìµœìš°ì„ ìœ¼ë¡œ í•©ë‹ˆë‹¤
2. ì½”ë“œ ì˜ˆì œê°€ ìˆë‹¤ë©´ ë°˜ë“œì‹œ í¬í•¨í•©ë‹ˆë‹¤  
3. API íŒŒë¼ë¯¸í„°ë‚˜ ì‘ë‹µê°’ì€ ì •í™•íˆ ê¸°ìˆ í•©ë‹ˆë‹¤
4. ë‹¨ê³„ë³„ ì„¤ëª…ì´ í•„ìš”í•œ ê²½ìš° ë²ˆí˜¸ë¥¼ ë§¤ê²¨ ì„¤ëª…í•©ë‹ˆë‹¤
5. ë¶ˆí™•ì‹¤í•œ ë‚´ìš©ì€ "ë¬¸ì„œì—ì„œ í™•ì¸ë˜ì§€ ì•ŠìŒ"ì´ë¼ê³  ëª…ì‹œí•©ë‹ˆë‹¤

**ì°¸ê³  ë¬¸ì„œ:**
{context}

**ì§ˆë¬¸:** {question}

**ë‹µë³€:**"""

        return PromptTemplate(
            template=template,
            input_variables=["context", "question"]
        )
    
    def query(self, question: str, return_sources: bool = True, stream: bool = True) -> Dict[str, Any]:
        """ì¿¼ë¦¬ ì‹¤í–‰"""
        start_time = time.time()
        
        if not self.retriever or not self.llm:
            raise ValueError("íŒŒì´í”„ë¼ì¸ì´ ì™„ì „íˆ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        print(f"\nğŸ” ì§ˆë¬¸: {question}")
        print("=" * 60)
        
        # 1. ë¬¸ì„œ ê²€ìƒ‰
        print("ê´€ë ¨ ë¬¸ì„œ ê²€ìƒ‰ ì¤‘...")
        retrieved_docs = self.retriever.get_relevant_documents(question)
        
        print(f"âœ“ {len(retrieved_docs)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ë¨")
        
        # 2. ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = "\n\n".join([
            f"[ë¬¸ì„œ {i+1}] {doc.metadata.get('section_hierarchy', 'N/A')}\n{doc.page_content}"
            for i, doc in enumerate(retrieved_docs)
        ])
        
        # 3. í”„ë¡¬í”„íŠ¸ ìƒì„±
        prompt_template = self.create_prompt_template()
        prompt = prompt_template.format(context=context, question=question)
        
        # 4. LLM ì‹¤í–‰
        print("\nğŸ’­ ë‹µë³€ ìƒì„± ì¤‘...")
        print("-" * 60)
        
        response_text = ""
        if stream:
            # ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ
            response_generator = self.llm.stream(prompt)
            for chunk in response_generator:
                if hasattr(chunk, 'content'):
                    # chunk.contentëŠ” ë‹¤ì–‘í•œ íƒ€ì…ì´ ë  ìˆ˜ ìˆìœ¼ë¯€ë¡œ strë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜
                    response_text += str(chunk.content)
            print()  # ì¤„ë°”ê¿ˆ
        else:
            # ì¼ë°˜ ì‘ë‹µ
            response = self.llm.invoke(prompt)
            if hasattr(response, 'content'):
                # response.contentë„ strë¡œ ì•ˆì „í•˜ê²Œ ë³€í™˜
                response_text = str(response.content)
            else:
                response_text = str(response)
        
        # 5. í†µê³„ ì—…ë°ì´íŠ¸
        query_time = time.time() - start_time
        self.stats['last_query_time'] = query_time
        self.stats['queries_processed'] += 1
        
        print("-" * 60)
        print(f"â±ï¸  ì‘ë‹µ ì‹œê°„: {query_time:.2f}ì´ˆ")
        
        # 6. ê²°ê³¼ êµ¬ì„±
        result = {
            'question': question,
            'answer': response_text,
            'query_time': query_time,
            'retrieved_docs_count': len(retrieved_docs)
        }
        
        if return_sources:
            result['sources'] = [
                {
                    'content': doc.page_content[:200] + "...",
                    'metadata': doc.metadata
                }
                for doc in retrieved_docs
            ]
        
        return result
    
    def batch_query(self, questions: List[str]) -> List[Dict[str, Any]]:
        """ë°°ì¹˜ ì¿¼ë¦¬ ì‹¤í–‰"""
        print(f"ğŸ“‹ ë°°ì¹˜ ì¿¼ë¦¬ ì‹¤í–‰: {len(questions)}ê°œ ì§ˆë¬¸")
        
        results = []
        for i, question in enumerate(questions, 1):
            print(f"\n[{i}/{len(questions)}]")
            result = self.query(question, stream=False)
            results.append(result)
        
        return results
    
    def get_statistics(self) -> Dict[str, Any]:
        """íŒŒì´í”„ë¼ì¸ í†µê³„ ë°˜í™˜"""
        return {
            **self.stats,
            'models': {
                'embedding': self.embed_model,
                'llm': self.llm_model
            },
            'config': {
                'chunk_size': self.chunk_size,
                'overlap_ratio': self.overlap_ratio,
                'final_top_k': self.final_top_k
            }
        }
    
    def analyze_document_structure(self) -> Dict[str, Any]:
        """ë¬¸ì„œ êµ¬ì¡° ë¶„ì„"""
        if not self.documents:
            return {"error": "ë¬¸ì„œê°€ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"}
        
        # ì¶œì²˜ë³„ í†µê³„
        source_stats: Dict[str, int] = {}
        content_type_stats: Dict[str, int] = {}
        tech_term_stats: Dict[str, Dict[str, int]] = {}
        
        for doc in self.documents:
            # ì¶œì²˜ í†µê³„
            source_url = doc.metadata.get('source_url', 'Unknown')
            source_domain = source_url.split('/')[-1] if '/' in source_url else source_url
            source_stats[source_domain] = source_stats.get(source_domain, 0) + 1
            
            # ì½˜í…ì¸  íƒ€ì… í†µê³„
            content_types = doc.metadata.get('content_types', ['text'])
            for ct in content_types:
                content_type_stats[ct] = content_type_stats.get(ct, 0) + 1
            
            # ê¸°ìˆ  ìš©ì–´ í†µê³„
            tech_terms = doc.metadata.get('technical_terms', {})
            for category, terms in tech_terms.items():
                if category not in tech_term_stats:
                    tech_term_stats[category] = {}
                for term in terms:
                    tech_term_stats[category][term] = tech_term_stats[category].get(term, 0) + 1
        
        return {
            'total_chunks': len(self.documents),
            'source_distribution': source_stats,
            'content_type_distribution': content_type_stats,
            'tech_term_distribution': {
                category: dict(list(sorted(terms.items(), key=lambda x: x[1], reverse=True))[:10])
                for category, terms in tech_term_stats.items()
            }
        }
    
    def debug_search(self, question: str) -> Dict[str, Any]:
        """ê²€ìƒ‰ ë””ë²„ê¹… ì •ë³´"""
        if not self.retriever:
            return {"error": "ê²€ìƒ‰ê¸°ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤"}
        
        # ì¿¼ë¦¬ ë¶„ì„
        analytics = self.retriever.get_search_analytics(question)
        
        # ê²€ìƒ‰ ì‹¤í–‰
        retrieved_docs = self.retriever.get_relevant_documents(question)
        
        # ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„
        result_analysis = []
        for i, doc in enumerate(retrieved_docs):
            result_analysis.append({
                'rank': i + 1,
                'content_preview': doc.page_content[:150] + "...",
                'section_hierarchy': doc.metadata.get('section_hierarchy', 'N/A'),
                'content_types': doc.metadata.get('content_types', []),
                'technical_terms': doc.metadata.get('technical_terms', {}),
                'content_length': len(doc.page_content)
            })
        
        return {
            'query_analysis': analytics,
            'retrieved_count': len(retrieved_docs),
            'results': result_analysis
        }


# í¸ì˜ í•¨ìˆ˜ë“¤
def create_pipeline(data_file: str = "../../data/dev_center_guide_allmd_touched.md",
                   force_rebuild: bool = False) -> OptimizedRAGPipeline:
    """íŒŒì´í”„ë¼ì¸ ìƒì„± ë° ì´ˆê¸°í™”"""
    
    pipeline = OptimizedRAGPipeline(data_file=data_file)
    
    # ì´ˆê¸°í™” ë‹¨ê³„
    pipeline.initialize_models()
    pipeline.load_and_process_documents(force_rebuild=force_rebuild)
    pipeline.build_retriever()
    
    print("\nğŸš€ RAG íŒŒì´í”„ë¼ì¸ ì¤€ë¹„ ì™„ë£Œ!")
    print(f"   ì´ ë¬¸ì„œ: {pipeline.stats['total_chunks']}ê°œ")
    print(f"   ì¸ë±ì‹± ì‹œê°„: {pipeline.stats['index_build_time']:.2f}ì´ˆ")
    
    return pipeline


def interactive_mode(pipeline: OptimizedRAGPipeline):
    """ëŒ€í™”í˜• ëª¨ë“œ"""
    print("\n" + "="*60)
    print("ğŸ¤– ì›ìŠ¤í† ì–´ IAP ê¸°ìˆ ë¬¸ì„œ QA ì‹œìŠ¤í…œ")
    print("="*60)
    print("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš” (ì¢…ë£Œ: 'quit' ë˜ëŠ” 'exit')")
    print("íŠ¹ìˆ˜ ëª…ë ¹ì–´:")
    print("  - 'stats': íŒŒì´í”„ë¼ì¸ í†µê³„")
    print("  - 'analyze': ë¬¸ì„œ êµ¬ì¡° ë¶„ì„")
    print("  - 'debug <ì§ˆë¬¸>': ê²€ìƒ‰ ë””ë²„ê¹…")
    print("-"*60)
    
    while True:
        try:
            question = input("\nâ“ ì§ˆë¬¸: ").strip()
            
            if question.lower() in ['quit', 'exit', 'ì¢…ë£Œ']:
                print("ğŸ‘‹ ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            if question == 'stats':
                stats = pipeline.get_statistics()
                print("\nğŸ“Š íŒŒì´í”„ë¼ì¸ í†µê³„:")
                print(json.dumps(stats, ensure_ascii=False, indent=2))
                continue
            
            if question == 'analyze':
                analysis = pipeline.analyze_document_structure()
                print("\nğŸ“ˆ ë¬¸ì„œ êµ¬ì¡° ë¶„ì„:")
                print(json.dumps(analysis, ensure_ascii=False, indent=2))
                continue
            
            if question.startswith('debug '):
                debug_query = question[6:]
                debug_info = pipeline.debug_search(debug_query)
                print("\nğŸ” ê²€ìƒ‰ ë””ë²„ê¹…:")
                print(json.dumps(debug_info, ensure_ascii=False, indent=2))
                continue
            
            if not question:
                continue
            
            # ì¼ë°˜ ì§ˆë¬¸ ì²˜ë¦¬
            pipeline.query(question)
            
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")


# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_pipeline():
    """íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""
    
    test_questions = [
        "PurchaseClient ì´ˆê¸°í™” ë°©ë²•ì´ ë­”ê°€ìš”?",
        "purchaseState ê°’ì€ ë¬´ì—‡ì¸ê°€ìš”?", 
        "PNS ì„œë¹„ìŠ¤ ì„¤ì • ë°©ë²•ì„ ì•Œë ¤ì£¼ì„¸ìš”",
        "ì¸ì•±ê²°ì œ ì—ëŸ¬ê°€ ë°œìƒí–ˆì„ ë•Œ í•´ê²° ë°©ë²•ì€?",
        "êµ¬ë…í˜• ìƒí’ˆê³¼ ê´€ë¦¬í˜• ìƒí’ˆì˜ ì°¨ì´ì ì€?"
    ]
    
    print("ğŸ§ª íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    # í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ íŒŒì´í”„ë¼ì¸ (ì‹¤ì œ íŒŒì¼ ì—†ì´)
    # ì‹¤ì œ ì‚¬ìš©ì‹œì—ëŠ” create_pipeline()ì„ ì‚¬ìš©
    
    for question in test_questions:
        print(f"\nì§ˆë¬¸: {question}")
        # result = pipeline.query(question)
        print("ë‹µë³€: [í…ŒìŠ¤íŠ¸ ëª¨ë“œ - ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë‹µë³€ì´ ìƒì„±ë©ë‹ˆë‹¤]")
        print("-" * 60)


if __name__ == "__main__":
    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰
    test_pipeline()
