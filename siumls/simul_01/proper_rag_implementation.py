#!/usr/bin/env python3
"""
ì˜¬ë°”ë¥¸ RAG êµ¬í˜„: ê²€ìƒ‰ìš© + ìƒì„±ìš© ëª¨ë¸ ë¶„ë¦¬
"""

import os
import re
import torch
from typing import List, Dict, Any
from pathlib import Path
from langchain.docstore.document import Document
from langchain_ollama import OllamaEmbeddings, Ollama
from langchain_community.vectorstores import FAISS
from langchain.schema import BaseRetriever
from langchain.chains import RetrievalQA
import json
from datetime import datetime

class ProperRAGImplementation:
    def __init__(self, markdown_file_path: str):
        self.markdown_file_path = markdown_file_path
        self.raw_text = self.load_markdown_file(markdown_file_path)
        
    def load_markdown_file(self, file_path: str) -> str:
        """ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        with open(file_path, 'r', encoding='utf-8') as file:
            return file.read()
    
    def extract_headers(self, text: str) -> List[Dict[str, Any]]:
        """ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ì—ì„œ í—¤ë” ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        headers = []
        lines = text.split('\n')
        
        for i, line in enumerate(lines):
            header_match = re.match(r'^(#{1,6})\s+(.+)$', line.strip())
            if header_match:
                level = len(header_match.group(1))
                title = header_match.group(2).strip()
                headers.append({
                    'level': level,
                    'title': title,
                    'line_number': i,
                    'content_start': i + 1
                })
        
        return headers
    
    def get_content_between_headers(self, text: str, start_header: Dict, end_header: Dict[str, Any] | None = None) -> str:
        """ë‘ í—¤ë” ì‚¬ì´ì˜ ë‚´ìš©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        lines = text.split('\n')
        start_line = start_header['content_start']
        
        if end_header:
            end_line = end_header['line_number']
        else:
            end_line = len(lines)
        
        return '\n'.join(lines[start_line:end_line])
    
    def create_documents(self) -> List[Document]:
        """ë¬¸ì„œë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        headers = self.extract_headers(self.raw_text)
        docs = []
        
        for i, header in enumerate(headers):
            content = self.get_content_between_headers(
                self.raw_text,
                header,
                headers[i + 1] if i + 1 < len(headers) else None
            )
            
            # ì œëª© ê³„ì¸µ êµ¬ì¡° ìƒì„±
            title_hierarchy = self.build_title_hierarchy(headers, i)
            
            doc = Document(
                page_content=f"ì œëª©: {title_hierarchy}\n\në‚´ìš©:\n{content}",
                metadata={
                    'title': header['title'],
                    'title_hierarchy': title_hierarchy,
                    'header_level': header['level'],
                    'source': self.markdown_file_path,
                    'line_number': header['line_number']
                }
            )
            docs.append(doc)
        
        return docs
    
    def build_title_hierarchy(self, headers: List[Dict], current_index: int) -> str:
        """í˜„ì¬ í—¤ë”ê¹Œì§€ì˜ ì œëª© ê³„ì¸µ êµ¬ì¡°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        current_header = headers[current_index]
        hierarchy = [current_header['title']]
        
        for i in range(current_index - 1, -1, -1):
            if headers[i]['level'] < current_header['level']:
                hierarchy.insert(0, headers[i]['title'])
                current_header = headers[i]
        
        return ' / '.join(hierarchy)
    
    def setup_embedding_model(self) -> OllamaEmbeddings:
        """ê²€ìƒ‰ìš© ì„ë² ë”© ëª¨ë¸ì„ ì„¤ì •í•©ë‹ˆë‹¤."""
        print("ğŸ” ê²€ìƒ‰ìš© ì„ë² ë”© ëª¨ë¸ ì„¤ì •")
        
        # ê²€ìƒ‰ì— ìµœì í™”ëœ ëª¨ë¸ë“¤ (ìš°ì„ ìˆœìœ„ ìˆœ)
        embedding_models = [
            "nomic-embed-text",    # ë‹¤êµ­ì–´ ê²€ìƒ‰ íŠ¹í™”
            "all-minilm",          # ê²½ëŸ‰ ê²€ìƒ‰ ëª¨ë¸
            "bge-small-en"         # ì˜ì–´ ê²€ìƒ‰ íŠ¹í™”
        ]
        
        for model_name in embedding_models:
            try:
                print(f"ì„ë² ë”© ëª¨ë¸ ì‹œë„: {model_name}")
                embeddings = OllamaEmbeddings(model=model_name)
                
                # í…ŒìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
                test_text = "PNS purchaseState COMPLETED CANCELED"
                test_embedding = embeddings.embed_query(test_text)
                print(f"âœ… {model_name} ëª¨ë¸ ì„±ê³µ - ì„ë² ë”© ì°¨ì›: {len(test_embedding)}")
                return embeddings
                
            except Exception as e:
                print(f"âŒ {model_name} ëª¨ë¸ ì‹¤íŒ¨: {e}")
                continue
        
        # ëª¨ë“  ëª¨ë¸ì´ ì‹¤íŒ¨í•˜ë©´ ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©
        print("ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©: nomic-embed-text")
        return OllamaEmbeddings(model="nomic-embed-text")
    
    def setup_generation_model(self) -> Ollama:
        """ìƒì„±ìš© ëª¨ë¸ì„ ì„¤ì •í•©ë‹ˆë‹¤."""
        print("ğŸ¤– ìƒì„±ìš© ëª¨ë¸ ì„¤ì •")
        
        # ìƒì„±ì— ìµœì í™”ëœ ëª¨ë¸ë“¤ (ìš°ì„ ìˆœìœ„ ìˆœ)
        generation_models = [
            "qwen2.5:7b",           # ë‹¤êµ­ì–´ ìƒì„± ìš°ìˆ˜
            "llama3.2:3b",          # ê²½ëŸ‰ ìƒì„±
            "mistral:7b",           # ê¸°ìˆ  ë¬¸ì„œ ìƒì„±
            "exaone3.5:latest"      # ì¼ë°˜ ìƒì„±
        ]
        
        for model_name in generation_models:
            try:
                print(f"ìƒì„± ëª¨ë¸ ì‹œë„: {model_name}")
                llm = Ollama(model=model_name)
                
                # í…ŒìŠ¤íŠ¸ ìƒì„±
                test_prompt = "ì•ˆë…•í•˜ì„¸ìš”. ê°„ë‹¨íˆ ë‹µë³€í•´ì£¼ì„¸ìš”."
                response = llm.invoke(test_prompt)
                print(f"âœ… {model_name} ëª¨ë¸ ì„±ê³µ")
                return llm
                
            except Exception as e:
                print(f"âŒ {model_name} ëª¨ë¸ ì‹¤íŒ¨: {e}")
                continue
        
        # ëª¨ë“  ëª¨ë¸ì´ ì‹¤íŒ¨í•˜ë©´ ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©
        print("ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©: exaone3.5:latest")
        return Ollama(model="exaone3.5:latest")
    
    def create_vectorstore(self, docs: List[Document], embeddings: OllamaEmbeddings) -> FAISS:
        """ë²¡í„° ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        print(f"\nğŸ“š ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì¤‘")
        print(f"ë¬¸ì„œ ìˆ˜: {len(docs)}")
        
        vectorstore = FAISS.from_documents(docs, embeddings)
        print("âœ… ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì™„ë£Œ")
        return vectorstore
    
    def test_retrieval(self, vectorstore: FAISS, query: str) -> List[Document]:
        """ê²€ìƒ‰ ì„±ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
        print(f"\nğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸: '{query}'")
        
        retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
        docs = retriever.get_relevant_documents(query)
        
        print(f"ê²€ìƒ‰ëœ ë¬¸ì„œ ìˆ˜: {len(docs)}")
        for i, doc in enumerate(docs):
            title = doc.metadata.get('title', 'Unknown')
            content_preview = doc.page_content[:200] + "..."
            print(f"  {i+1}. {title}")
            print(f"     {content_preview}")
        
        return docs
    
    def test_generation(self, llm: Ollama, query: str, context_docs: List[Document]) -> str:
        """ìƒì„± ì„±ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
        print(f"\nğŸ¤– ìƒì„± í…ŒìŠ¤íŠ¸: '{query}'")
        
        # ì»¨í…ìŠ¤íŠ¸ êµ¬ì„±
        context = "\n\n".join([doc.page_content for doc in context_docs])
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = f"""
ë‹¤ìŒ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.

ë¬¸ì„œ:
{context}

ì§ˆë¬¸: {query}

ë‹µë³€:"""
        
        try:
            response = llm.invoke(prompt)
            print(f"ìƒì„±ëœ ë‹µë³€: {response}")
            return response
        except Exception as e:
            print(f"ìƒì„± ì‹¤íŒ¨: {e}")
            return "ë‹µë³€ ìƒì„±ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤."
    
    def run_proper_rag_experiment(self):
        """ì˜¬ë°”ë¥¸ RAG ì‹¤í—˜ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        print("ğŸš€ ì˜¬ë°”ë¥¸ RAG ì‹¤í—˜ ì‹œì‘")
        
        # 1. ë¬¸ì„œ ìƒì„±
        docs = self.create_documents()
        print(f"ìƒì„±ëœ ë¬¸ì„œ ìˆ˜: {len(docs)}")
        
        # 2. ê²€ìƒ‰ìš© ëª¨ë¸ ì„¤ì •
        embeddings = self.setup_embedding_model()
        
        # 3. ìƒì„±ìš© ëª¨ë¸ ì„¤ì •
        llm = self.setup_generation_model()
        
        # 4. ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        vectorstore = self.create_vectorstore(docs, embeddings)
        
        # 5. í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ë“¤
        test_queries = [
            "PNSì˜ purchaseStateì—ëŠ” ì–´ë–¤ ê°’ë“¤ì´ ìˆë‚˜ìš”?",
            "ì›ìŠ¤í† ì–´ ì¸ì•±ê²°ì œì˜ ê²°ì œ ìƒíƒœëŠ” ë¬´ì—‡ì¸ê°€ìš”?",
            "COMPLETEDì™€ CANCELEDì˜ ì°¨ì´ì ì€ ë¬´ì—‡ì¸ê°€ìš”?"
        ]
        
        results = {}
        for query in test_queries:
            print(f"\n{'='*60}")
            print(f"í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬: {query}")
            print(f"{'='*60}")
            
            # ê²€ìƒ‰ ë‹¨ê³„
            retrieved_docs = self.test_retrieval(vectorstore, query)
            
            # ìƒì„± ë‹¨ê³„
            generated_answer = self.test_generation(llm, query, retrieved_docs)
            
            results[query] = {
                'retrieved_docs': len(retrieved_docs),
                'generated_answer': generated_answer,
                'doc_titles': [doc.metadata.get('title', 'Unknown') for doc in retrieved_docs]
            }
        
        # ê²°ê³¼ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = f"proper_rag_results_{timestamp}.json"
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“ ê²°ê³¼ê°€ {results_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return results

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    experiment = ProperRAGImplementation("data/dev_center_guide_allmd_touched.md")
    results = experiment.run_proper_rag_experiment()
    
    print("\nâœ… ì˜¬ë°”ë¥¸ RAG ì‹¤í—˜ ì™„ë£Œ!")

if __name__ == "__main__":
    main() 