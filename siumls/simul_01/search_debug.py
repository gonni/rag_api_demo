#!/usr/bin/env python3
"""
ê²€ìƒ‰ ë¬¸ì œ ì§„ë‹¨ ë° ë””ë²„ê¹… ë„êµ¬
"""

import re
from typing import List, Dict, Any
from langchain.docstore.document import Document
from langchain_ollama import OllamaEmbeddings
from langchain_community.vectorstores import FAISS

class SearchDebugger:
    def __init__(self, markdown_file_path: str):
        self.markdown_file_path = markdown_file_path
        self.raw_text = self.load_markdown_file(markdown_file_path)
        
    def load_markdown_file(self, file_path: str) -> str:
        """ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        with open(file_path, 'r', encoding='utf-8') as file:
            return file.read()
    
    def find_pns_documents(self) -> List[Dict[str, Any]]:
        """PNS ê´€ë ¨ ë¬¸ì„œë“¤ì„ ì°¾ìŠµë‹ˆë‹¤."""
        lines = self.raw_text.split('\n')
        pns_docs = []
        
        for i, line in enumerate(lines):
            if 'PNS' in line or 'purchaseState' in line:
                # ì£¼ë³€ ì»¨í…ìŠ¤íŠ¸ í¬í•¨
                context_start = max(0, i - 3)
                context_end = min(len(lines), i + 4)
                context = lines[context_start:context_end]
                
                pns_docs.append({
                    'line_number': i,
                    'line_content': line,
                    'context': context,
                    'context_lines': list(range(context_start, context_end))
                })
        
        return pns_docs
    
    def create_simple_documents(self) -> List[Document]:
        """ê°„ë‹¨í•œ ë¬¸ì„œ ë¶„í• ì„ ìƒì„±í•©ë‹ˆë‹¤."""
        lines = self.raw_text.split('\n')
        docs = []
        
        # 100ì¤„ì”© ì²­í¬ë¡œ ë¶„í• 
        chunk_size = 100
        for i in range(0, len(lines), chunk_size):
            chunk_lines = lines[i:i + chunk_size]
            content = '\n'.join(chunk_lines)
            
            # PNS ê´€ë ¨ í‚¤ì›Œë“œ ê²€ìƒ‰
            pns_count = content.lower().count('pns')
            purchase_state_count = content.lower().count('purchasestate')
            
            doc = Document(
                page_content=content,
                metadata={
                    'chunk_id': i // chunk_size,
                    'start_line': i,
                    'end_line': min(i + chunk_size, len(lines)),
                    'pns_count': pns_count,
                    'purchase_state_count': purchase_state_count,
                    'source': self.markdown_file_path
                }
            )
            docs.append(doc)
        
        return docs
    
    def test_embedding_similarity(self, query: str, docs: List[Document]) -> Dict[str, Any] | None :
        """ì„ë² ë”© ìœ ì‚¬ë„ë¥¼ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
        print(f"\nğŸ” ì„ë² ë”© ìœ ì‚¬ë„ í…ŒìŠ¤íŠ¸: '{query}'")
        
        try:
            embeddings = OllamaEmbeddings(model="nomic-embed-text")
            
            # ì¿¼ë¦¬ ì„ë² ë”©
            query_embedding = embeddings.embed_query(query)
            print(f"ì¿¼ë¦¬ ì„ë² ë”© ì°¨ì›: {len(query_embedding)}")
            
            # ë¬¸ì„œ ì„ë² ë”© ë° ìœ ì‚¬ë„ ê³„ì‚°
            similarities = []
            for i, doc in enumerate(docs):
                try:
                    doc_embedding = embeddings.embed_query(doc.page_content[:1000])  # ì²˜ìŒ 1000ìë§Œ
                    
                    # ì½”ì‚¬ì¸ ìœ ì‚¬ë„ ê³„ì‚°
                    import numpy as np
                    similarity = np.dot(query_embedding, doc_embedding) / (np.linalg.norm(query_embedding) * np.linalg.norm(doc_embedding))
                    
                    similarities.append({
                        'doc_id': i,
                        'similarity': similarity,
                        'pns_count': doc.metadata.get('pns_count', 0),
                        'purchase_state_count': doc.metadata.get('purchase_state_count', 0),
                        'content_preview': doc.page_content[:200]
                    })
                    
                except Exception as e:
                    print(f"ë¬¸ì„œ {i} ì„ë² ë”© ì‹¤íŒ¨: {e}")
            
            # ìœ ì‚¬ë„ ìˆœìœ¼ë¡œ ì •ë ¬
            similarities.sort(key=lambda x: x['similarity'], reverse=True)
            
            print(f"\nğŸ“Š ìœ ì‚¬ë„ ê²°ê³¼ (ìƒìœ„ 10ê°œ):")
            for i, sim in enumerate(similarities[:10]):
                print(f"  {i+1}. ë¬¸ì„œ {sim['doc_id']}: ìœ ì‚¬ë„ {sim['similarity']:.4f}, PNS {sim['pns_count']}, purchaseState {sim['purchase_state_count']}")
                print(f"     ë‚´ìš©: {sim['content_preview'][:100]}...")
            
            return {
                'query': query,
                'total_docs': len(docs),
                'similarities': similarities[:10]
            }
            
        except Exception as e:
            print(f"ì„ë² ë”© í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            return None
    
    def test_vectorstore_search(self, docs: List[Document], query: str) -> Dict[str, Any] | None:
        """ë²¡í„° ìŠ¤í† ì–´ ê²€ìƒ‰ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
        print(f"\nğŸ” ë²¡í„° ìŠ¤í† ì–´ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸: '{query}'")
        
        try:
            embeddings = OllamaEmbeddings(model="nomic-embed-text")
            vectorstore = FAISS.from_documents(docs, embeddings)
            
            # ë‹¤ì–‘í•œ ê²€ìƒ‰ ë°©ë²• ì‹œë„
            search_methods = [
                {"k": 5, "search_type": "similarity"},
                {"k": 10, "search_type": "similarity"},
                {"k": 5, "search_type": "mmr"},
                {"k": 10, "search_type": "mmr"}
            ]
            
            results = {}
            for method in search_methods:
                try:
                    retriever = vectorstore.as_retriever(search_kwargs=method)
                    retrieved_docs = retriever.get_relevant_documents(query)
                    
                    # ê´€ë ¨ì„± ë¶„ì„
                    pns_found = 0
                    purchase_state_found = 0
                    
                    for doc in retrieved_docs:
                        content = doc.page_content.lower()
                        if 'pns' in content:
                            pns_found += 1
                        if 'purchasestate' in content:
                            purchase_state_found += 1
                    
                    method_name = f"{method['search_type']}_k{method['k']}"
                    results[method_name] = {
                        'docs_retrieved': len(retrieved_docs),
                        'pns_found': pns_found,
                        'purchase_state_found': purchase_state_found,
                        'doc_ids': [doc.metadata.get('chunk_id', 'unknown') for doc in retrieved_docs]
                    }
                    
                    print(f"  {method_name}: ê²€ìƒ‰ëœ ë¬¸ì„œ {len(retrieved_docs)}ê°œ, PNS {pns_found}ê°œ, purchaseState {purchase_state_found}ê°œ")
                    
                except Exception as e:
                    print(f"  {method}: ì˜¤ë¥˜ - {e}")
            
            return results
            
        except Exception as e:
            print(f"ë²¡í„° ìŠ¤í† ì–´ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return None
    
    def run_debug_analysis(self):
        """ì „ì²´ ë””ë²„ê¹… ë¶„ì„ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        print("ğŸ” ê²€ìƒ‰ ë¬¸ì œ ì§„ë‹¨ ì‹œì‘")
        
        # 1. PNS ë¬¸ì„œ ì°¾ê¸°
        pns_docs = self.find_pns_documents()
        print(f"\nğŸ“„ PNS ê´€ë ¨ ë¬¸ì„œ ë°œê²¬: {len(pns_docs)}ê°œ")
        
        for i, doc in enumerate(pns_docs[:5]):
            print(f"  {i+1}. ë¼ì¸ {doc['line_number']}: {doc['line_content'][:100]}...")
        
        # 2. ê°„ë‹¨í•œ ë¬¸ì„œ ë¶„í• 
        docs = self.create_simple_documents()
        print(f"\nğŸ“š ë¬¸ì„œ ë¶„í•  ì™„ë£Œ: {len(docs)}ê°œ ì²­í¬")
        
        # PNS í¬í•¨ ë¬¸ì„œ ìˆ˜ ê³„ì‚°
        pns_chunks = sum(1 for doc in docs if doc.metadata['pns_count'] > 0)
        purchase_state_chunks = sum(1 for doc in docs if doc.metadata['purchase_state_count'] > 0)
        
        print(f"  PNS í¬í•¨ ì²­í¬: {pns_chunks}ê°œ")
        print(f"  purchaseState í¬í•¨ ì²­í¬: {purchase_state_chunks}ê°œ")
        
        # 3. ì„ë² ë”© ìœ ì‚¬ë„ í…ŒìŠ¤íŠ¸
        test_queries = ["PNS", "purchaseState", "PNS purchaseState"]
        
        for query in test_queries:
            similarity_result = self.test_embedding_similarity(query, docs)
            if similarity_result:
                print(f"âœ… '{query}' ì„ë² ë”© í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        
        # 4. ë²¡í„° ìŠ¤í† ì–´ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        for query in test_queries:
            search_result = self.test_vectorstore_search(docs, query)
            if search_result:
                print(f"âœ… '{query}' ë²¡í„° ìŠ¤í† ì–´ ê²€ìƒ‰ ì™„ë£Œ")
        
        print("\nâœ… ë””ë²„ê¹… ë¶„ì„ ì™„ë£Œ!")

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    debugger = SearchDebugger("data/dev_center_guide_allmd_touched.md")
    debugger.run_debug_analysis()

if __name__ == "__main__":
    main() 