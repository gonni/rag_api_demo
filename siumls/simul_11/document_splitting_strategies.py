"""
RAG ê²€ìƒ‰ ìµœì í™”ë¥¼ ìœ„í•œ ë‹¤ì–‘í•œ ë¬¸ì„œ ë¶„í•  ì „ëµ í…ŒìŠ¤íŠ¸

ì´ ëª¨ë“ˆì€ ë‹¤ìŒê³¼ ê°™ì€ ì „ëµë“¤ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤:
1. í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„í•  (PNS, purchaseState ë“± í•µì‹¬ í‚¤ì›Œë“œ ì¤‘ì‹¬)
2. ì˜ë¯¸ ê¸°ë°˜ ë¶„í•  (ë¬¸ë§¥ ë³´ì¡´)
3. í•˜ì´ë¸Œë¦¬ë“œ ë¶„í•  (í‚¤ì›Œë“œ + ì˜ë¯¸)
4. ìš°ì„ ìˆœìœ„ ê¸°ë°˜ ê²€ìƒ‰ (í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜)
"""

import os
import re
import pickle
from typing import List, Dict, Any, Tuple, Optional
from pathlib import Path
from dataclasses import dataclass

from langchain.docstore.document import Document
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_text_splitters import MarkdownHeaderTextSplitter
from langchain_ollama import OllamaEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_community.retrievers import BM25Retriever
from langchain.retrievers import EnsembleRetriever


@dataclass
class SearchResult:
    """ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„ì„ ìœ„í•œ ë°ì´í„° í´ë˜ìŠ¤"""
    strategy_name: str
    query: str
    documents: List[Document]
    keyword_scores: List[float]
    total_docs: int
    relevant_docs: int
    top_3_relevance: float


class DocumentSplittingStrategy:
    """ë¬¸ì„œ ë¶„í•  ì „ëµì˜ ê¸°ë³¸ í´ë˜ìŠ¤"""
    
    def __init__(self, document_path: str):
        self.document_path = document_path
        self.raw_text = self._load_document()
        
    def _load_document(self) -> str:
        """ì›ë³¸ ë¬¸ì„œ ë¡œë“œ"""
        with open(self.document_path, 'r', encoding='utf-8') as f:
            return f.read()
    
    def split_documents(self) -> List[Document]:
        """ë¬¸ì„œ ë¶„í•  (ê° ì „ëµì—ì„œ êµ¬í˜„)"""
        raise NotImplementedError
    
    def get_strategy_name(self) -> str:
        """ì „ëµ ì´ë¦„ ë°˜í™˜"""
        raise NotImplementedError


class KeywordBasedSplitter(DocumentSplittingStrategy):
    """í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„í•  ì „ëµ"""
    
    def __init__(self, document_path: str, target_keywords: Optional[List[str]] = None):
        super().__init__(document_path)
        self.target_keywords = target_keywords or [
            "PNS", "purchaseState", "Payment Notification", 
            "ê²°ì œ", "êµ¬ë§¤", "ìƒíƒœ", "ë©”ì‹œì§€", "ê·œê²©"
        ]
    
    def get_strategy_name(self) -> str:
        return "keyword_based"
    
    def split_documents(self) -> List[Document]:
        """í‚¤ì›Œë“œ ì¤‘ì‹¬ìœ¼ë¡œ ë¬¸ì„œ ë¶„í• """
        documents = []
        
        # 1. í—¤ë” ê¸°ë°˜ 1ì°¨ ë¶„í• 
        header_splitter = MarkdownHeaderTextSplitter(
            headers_to_split_on=[
                ("#", "Header 1"),
                ("##", "Header 2"), 
                ("###", "Header 3"),
            ]
        )
        header_docs = header_splitter.split_text(self.raw_text)
        
        # 2. í‚¤ì›Œë“œ ê¸°ë°˜ ì²­í‚¹ ë° ê°•í™”
        for doc in header_docs:
            enhanced_content = self._enhance_with_keywords(doc.page_content)
            
            # í‚¤ì›Œë“œ ë°€ë„ ê³„ì‚°
            keyword_density = self._calculate_keyword_density(enhanced_content)
            
            # í‚¤ì›Œë“œê°€ ë§ì´ í¬í•¨ëœ ë¬¸ì„œëŠ” ë” ì„¸ë¶„í™”
            if keyword_density > 0.02:  # 2% ì´ìƒ
                sub_docs = self._split_keyword_rich_content(enhanced_content, doc.metadata)
                documents.extend(sub_docs)
            else:
                # ì¼ë°˜ ì²­í‚¹
                text_splitter = RecursiveCharacterTextSplitter(
                    chunk_size=800,
                    chunk_overlap=100,
                    separators=["\n\n", "\n", ". ", ", ", " "]
                )
                chunks = text_splitter.split_text(enhanced_content)
                
                for i, chunk in enumerate(chunks):
                    metadata = doc.metadata.copy()
                    metadata.update({
                        "chunk_index": i,
                        "keyword_density": self._calculate_keyword_density(chunk),
                        "source_strategy": "keyword_based"
                    })
                    documents.append(Document(page_content=chunk, metadata=metadata))
        
        return documents
    
    def _enhance_with_keywords(self, content: str) -> str:
        """í‚¤ì›Œë“œ í™•ì¥ìœ¼ë¡œ ê²€ìƒ‰ì„± í–¥ìƒ"""
        enhancements = {
            "PNS": "PNS Payment Notification Service ê²°ì œì•Œë¦¼ì„œë¹„ìŠ¤",
            "purchaseState": "purchaseState êµ¬ë§¤ìƒíƒœ ê²°ì œìƒíƒœ",
            "COMPLETED": "COMPLETED ì™„ë£Œ êµ¬ë§¤ì™„ë£Œ ê²°ì œì™„ë£Œ",
            "CANCELED": "CANCELED ì·¨ì†Œ êµ¬ë§¤ì·¨ì†Œ ê²°ì œì·¨ì†Œ"
        }
        
        enhanced = content
        for keyword, expansion in enhancements.items():
            if keyword in content:
                enhanced = enhanced.replace(keyword, expansion, 1)  # ì²« ë²ˆì§¸ë§Œ êµì²´
        
        return enhanced
    
    def _calculate_keyword_density(self, content: str) -> float:
        """í‚¤ì›Œë“œ ë°€ë„ ê³„ì‚°"""
        words = content.lower().split()
        keyword_count = sum(1 for word in words 
                          if any(kw.lower() in word for kw in self.target_keywords))
        return keyword_count / len(words) if words else 0
    
    def _split_keyword_rich_content(self, content: str, base_metadata: Dict) -> List[Document]:
        """í‚¤ì›Œë“œê°€ ë§ì€ ì½˜í…ì¸ ë¥¼ ë” ì„¸ë°€í•˜ê²Œ ë¶„í• """
        documents = []
        
        # í‚¤ì›Œë“œ ì¤‘ì‹¬ìœ¼ë¡œ ì„¸ê·¸ë¨¼íŠ¸ ë¶„í• 
        segments = self._segment_by_keywords(content)
        
        for i, segment in enumerate(segments):
            metadata = base_metadata.copy()
            metadata.update({
                "chunk_index": i,
                "keyword_density": self._calculate_keyword_density(segment),
                "source_strategy": "keyword_based_detailed",
                "keywords_found": self._extract_keywords_from_content(segment)
            })
            documents.append(Document(page_content=segment, metadata=metadata))
        
        return documents
    
    def _segment_by_keywords(self, content: str) -> List[str]:
        """í‚¤ì›Œë“œë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì„¸ê·¸ë¨¼íŠ¸ ë¶„í• """
        # ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
        sentences = re.split(r'[.!?]\s+', content)
        segments = []
        current_segment = []
        
        for sentence in sentences:
            current_segment.append(sentence)
            
            # í‚¤ì›Œë“œê°€ í¬í•¨ëœ ë¬¸ì¥ì´ë©´ì„œ ì ì • ê¸¸ì´ê°€ ë˜ë©´ ì„¸ê·¸ë¨¼íŠ¸ ì™„ì„±
            has_keyword = any(kw.lower() in sentence.lower() for kw in self.target_keywords)
            segment_text = '. '.join(current_segment)
            
            if has_keyword and len(segment_text) > 200:
                segments.append(segment_text + '.')
                current_segment = []
        
        # ë‚¨ì€ ë¬¸ì¥ë“¤ ì²˜ë¦¬
        if current_segment:
            segments.append('. '.join(current_segment) + '.')
        
        return [seg for seg in segments if len(seg.strip()) > 50]
    
    def _extract_keywords_from_content(self, content: str) -> List[str]:
        """ì½˜í…ì¸ ì—ì„œ ë°œê²¬ëœ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        found_keywords = []
        content_lower = content.lower()
        
        for keyword in self.target_keywords:
            if keyword.lower() in content_lower:
                found_keywords.append(keyword)
        
        return found_keywords


class SemanticBasedSplitter(DocumentSplittingStrategy):
    """ì˜ë¯¸ ê¸°ë°˜ ë¶„í•  ì „ëµ"""
    
    def get_strategy_name(self) -> str:
        return "semantic_based"
    
    def split_documents(self) -> List[Document]:
        """ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¬¸ì„œ ë¶„í• """
        documents = []
        
        # í—¤ë” ê¸°ë°˜ ë¶„í• 
        header_splitter = MarkdownHeaderTextSplitter(
            headers_to_split_on=[
                ("#", "Header 1"),
                ("##", "Header 2"),
                ("###", "Header 3"),
            ]
        )
        header_docs = header_splitter.split_text(self.raw_text)
        
        # ê° ì„¹ì…˜ì„ ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í• 
        for doc in header_docs:
            semantic_chunks = self._split_by_semantic_units(doc.page_content)
            
            for i, chunk in enumerate(semantic_chunks):
                metadata = doc.metadata.copy()
                metadata.update({
                    "chunk_index": i,
                    "source_strategy": "semantic_based",
                    "semantic_score": self._calculate_semantic_score(chunk)
                })
                documents.append(Document(page_content=chunk, metadata=metadata))
        
        return documents
    
    def _split_by_semantic_units(self, content: str) -> List[str]:
        """ì˜ë¯¸ ë‹¨ìœ„ë¡œ ë¶„í• """
        # ë‹¨ë½, ë¬¸ì¥, ì˜ë¯¸ êµ¬ì¡°ë¥¼ ê³ ë ¤í•œ ë¶„í• 
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1200,  # ë” í° ì²­í¬ë¡œ ë¬¸ë§¥ ë³´ì¡´
            chunk_overlap=200,
            separators=["\n\n\n", "\n\n", "\n", ". ", "? ", "! ", ", "]
        )
        return text_splitter.split_text(content)
    
    def _calculate_semantic_score(self, content: str) -> float:
        """ì˜ë¯¸ì  ì™„ì„±ë„ ì ìˆ˜ ê³„ì‚°"""
        # ê°„ë‹¨í•œ íœ´ë¦¬ìŠ¤í‹±ìœ¼ë¡œ ì˜ë¯¸ì  ì™„ì„±ë„ ì¸¡ì •
        sentences = re.split(r'[.!?]', content)
        complete_sentences = [s for s in sentences if len(s.strip()) > 10]
        
        # ì™„ì„±ëœ ë¬¸ì¥ ë¹„ìœ¨, ê¸¸ì´ ì ì •ì„± ë“±ì„ ê³ ë ¤
        completeness = len(complete_sentences) / len(sentences) if sentences else 0
        length_score = min(1.0, len(content) / 500)  # 500ì ê¸°ì¤€ ì •ê·œí™”
        
        return (completeness + length_score) / 2


class HybridSplitter(DocumentSplittingStrategy):
    """í•˜ì´ë¸Œë¦¬ë“œ ë¶„í•  ì „ëµ (í‚¤ì›Œë“œ + ì˜ë¯¸)"""
    
    def __init__(self, document_path: str, target_keywords: Optional[List[str]] = None):
        super().__init__(document_path)
        self.keyword_splitter = KeywordBasedSplitter(document_path, target_keywords)
        self.semantic_splitter = SemanticBasedSplitter(document_path)
    
    def get_strategy_name(self) -> str:
        return "hybrid"
    
    def split_documents(self) -> List[Document]:
        """í‚¤ì›Œë“œì™€ ì˜ë¯¸ë¥¼ ëª¨ë‘ ê³ ë ¤í•œ ë¶„í• """
        documents = []
        
        # 1ì°¨: í‚¤ì›Œë“œ ê¸°ë°˜ ë¶„í• 
        keyword_docs = self.keyword_splitter.split_documents()
        
        # 2ì°¨: í‚¤ì›Œë“œ ë°€ë„ê°€ ë‚®ì€ ë¬¸ì„œëŠ” ì˜ë¯¸ ê¸°ë°˜ìœ¼ë¡œ ì¬ë¶„í• 
        for doc in keyword_docs:
            keyword_density = doc.metadata.get('keyword_density', 0)
            
            if keyword_density < 0.01:  # í‚¤ì›Œë“œ ë°€ë„ê°€ ë‚®ìœ¼ë©´
                # ì˜ë¯¸ ê¸°ë°˜ìœ¼ë¡œ ì¬ë¶„í• 
                semantic_chunks = self.semantic_splitter._split_by_semantic_units(doc.page_content)
                
                for i, chunk in enumerate(semantic_chunks):
                    metadata = doc.metadata.copy()
                    metadata.update({
                        "chunk_index": f"{doc.metadata.get('chunk_index', 0)}_{i}",
                        "source_strategy": "hybrid_semantic",
                        "semantic_score": self.semantic_splitter._calculate_semantic_score(chunk)
                    })
                    documents.append(Document(page_content=chunk, metadata=metadata))
            else:
                # í‚¤ì›Œë“œ ë°€ë„ê°€ ë†’ìœ¼ë©´ ê·¸ëŒ€ë¡œ ìœ ì§€
                doc.metadata["source_strategy"] = "hybrid_keyword"
                documents.append(doc)
        
        return documents


class SmartRetriever:
    """ìŠ¤ë§ˆíŠ¸ ê²€ìƒ‰ê¸° - í‚¤ì›Œë“œ ìš°ì„ ìˆœìœ„ ê¸°ë°˜"""
    
    def __init__(self, documents: List[Document], embedding_model_name: str = "bge-m3:latest"):
        self.documents = documents
        self.embedding_model_name = embedding_model_name
        self.vector_store = None
        self.bm25_retriever = None
        self.ensemble_retriever = None
        
    def build_retrievers(self):
        """ê²€ìƒ‰ê¸° êµ¬ì¶•"""
        print(f"ğŸ”§ ê²€ìƒ‰ê¸° êµ¬ì¶• ì¤‘... (ë¬¸ì„œ ìˆ˜: {len(self.documents)})")
        
        # Vector store êµ¬ì¶•
        embeddings = OllamaEmbeddings(model=self.embedding_model_name)
        self.vector_store = FAISS.from_documents(self.documents, embeddings)
        
        # BM25 ê²€ìƒ‰ê¸° êµ¬ì¶•
        self.bm25_retriever = BM25Retriever.from_documents(
            self.documents,
            bm25_params={"k1": 1.5, "b": 0.75}
        )
        self.bm25_retriever.k = 20
        
        # Vector ê²€ìƒ‰ê¸°
        vector_retriever = self.vector_store.as_retriever(
            search_type="mmr",
            search_kwargs={"k": 20, "fetch_k": 50, "lambda_mult": 0.7}
        )
        
        # ì•™ìƒë¸” ê²€ìƒ‰ê¸°
        self.ensemble_retriever = EnsembleRetriever(
            retrievers=[self.bm25_retriever, vector_retriever],
            weights=[0.6, 0.4]  # BM25ì— ë” ë†’ì€ ê°€ì¤‘ì¹˜
        )
        
        print("âœ… ê²€ìƒ‰ê¸° êµ¬ì¶• ì™„ë£Œ")
    
    def smart_search(self, query: str, max_results: int = 10) -> List[Document]:
        """ìŠ¤ë§ˆíŠ¸ ê²€ìƒ‰ - í‚¤ì›Œë“œ ìš°ì„ ìˆœìœ„ ì ìš©"""
        if not self.ensemble_retriever:
            raise ValueError("ê²€ìƒ‰ê¸°ê°€ êµ¬ì¶•ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. build_retrievers()ë¥¼ ë¨¼ì € í˜¸ì¶œí•˜ì„¸ìš”.")
        
        # 1. ì•™ìƒë¸” ê²€ìƒ‰ìœ¼ë¡œ ë” ë§ì€ í›„ë³´ ê²€ìƒ‰
        raw_results = self.ensemble_retriever.invoke(query)
        
        # 2. í‚¤ì›Œë“œ ê¸°ë°˜ í•„í„°ë§ ë° ì ìˆ˜ ê³„ì‚°
        scored_results = self._score_documents(query, raw_results)
        
        # 3. ì ìˆ˜ìˆœ ì •ë ¬
        scored_results.sort(key=lambda x: x[0], reverse=True)
        
        # 4. ìƒìœ„ ê²°ê³¼ ë°˜í™˜
        return [doc for score, doc in scored_results[:max_results]]
    
    def _score_documents(self, query: str, documents: List[Document]) -> List[Tuple[float, Document]]:
        """ë¬¸ì„œ ì ìˆ˜ ê³„ì‚°"""
        scored_docs = []
        query_keywords = self._extract_query_keywords(query)
        
        for doc in documents:
            score = 0.0
            content_lower = doc.page_content.lower()
            
            # 1. í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ (ê°€ì¥ ì¤‘ìš”)
            keyword_matches = 0
            for keyword in query_keywords:
                if keyword.lower() in content_lower:
                    keyword_matches += 1
                    # ì •í™•í•œ ë§¤ì¹­ì— ë†’ì€ ì ìˆ˜
                    if keyword.lower() == keyword.lower():  # ì™„ì „ ì¼ì¹˜
                        score += 10
                    else:
                        score += 5
            
            # 2. í‚¤ì›Œë“œ ë°€ë„ ì ìˆ˜
            density = doc.metadata.get('keyword_density', 0)
            score += density * 20
            
            # 3. ì „ëµë³„ ë³´ë„ˆìŠ¤ ì ìˆ˜
            strategy = doc.metadata.get('source_strategy', '')
            if 'keyword' in strategy:
                score += 5
            
            # 4. ìœ„ì¹˜ ì ìˆ˜ (ë¬¸ì„œ ì•ë¶€ë¶„ì— í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ê°€ì )
            first_half = content_lower[:len(content_lower)//2]
            if any(kw.lower() in first_half for kw in query_keywords):
                score += 8
            
            # 5. ê¸¸ì´ ì ì •ì„± ì ìˆ˜
            doc_length = len(doc.page_content.split())
            if 50 <= doc_length <= 300:  # ì ì • ê¸¸ì´
                score += 3
            
            doc.metadata['search_score'] = score
            doc.metadata['keyword_matches'] = keyword_matches
            scored_docs.append((score, doc))
        
        return scored_docs
    
    def _extract_query_keywords(self, query: str) -> List[str]:
        """ì¿¼ë¦¬ì—ì„œ ì¤‘ìš” í‚¤ì›Œë“œ ì¶”ì¶œ"""
        # ê¸°ìˆ  ìš©ì–´ íŒ¨í„´
        tech_patterns = [
            r'\b[A-Z]{2,}\b',  # PNS, API ë“±
            r'\b[a-z]+[A-Z][a-zA-Z]*\b',  # purchaseState ë“±
        ]
        
        keywords = []
        for pattern in tech_patterns:
            keywords.extend(re.findall(pattern, query))
        
        # í•œê¸€ í‚¤ì›Œë“œ ì¶”ê°€
        korean_keywords = ['ë©”ì‹œì§€', 'ê·œê²©', 'ê°’', 'êµ¬ì„±', 'ìƒíƒœ', 'ê²°ì œ', 'ì„œë²„']
        for keyword in korean_keywords:
            if keyword in query:
                keywords.append(keyword)
        
        return list(set(keywords))


class RAGExperimentRunner:
    """RAG ì‹¤í—˜ ì‹¤í–‰ê¸°"""
    
    def __init__(self, document_path: str):
        self.document_path = document_path
        self.strategies = {
            'keyword': KeywordBasedSplitter(document_path),
            'semantic': SemanticBasedSplitter(document_path), 
            'hybrid': HybridSplitter(document_path)
        }
        self.test_queries = [
            "PNS ë©”ì‹œì§€ ì„œë²„ ê·œê²©ì˜ purchaseStateëŠ” ì–´ë–¤ ê°’ìœ¼ë¡œ êµ¬ì„±ë˜ë‚˜ìš”?",
            "Payment Notification Serviceì˜ ë©”ì‹œì§€ êµ¬ì¡°ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?",
            "purchaseState COMPLETED CANCELED ê°’ì€ ë¬´ì—‡ì„ ì˜ë¯¸í•˜ë‚˜ìš”?",
            "ê²°ì œ ìƒíƒœ ì •ë³´ë¥¼ ì„œë²„ì—ì„œ ë°›ëŠ” ë°©ë²•ì€?",
            "PNS ì„¤ì • ë°©ë²•ê³¼ URL êµ¬ì„±ì€?"
        ]
    
    def run_experiments(self) -> Dict[str, List[SearchResult]]:
        """ëª¨ë“  ì „ëµì— ëŒ€í•´ ì‹¤í—˜ ì‹¤í–‰"""
        results = {}
        
        for strategy_name, splitter in self.strategies.items():
            print(f"\nğŸ§ª ì‹¤í—˜ ì‹œì‘: {strategy_name}")
            strategy_results = []
            
            # ë¬¸ì„œ ë¶„í• 
            documents = splitter.split_documents()
            print(f"ğŸ“„ ë¶„í• ëœ ë¬¸ì„œ ìˆ˜: {len(documents)}")
            
            # ê²€ìƒ‰ê¸° êµ¬ì¶•
            retriever = SmartRetriever(documents)
            retriever.build_retrievers()
            
            # ê° ì¿¼ë¦¬ì— ëŒ€í•´ í…ŒìŠ¤íŠ¸
            for query in self.test_queries:
                print(f"ğŸ” ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸: {query[:30]}...")
                
                search_results = retriever.smart_search(query, max_results=10)
                analysis = self._analyze_results(query, search_results, strategy_name)
                strategy_results.append(analysis)
            
            results[strategy_name] = strategy_results
            print(f"âœ… {strategy_name} ì „ëµ ì‹¤í—˜ ì™„ë£Œ")
        
        return results
    
    def _analyze_results(self, query: str, documents: List[Document], strategy_name: str) -> SearchResult:
        """ê²€ìƒ‰ ê²°ê³¼ ë¶„ì„"""
        query_keywords = self._extract_keywords(query)
        keyword_scores = []
        relevant_count = 0
        
        for doc in documents:
            # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê³„ì‚°
            content_lower = doc.page_content.lower()
            matches = sum(1 for kw in query_keywords if kw.lower() in content_lower)
            score = matches / len(query_keywords) if query_keywords else 0
            keyword_scores.append(score)
            
            if score > 0.3:  # 30% ì´ìƒ í‚¤ì›Œë“œ ë§¤ì¹­
                relevant_count += 1
        
        # ìƒìœ„ 3ê°œ ë¬¸ì„œì˜ í‰ê·  ê´€ë ¨ì„±
        top_3_relevance = sum(keyword_scores[:3]) / 3 if len(keyword_scores) >= 3 else 0
        
        return SearchResult(
            strategy_name=strategy_name,
            query=query,
            documents=documents,
            keyword_scores=keyword_scores,
            total_docs=len(documents),
            relevant_docs=relevant_count,
            top_3_relevance=top_3_relevance
        )
    
    def _extract_keywords(self, query: str) -> List[str]:
        """ì¿¼ë¦¬ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ"""
        keywords = []
        
        # ì˜ë¬¸ íŒ¨í„´
        tech_patterns = [r'\b[A-Z]{2,}\b', r'\b[a-z]+[A-Z][a-zA-Z]*\b']
        for pattern in tech_patterns:
            keywords.extend(re.findall(pattern, query))
        
        # í•œê¸€ í‚¤ì›Œë“œ
        korean_words = ['ë©”ì‹œì§€', 'ê·œê²©', 'ê°’', 'êµ¬ì„±', 'ìƒíƒœ', 'ê²°ì œ', 'ì„œë²„']
        for word in korean_words:
            if word in query:
                keywords.append(word)
        
        return list(set(keywords))
    
    def print_results_summary(self, results: Dict[str, List[SearchResult]]):
        """ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*80)
        print("ğŸ† RAG ê²€ìƒ‰ ìµœì í™” ì‹¤í—˜ ê²°ê³¼ ìš”ì•½")
        print("="*80)
        
        for strategy_name, strategy_results in results.items():
            print(f"\nğŸ“Š ì „ëµ: {strategy_name.upper()}")
            print("-" * 50)
            
            total_relevance = sum(r.top_3_relevance for r in strategy_results)
            avg_relevance = total_relevance / len(strategy_results)
            total_relevant_docs = sum(r.relevant_docs for r in strategy_results)
            
            print(f"í‰ê·  ìƒìœ„3 ê´€ë ¨ì„±: {avg_relevance:.3f}")
            print(f"ì „ì²´ ê´€ë ¨ ë¬¸ì„œ ìˆ˜: {total_relevant_docs}")
            print(f"í‰ê·  ê´€ë ¨ ë¬¸ì„œ ë¹„ìœ¨: {total_relevant_docs/len(strategy_results):.1f}")
            
            # ê° ì¿¼ë¦¬ë³„ ìƒì„¸ ê²°ê³¼
            for result in strategy_results:
                print(f"  â€¢ {result.query[:40]}... -> ê´€ë ¨ì„±: {result.top_3_relevance:.3f}")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ RAG ê²€ìƒ‰ ìµœì í™” ì‹¤í—˜ ì‹œì‘")
    
    document_path = "../data/dev_center_guide_allmd_touched.md"
    
    if not os.path.exists(document_path):
        print(f"âŒ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {document_path}")
        return
    
    # ì‹¤í—˜ ì‹¤í–‰
    runner = RAGExperimentRunner(document_path)
    results = runner.run_experiments()
    
    # ê²°ê³¼ ì¶œë ¥
    runner.print_results_summary(results)
    
    # ê²°ê³¼ ì €ì¥
    os.makedirs("results", exist_ok=True)
    output_path = "results/experiment_results.pkl"
    with open(output_path, "wb") as f:
        pickle.dump(results, f)
    print(f"\nğŸ’¾ ê²°ê³¼ ì €ì¥ë¨: {output_path}")


if __name__ == "__main__":
    main()
