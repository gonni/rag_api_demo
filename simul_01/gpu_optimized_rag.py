#!/usr/bin/env python3
"""
GPU ìµœì í™”ëœ RAG ì‹¤í—˜
"""

import os
import re
import torch
from typing import List, Dict, Any
from pathlib import Path
from langchain.docstore.document import Document
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_ollama import OllamaEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import MarkdownHeaderTextSplitter
import json
from datetime import datetime

# GPU ë©”ëª¨ë¦¬ ìµœì í™” ì„¤ì •
if torch.cuda.is_available():
    torch.cuda.empty_cache()
    # GPU ë©”ëª¨ë¦¬ í• ë‹¹ ìµœì í™”
    torch.backends.cudnn.benchmark = True
    print(f"GPU ì‚¬ìš© ê°€ëŠ¥: {torch.cuda.get_device_name(0)}")
    print(f"GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")

class GPUOptimizedDocumentSplitterExperiment:
    def __init__(self, markdown_file_path: str):
        self.markdown_file_path = markdown_file_path
        self.raw_text = self.load_markdown_file(markdown_file_path)
        self.results = {}
        
    def load_markdown_file(self, file_path: str) -> str:
        """ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        with open(file_path, 'r', encoding='utf-8') as file:
            return file.read()
    
    def extract_headers(self, text: str) -> List[Dict[str, Any]]:
        """ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ì—ì„œ í—¤ë” ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        headers = []
        lines = text.split('\n')
        
        for i, line in enumerate(lines):
            header_match = re.match(r'^(#{1,6})\s+(.+)$', line.strip())
            if header_match:
                level = len(header_match.group(1))
                title = header_match.group(2).strip()
                headers.append({
                    'level': level,
                    'title': title,
                    'line_number': i,
                    'content_start': i + 1
                })
        
        return headers
    
    def get_content_between_headers(self, text: str, start_header: Dict, end_header: Dict[str, Any] | None = None) -> str:
        """ë‘ í—¤ë” ì‚¬ì´ì˜ ë‚´ìš©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        lines = text.split('\n')
        start_line = start_header['content_start']
        
        if end_header:
            end_line = end_header['line_number']
        else:
            end_line = len(lines)
        
        return '\n'.join(lines[start_line:end_line])
    
    def strategy_1_hierarchical_with_context(self) -> List[Document]:
        """ì „ëµ 1: ê³„ì¸µì  ë¶„í•  + ì „ì²´ ë§¥ë½ í¬í•¨"""
        headers = self.extract_headers(self.raw_text)
        docs = []
        
        # ëŒ€ì œëª©(##) ê¸°ì¤€ìœ¼ë¡œ ë©”ì¸ ì„¹ì…˜ ìƒì„±
        main_sections = []
        for i, header in enumerate(headers):
            if header['level'] == 2:
                section_content = self.get_content_between_headers(
                    self.raw_text, 
                    header, 
                    headers[i + 1] if i + 1 < len(headers) else None
                )
                main_sections.append({
                    'title': header['title'],
                    'content': section_content,
                    'start_line': header['line_number']
                })
        
        # ê° ë©”ì¸ ì„¹ì…˜ì— ëŒ€í•´ ì„¸ë¶€ ë¶„í• 
        for section in main_sections:
            # ë©”ì¸ ì„¹ì…˜ ì „ì²´ë¥¼ í•˜ë‚˜ì˜ ë¬¸ì„œë¡œ ìƒì„±
            main_doc = Document(
                page_content=f"[MAIN_SECTION]: {section['title']}\n\n{section['content']}",
                metadata={
                    'type': 'main_section',
                    'title': section['title'],
                    'source': self.markdown_file_path,
                    'section_level': 2
                }
            )
            docs.append(main_doc)
            
            # í•´ë‹¹ ì„¹ì…˜ ë‚´ì˜ ì†Œì œëª©ë“¤ë¡œ ì„¸ë¶€ ë¶„í• 
            section_headers = self.extract_headers(section['content'])
            for sub_header in section_headers:
                if sub_header['level'] >= 3:
                    sub_content = self.get_content_between_headers(
                        section['content'],
                        sub_header,
                        section_headers[section_headers.index(sub_header) + 1] if section_headers.index(sub_header) + 1 < len(section_headers) else None
                    )
                    
                    title_hierarchy = f"{section['title']} / {sub_header['title']}"
                    
                    sub_doc = Document(
                        page_content=f"[SUBSECTION]: {title_hierarchy}\n\n{sub_content}",
                        metadata={
                            'type': 'subsection',
                            'title': sub_header['title'],
                            'parent_title': section['title'],
                            'title_hierarchy': title_hierarchy,
                            'source': self.markdown_file_path,
                            'section_level': sub_header['level']
                        }
                    )
                    docs.append(sub_doc)
        
        return docs
    
    def create_vectorstore(self, docs: List[Document], strategy_name: str) -> FAISS:
        """GPU ìµœì í™”ëœ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        print(f"\n=== {strategy_name} ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì¤‘ ===")
        print(f"ë¬¸ì„œ ìˆ˜: {len(docs)}")
        
        # GPU ë©”ëª¨ë¦¬ ìƒíƒœ í™•ì¸
        if torch.cuda.is_available():
            print(f"GPU ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰: {torch.cuda.memory_allocated() / 1e9:.2f}GB")
            print(f"GPU ë©”ëª¨ë¦¬ ìºì‹œ: {torch.cuda.memory_reserved() / 1e9:.2f}GB")
        
        # ë°°ì¹˜ í¬ê¸° ì¡°ì • (GPU ë©”ëª¨ë¦¬ì— ë”°ë¼)
        batch_size = 32 if torch.cuda.is_available() else 16
        
        embeddings = OllamaEmbeddings(model="exaone3.5:latest")
        
        # ë°°ì¹˜ ì²˜ë¦¬ë¡œ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        vectorstore = FAISS.from_documents(docs, embeddings)
        
        # GPU ë©”ëª¨ë¦¬ ì •ë¦¬
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
            print(f"GPU ë©”ëª¨ë¦¬ ì •ë¦¬ í›„: {torch.cuda.memory_allocated() / 1e9:.2f}GB")
        
        return vectorstore
    
    def test_query(self, vectorstore: FAISS, query: str, strategy_name: str) -> Dict[str, Any]:
        """ì¿¼ë¦¬ë¥¼ í…ŒìŠ¤íŠ¸í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤."""
        retriever = vectorstore.as_retriever(search_kwargs={"k": 5})
        docs = retriever.get_relevant_documents(query)
        
        # ê²°ê³¼ ë¶„ì„
        relevant_docs = []
        for i, doc in enumerate(docs):
            relevance_score = 0
            if 'purchaseState' in doc.page_content:
                relevance_score += 10
            if 'COMPLETED' in doc.page_content or 'CANCELED' in doc.page_content:
                relevance_score += 5
            if 'PNS' in doc.page_content:
                relevance_score += 3
            
            relevant_docs.append({
                'rank': i + 1,
                'title': doc.metadata.get('title', 'Unknown'),
                'content_preview': doc.page_content[:200] + '...',
                'relevance_score': relevance_score,
                'metadata': doc.metadata
            })
        
        return {
            'strategy': strategy_name,
            'query': query,
            'total_docs': len(docs),
            'relevant_docs': relevant_docs,
            'avg_relevance_score': sum(d['relevance_score'] for d in relevant_docs) / len(relevant_docs) if relevant_docs else 0
        }
    
    def run_experiment(self, test_queries: List[str] | None = None):
        """GPU ìµœì í™”ëœ ì‹¤í—˜ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        if test_queries is None:
            test_queries = [
                "PNSì˜ purchaseStateì—ëŠ” ì–´ë–¤ ê°’ë“¤ì´ ìˆë‚˜ìš”?",
                "purchaseState COMPLETED CANCELED",
                "ì›ìŠ¤í† ì–´ ê²°ì œ ìƒíƒœ ê°’",
                "PNS payment notification service"
            ]
        
        strategies = {
            'strategy_1': self.strategy_1_hierarchical_with_context,
        }
        
        results = {}
        
        for strategy_name, strategy_func in strategies.items():
            print(f"\n=== ì‹¤í–‰ ì¤‘: {strategy_name} ===")
            
            # ë¬¸ì„œ ë¶„í• 
            docs = strategy_func()
            print(f"ìƒì„±ëœ ë¬¸ì„œ ìˆ˜: {len(docs)}")
            
            # GPU ìµœì í™”ëœ ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
            vectorstore = self.create_vectorstore(docs, strategy_name)
            
            # ê° ì¿¼ë¦¬ í…ŒìŠ¤íŠ¸
            strategy_results = []
            for query in test_queries:
                result = self.test_query(vectorstore, query, strategy_name)
                strategy_results.append(result)
                print(f"ì¿¼ë¦¬: {query}")
                print(f"í‰ê·  ê´€ë ¨ì„± ì ìˆ˜: {result['avg_relevance_score']:.2f}")
            
            results[strategy_name] = {
                'doc_count': len(docs),
                'query_results': strategy_results
            }
        
        # ê²°ê³¼ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = f"gpu_experiment_results_{timestamp}.json"
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)
        
        print(f"\nì‹¤í—˜ ê²°ê³¼ê°€ {results_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        return results

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸš€ GPU ìµœì í™”ëœ RAG ì‹¤í—˜ ì‹œì‘")
    
    # GPU ìƒíƒœ í™•ì¸
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
    else:
        print("CPU ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.")
    
    # ì‹¤í—˜ ì‹¤í–‰
    experiment = GPUOptimizedDocumentSplitterExperiment("data/dev_center_guide_allmd_touched.md")
    
    # í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì •ì˜
    test_queries = [
        "PNSì˜ purchaseStateì—ëŠ” ì–´ë–¤ ê°’ë“¤ì´ ìˆë‚˜ìš”?",
        "purchaseState COMPLETED CANCELED ê°’",
        "ì›ìŠ¤í† ì–´ ê²°ì œ ìƒíƒœ ê°’ë“¤",
        "PNS payment notification service purchaseState",
        "COMPLETED CANCELED ê²°ì œ ìƒíƒœ"
    ]
    
    # ì‹¤í—˜ ì‹¤í–‰
    results = experiment.run_experiment(test_queries)
    
    print("\nâœ… GPU ìµœì í™” ì‹¤í—˜ ì™„ë£Œ!")

if __name__ == "__main__":
    main() 