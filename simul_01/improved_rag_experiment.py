#!/usr/bin/env python3
"""
ê²€ìƒ‰ ì„±ëŠ¥ ê°œì„ ëœ RAG ì‹¤í—˜
"""

import os
import re
import torch
from typing import List, Dict, Any
from pathlib import Path
from langchain.docstore.document import Document
from langchain_community.document_loaders import TextLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_ollama import OllamaEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_text_splitters import MarkdownHeaderTextSplitter
import json
from datetime import datetime

class ImprovedRAGExperiment:
    def __init__(self, markdown_file_path: str):
        self.markdown_file_path = markdown_file_path
        self.raw_text = self.load_markdown_file(markdown_file_path)
        
    def load_markdown_file(self, file_path: str) -> str:
        """ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        with open(file_path, 'r', encoding='utf-8') as file:
            return file.read()
    
    def extract_headers(self, text: str) -> List[Dict[str, Any]]:
        """ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ì—ì„œ í—¤ë” ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        headers = []
        lines = text.split('\n')
        
        for i, line in enumerate(lines):
            header_match = re.match(r'^(#{1,6})\s+(.+)$', line.strip())
            if header_match:
                level = len(header_match.group(1))
                title = header_match.group(2).strip()
                headers.append({
                    'level': level,
                    'title': title,
                    'line_number': i,
                    'content_start': i + 1
                })
        
        return headers
    
    def get_content_between_headers(self, text: str, start_header: Dict, end_header: Dict[str, Any] | None = None) -> str:
        """ë‘ í—¤ë” ì‚¬ì´ì˜ ë‚´ìš©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        lines = text.split('\n')
        start_line = start_header['content_start']
        
        if end_header:
            end_line = end_header['line_number']
        else:
            end_line = len(lines)
        
        return '\n'.join(lines[start_line:end_line])
    
    def strategy_improved_search(self) -> List[Document]:
        """ê²€ìƒ‰ ì„±ëŠ¥ ê°œì„ ëœ ë¶„í•  ì „ëµ"""
        headers = self.extract_headers(self.raw_text)
        docs = []
        
        # PNS ê´€ë ¨ í‚¤ì›Œë“œ
        pns_keywords = ['PNS', 'purchaseState', 'COMPLETED', 'CANCELED', 'ê²°ì œ', 'ì·¨ì†Œ', 'Payment Notification Service']
        
        for i, header in enumerate(headers):
            content = self.get_content_between_headers(
                self.raw_text,
                header,
                headers[i + 1] if i + 1 < len(headers) else None
            )
            
            # PNS ê´€ë ¨ í‚¤ì›Œë“œ ê²€ìƒ‰
            found_keywords = []
            for keyword in pns_keywords:
                if keyword.lower() in content.lower():
                    found_keywords.append(keyword)
            
            # ì œëª© ê³„ì¸µ êµ¬ì¡° ìƒì„±
            title_hierarchy = self.build_title_hierarchy(headers, i)
            
            # ê²€ìƒ‰ ìµœì í™”ëœ ë‚´ìš© êµ¬ì„±
            enhanced_content = f"""
[SEARCH_OPTIMIZED_SECTION]
[KEYWORDS]: {', '.join(found_keywords) if found_keywords else 'None'}
[FULL_TITLE]: {title_hierarchy}
[SHORT_TITLE]: {header['title']}
[HEADER_LEVEL]: {header['level']}
[CONTENT]:
{content}
[END_SECTION]
"""
            
            doc = Document(
                page_content=enhanced_content,
                metadata={
                    'type': 'search_optimized',
                    'title': header['title'],
                    'title_hierarchy': title_hierarchy,
                    'keywords': found_keywords,
                    'keyword_count': len(found_keywords),
                    'header_level': header['level'],
                    'source': self.markdown_file_path,
                    'line_number': header['line_number']
                }
            )
            docs.append(doc)
        
        return docs
    
    def build_title_hierarchy(self, headers: List[Dict], current_index: int) -> str:
        """í˜„ì¬ í—¤ë”ê¹Œì§€ì˜ ì œëª© ê³„ì¸µ êµ¬ì¡°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        current_header = headers[current_index]
        hierarchy = [current_header['title']]
        
        for i in range(current_index - 1, -1, -1):
            if headers[i]['level'] < current_header['level']:
                hierarchy.insert(0, headers[i]['title'])
                current_header = headers[i]
        
        return ' / '.join(hierarchy)
    
    def create_vectorstore(self, docs: List[Document], strategy_name: str) -> FAISS:
        """ë²¡í„° ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        print(f"\n=== {strategy_name} ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì¤‘ ===")
        print(f"ë¬¸ì„œ ìˆ˜: {len(docs)}")
        
        # GPU ìƒíƒœ í™•ì¸
        if torch.cuda.is_available():
            print(f"GPU: {torch.cuda.get_device_name(0)}")
            print(f"GPU ë©”ëª¨ë¦¬: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f}GB")
        
        # ë‹¤ì–‘í•œ ì„ë² ë”© ëª¨ë¸ ì‹œë„
        embedding_models = [
            "nomic-embed-text",  # ê¸°ë³¸ ëª¨ë¸
            "llama3.2:3b",       # ëŒ€ì•ˆ ëª¨ë¸
            "llama3.2:1b"        # ê²½ëŸ‰ ëª¨ë¸
        ]
        
        for model_name in embedding_models:
            try:
                print(f"ì„ë² ë”© ëª¨ë¸ ì‹œë„: {model_name}")
                embeddings = OllamaEmbeddings(model=model_name)
                
                # í…ŒìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
                test_text = "PNS purchaseState COMPLETED CANCELED"
                test_embedding = embeddings.embed_query(test_text)
                print(f"ì„ë² ë”© ì°¨ì›: {len(test_embedding)}")
                
                vectorstore = FAISS.from_documents(docs, embeddings)
                print(f"âœ… {model_name} ëª¨ë¸ë¡œ ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì„±ê³µ")
                return vectorstore
                
            except Exception as e:
                print(f"âŒ {model_name} ëª¨ë¸ ì‹¤íŒ¨: {e}")
                continue
        
        # ëª¨ë“  ëª¨ë¸ì´ ì‹¤íŒ¨í•˜ë©´ ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©
        print("ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©")
        embeddings = OllamaEmbeddings(model="nomic-embed-text")
        vectorstore = FAISS.from_documents(docs, embeddings)
        return vectorstore
    
    def test_search_performance(self, vectorstore: FAISS, query: str) -> Dict[str, Any] | None:
        """ê²€ìƒ‰ ì„±ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
        print(f"\nğŸ” ê²€ìƒ‰ í…ŒìŠ¤íŠ¸: '{query}'")
        
        # ë‹¤ì–‘í•œ ê²€ìƒ‰ ë°©ë²• ì‹œë„
        search_methods = [
            {"k": 5, "search_type": "similarity"},
            {"k": 10, "search_type": "similarity"},
            {"k": 5, "search_type": "mmr"},
            {"k": 10, "search_type": "mmr"}
        ]
        
        best_results = None
        best_score = 0
        
        for method in search_methods:
            try:
                retriever = vectorstore.as_retriever(search_kwargs=method)
                docs = retriever.get_relevant_documents(query)
                
                # ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
                relevance_score = 0
                pns_found = 0
                purchase_state_found = 0
                
                for doc in docs:
                    content = doc.page_content.lower()
                    if 'pns' in content:
                        relevance_score += 10
                        pns_found += 1
                    if 'purchasestate' in content or 'purchase_state' in content:
                        relevance_score += 8
                        purchase_state_found += 1
                    if 'completed' in content or 'canceled' in content:
                        relevance_score += 5
                    if 'ê²°ì œ' in content or 'ì·¨ì†Œ' in content:
                        relevance_score += 3
                
                avg_score = relevance_score / len(docs) if docs else 0
                
                print(f"  {method}: ì ìˆ˜ {avg_score:.2f}, PNS {pns_found}ê°œ, purchaseState {purchase_state_found}ê°œ")
                
                if avg_score > best_score:
                    best_score = avg_score
                    best_results = {
                        'method': method,
                        'docs': docs,
                        'score': avg_score,
                        'pns_found': pns_found,
                        'purchase_state_found': purchase_state_found
                    }
                    
            except Exception as e:
                print(f"  {method}: ì˜¤ë¥˜ - {e}")
        
        return best_results
    
    def analyze_document_distribution(self, docs: List[Document]) -> Dict[str, Any]:
        """ë¬¸ì„œ ë¶„í¬ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
        pns_docs = []
        purchase_state_docs = []
        
        for doc in docs:
            content = doc.page_content.lower()
            if 'pns' in content:
                pns_docs.append(doc)
            if 'purchasestate' in content or 'purchase_state' in content:
                purchase_state_docs.append(doc)
        
        return {
            'total_docs': len(docs),
            'pns_docs': len(pns_docs),
            'purchase_state_docs': len(purchase_state_docs),
            'pns_doc_titles': [doc.metadata.get('title', 'Unknown') for doc in pns_docs],
            'purchase_state_doc_titles': [doc.metadata.get('title', 'Unknown') for doc in purchase_state_docs]
        }
    
    def run_improved_experiment(self):
        """ê°œì„ ëœ ì‹¤í—˜ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        print("ğŸš€ ê²€ìƒ‰ ì„±ëŠ¥ ê°œì„  ì‹¤í—˜ ì‹œì‘")
        
        # ë¬¸ì„œ ë¶„í• 
        docs = self.strategy_improved_search()
        print(f"ìƒì„±ëœ ë¬¸ì„œ ìˆ˜: {len(docs)}")
        
        # ë¬¸ì„œ ë¶„í¬ ë¶„ì„
        distribution = self.analyze_document_distribution(docs)
        print(f"\nğŸ“Š ë¬¸ì„œ ë¶„í¬ ë¶„ì„:")
        print(f"  ì´ ë¬¸ì„œ ìˆ˜: {distribution['total_docs']}")
        print(f"  PNS í¬í•¨ ë¬¸ì„œ: {distribution['pns_docs']}")
        print(f"  purchaseState í¬í•¨ ë¬¸ì„œ: {distribution['purchase_state_docs']}")
        
        if distribution['pns_docs'] > 0:
            print(f"  PNS ë¬¸ì„œ ì œëª©ë“¤: {distribution['pns_doc_titles'][:5]}")
        
        # ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        vectorstore = self.create_vectorstore(docs, "improved_search")
        
        # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        test_queries = [
            "PNS",
            "purchaseState",
            "PNS purchaseState",
            "COMPLETED CANCELED",
            "Payment Notification Service"
        ]
        
        results = {}
        for query in test_queries:
            result = self.test_search_performance(vectorstore, query)
            if result:
                results[query] = result
                print(f"âœ… '{query}' ê²€ìƒ‰ ì™„ë£Œ - ì ìˆ˜: {result['score']:.2f}")
            else:
                print(f"âŒ '{query}' ê²€ìƒ‰ ì‹¤íŒ¨")
        
        # ê²°ê³¼ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = f"improved_experiment_results_{timestamp}.json"
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump({
                'distribution': distribution,
                'search_results': results,
                'timestamp': timestamp
            }, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“ ê²°ê³¼ê°€ {results_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return results

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    experiment = ImprovedRAGExperiment("data/dev_center_guide_allmd_touched.md")
    results = experiment.run_improved_experiment()
    
    print("\nâœ… ê°œì„ ëœ ì‹¤í—˜ ì™„ë£Œ!")

if __name__ == "__main__":
    main() 