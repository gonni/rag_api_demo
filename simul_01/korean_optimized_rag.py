#!/usr/bin/env python3
"""
í•œêµ­ì–´ ìµœì í™”ëœ RAG ì‹¤í—˜
"""

import os
import re
import torch
from typing import List, Dict, Any
from pathlib import Path
from langchain.docstore.document import Document
from langchain_ollama import OllamaEmbeddings
from langchain_community.vectorstores import FAISS
import json
from datetime import datetime

class KoreanOptimizedRAG:
    def __init__(self, markdown_file_path: str):
        self.markdown_file_path = markdown_file_path
        self.raw_text = self.load_markdown_file(markdown_file_path)
        
    def load_markdown_file(self, file_path: str) -> str:
        """ë§ˆí¬ë‹¤ìš´ íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
        with open(file_path, 'r', encoding='utf-8') as file:
            return file.read()
    
    def extract_headers(self, text: str) -> List[Dict[str, Any]]:
        """ë§ˆí¬ë‹¤ìš´ í…ìŠ¤íŠ¸ì—ì„œ í—¤ë” ì •ë³´ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        headers = []
        lines = text.split('\n')
        
        for i, line in enumerate(lines):
            header_match = re.match(r'^(#{1,6})\s+(.+)$', line.strip())
            if header_match:
                level = len(header_match.group(1))
                title = header_match.group(2).strip()
                headers.append({
                    'level': level,
                    'title': title,
                    'line_number': i,
                    'content_start': i + 1
                })
        
        return headers
    
    def get_content_between_headers(self, text: str, start_header: Dict, end_header: Dict[str, Any] | None = None) -> str:
        """ë‘ í—¤ë” ì‚¬ì´ì˜ ë‚´ìš©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤."""
        lines = text.split('\n')
        start_line = start_header['content_start']
        
        if end_header:
            end_line = end_header['line_number']
        else:
            end_line = len(lines)
        
        return '\n'.join(lines[start_line:end_line])
    
    def strategy_korean_optimized(self) -> List[Document]:
        """í•œêµ­ì–´ ìµœì í™”ëœ ë¶„í•  ì „ëµ"""
        headers = self.extract_headers(self.raw_text)
        docs = []
        
        # í•œêµ­ì–´ ì „ë¬¸ í‚¤ì›Œë“œ
        korean_keywords = [
            'PNS', 'purchaseState', 'COMPLETED', 'CANCELED',
            'ê²°ì œ', 'ì·¨ì†Œ', 'ì›ìŠ¤í† ì–´', 'ì¸ì•±ê²°ì œ', 'Payment Notification Service',
            'API', 'SDK', 'ê°œë°œì', 'ê°€ì´ë“œ', 'ë¬¸ì„œ'
        ]
        
        for i, header in enumerate(headers):
            content = self.get_content_between_headers(
                self.raw_text,
                header,
                headers[i + 1] if i + 1 < len(headers) else None
            )
            
            # í•œêµ­ì–´ í‚¤ì›Œë“œ ê²€ìƒ‰
            found_keywords = []
            for keyword in korean_keywords:
                if keyword.lower() in content.lower():
                    found_keywords.append(keyword)
            
            # ì œëª© ê³„ì¸µ êµ¬ì¡° ìƒì„±
            title_hierarchy = self.build_title_hierarchy(headers, i)
            
            # í•œêµ­ì–´ ìµœì í™”ëœ ë‚´ìš© êµ¬ì„±
            enhanced_content = f"""
[í•œêµ­ì–´_ìµœì í™”_ì„¹ì…˜]
[í‚¤ì›Œë“œ]: {', '.join(found_keywords) if found_keywords else 'ì—†ìŒ'}
[ì „ì²´_ì œëª©]: {title_hierarchy}
[ì§§ì€_ì œëª©]: {header['title']}
[í—¤ë”_ë ˆë²¨]: {header['level']}
[ë‚´ìš©]:
{content}
[ì„¹ì…˜_ë]
"""
            
            doc = Document(
                page_content=enhanced_content,
                metadata={
                    'type': 'korean_optimized',
                    'title': header['title'],
                    'title_hierarchy': title_hierarchy,
                    'keywords': found_keywords,
                    'keyword_count': len(found_keywords),
                    'header_level': header['level'],
                    'source': self.markdown_file_path,
                    'line_number': header['line_number']
                }
            )
            docs.append(doc)
        
        return docs
    
    def build_title_hierarchy(self, headers: List[Dict], current_index: int) -> str:
        """í˜„ì¬ í—¤ë”ê¹Œì§€ì˜ ì œëª© ê³„ì¸µ êµ¬ì¡°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        current_header = headers[current_index]
        hierarchy = [current_header['title']]
        
        for i in range(current_index - 1, -1, -1):
            if headers[i]['level'] < current_header['level']:
                hierarchy.insert(0, headers[i]['title'])
                current_header = headers[i]
        
        return ' / '.join(hierarchy)
    
    def create_vectorstore_with_korean_models(self, docs: List[Document]) -> FAISS:
        """í•œêµ­ì–´ ìµœì í™”ëœ ë²¡í„° ìŠ¤í† ì–´ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        print(f"\n=== í•œêµ­ì–´ ìµœì í™” ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì¤‘ ===")
        print(f"ë¬¸ì„œ ìˆ˜: {len(docs)}")
        
        # í•œêµ­ì–´ ìµœì í™” ëª¨ë¸ë“¤
        korean_models = [
            "qwen2.5:7b",           # ë‹¤êµ­ì–´ ì§€ì› ìš°ìˆ˜
            "koalpaca-polyglot-5.8b", # í•œêµ­ì–´ íŠ¹í™”
            "llama3.2:3b",          # ê²½ëŸ‰ ë‹¤êµ­ì–´
            "mistral:7b",           # ê¸°ìˆ  ë¬¸ì„œ íŠ¹í™”
            "nomic-embed-text"      # ê¸°ë³¸ ëª¨ë¸
        ]
        
        for model_name in korean_models:
            try:
                print(f"í•œêµ­ì–´ ëª¨ë¸ ì‹œë„: {model_name}")
                embeddings = OllamaEmbeddings(model=model_name)
                
                # í•œêµ­ì–´ í…ŒìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±
                test_text = "PNS purchaseState COMPLETED CANCELED ê²°ì œ ì·¨ì†Œ"
                test_embedding = embeddings.embed_query(test_text)
                print(f"ì„ë² ë”© ì°¨ì›: {len(test_embedding)}")
                
                vectorstore = FAISS.from_documents(docs, embeddings)
                print(f"âœ… {model_name} ëª¨ë¸ë¡œ ë²¡í„° ìŠ¤í† ì–´ ìƒì„± ì„±ê³µ")
                return vectorstore
                
            except Exception as e:
                print(f"âŒ {model_name} ëª¨ë¸ ì‹¤íŒ¨: {e}")
                continue
        
        # ëª¨ë“  ëª¨ë¸ì´ ì‹¤íŒ¨í•˜ë©´ ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©
        print("ê¸°ë³¸ ëª¨ë¸ ì‚¬ìš©")
        embeddings = OllamaEmbeddings(model="nomic-embed-text")
        vectorstore = FAISS.from_documents(docs, embeddings)
        return vectorstore
    
    def test_korean_search(self, vectorstore: FAISS, query: str) -> Dict[str, Any] | None:
        """í•œêµ­ì–´ ê²€ìƒ‰ ì„±ëŠ¥ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
        print(f"\nğŸ” í•œêµ­ì–´ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸: '{query}'")
        
        # í•œêµ­ì–´ ê²€ìƒ‰ ë°©ë²•ë“¤
        search_methods = [
            {"k": 5, "search_type": "similarity"},
            {"k": 10, "search_type": "similarity"},
            {"k": 5, "search_type": "mmr"},
            {"k": 10, "search_type": "mmr"}
        ]
        
        best_results = None
        best_score = 0
        
        for method in search_methods:
            try:
                retriever = vectorstore.as_retriever(search_kwargs=method)
                docs = retriever.get_relevant_documents(query)
                
                # í•œêµ­ì–´ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°
                relevance_score = 0
                pns_found = 0
                purchase_state_found = 0
                korean_found = 0
                
                for doc in docs:
                    content = doc.page_content.lower()
                    if 'pns' in content:
                        relevance_score += 10
                        pns_found += 1
                    if 'purchasestate' in content or 'purchase_state' in content:
                        relevance_score += 8
                        purchase_state_found += 1
                    if 'ê²°ì œ' in content or 'ì·¨ì†Œ' in content:
                        relevance_score += 6
                        korean_found += 1
                    if 'completed' in content or 'canceled' in content:
                        relevance_score += 5
                    if 'ì›ìŠ¤í† ì–´' in content or 'ì¸ì•±ê²°ì œ' in content:
                        relevance_score += 4
                
                avg_score = relevance_score / len(docs) if docs else 0
                
                print(f"  {method}: ì ìˆ˜ {avg_score:.2f}, PNS {pns_found}ê°œ, purchaseState {purchase_state_found}ê°œ, í•œêµ­ì–´ {korean_found}ê°œ")
                
                if avg_score > best_score:
                    best_score = avg_score
                    best_results = {
                        'method': method,
                        'docs': docs,
                        'score': avg_score,
                        'pns_found': pns_found,
                        'purchase_state_found': purchase_state_found,
                        'korean_found': korean_found
                    }
                    
            except Exception as e:
                print(f"  {method}: ì˜¤ë¥˜ - {e}")
        
        return best_results
    
    def run_korean_experiment(self):
        """í•œêµ­ì–´ ìµœì í™” ì‹¤í—˜ì„ ì‹¤í–‰í•©ë‹ˆë‹¤."""
        print("ğŸš€ í•œêµ­ì–´ ìµœì í™” RAG ì‹¤í—˜ ì‹œì‘")
        
        # ë¬¸ì„œ ë¶„í• 
        docs = self.strategy_korean_optimized()
        print(f"ìƒì„±ëœ ë¬¸ì„œ ìˆ˜: {len(docs)}")
        
        # ë²¡í„° ìŠ¤í† ì–´ ìƒì„±
        vectorstore = self.create_vectorstore_with_korean_models(docs)
        
        # í•œêµ­ì–´ ê²€ìƒ‰ í…ŒìŠ¤íŠ¸
        korean_queries = [
            "PNS",
            "purchaseState",
            "ê²°ì œ ìƒíƒœ",
            "ì›ìŠ¤í† ì–´ ì¸ì•±ê²°ì œ",
            "COMPLETED CANCELED",
            "Payment Notification Service"
        ]
        
        results = {}
        for query in korean_queries:
            result = self.test_korean_search(vectorstore, query)
            if result:
                results[query] = result
                print(f"âœ… '{query}' í•œêµ­ì–´ ê²€ìƒ‰ ì™„ë£Œ - ì ìˆ˜: {result['score']:.2f}")
            else:
                print(f"âŒ '{query}' í•œêµ­ì–´ ê²€ìƒ‰ ì‹¤íŒ¨")
        
        # ê²°ê³¼ ì €ì¥
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        results_file = f"korean_experiment_results_{timestamp}.json"
        
        with open(results_file, 'w', encoding='utf-8') as f:
            json.dump({
                'korean_search_results': results,
                'timestamp': timestamp
            }, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ“ ê²°ê³¼ê°€ {results_file}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        return results

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    experiment = KoreanOptimizedRAG("data/dev_center_guide_allmd_touched.md")
    results = experiment.run_korean_experiment()
    
    print("\nâœ… í•œêµ­ì–´ ìµœì í™” ì‹¤í—˜ ì™„ë£Œ!")

if __name__ == "__main__":
    main() 